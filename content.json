{"meta":{"title":"qsing`s blog","subtitle":"qsing`s blog","description":"docker k8s go pyhton shell","author":"qsing","url":"https://iqsing.github.io","root":"/"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2022-02-04T05:16:11.653Z","updated":"2022-02-04T05:16:11.653Z","comments":false,"path":"/404.html","permalink":"https://iqsing.github.io/404.html","excerpt":"","text":""},{"title":"About me","date":"2022-02-04T05:16:12.095Z","updated":"2022-02-04T05:16:12.095Z","comments":false,"path":"about/index.html","permalink":"https://iqsing.github.io/about/index.html","excerpt":"","text":"一个DevOps爱好者。学习关于云原生相关的技术。"},{"title":"分类","date":"2022-02-04T05:16:12.102Z","updated":"2022-02-04T05:16:12.102Z","comments":false,"path":"categories/index.html","permalink":"https://iqsing.github.io/categories/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2022-02-04T05:16:12.115Z","updated":"2022-02-04T05:16:12.115Z","comments":true,"path":"links/index.html","permalink":"https://iqsing.github.io/links/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2022-02-04T05:16:12.126Z","updated":"2022-02-04T05:16:12.126Z","comments":false,"path":"repository/index.html","permalink":"https://iqsing.github.io/repository/index.html","excerpt":"","text":""},{"title":"标签","date":"2022-02-04T05:16:12.143Z","updated":"2022-02-04T05:16:12.143Z","comments":false,"path":"tags/index.html","permalink":"https://iqsing.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Prometheus TSDB存储原理","slug":"Prometheus TSDB存储原理","date":"2022-04-15T12:16:21.000Z","updated":"2022-04-14T16:53:21.135Z","comments":true,"path":"2022/04/15/Prometheus TSDB存储原理/","link":"","permalink":"https://iqsing.github.io/2022/04/15/Prometheus%20TSDB%E5%AD%98%E5%82%A8%E5%8E%9F%E7%90%86/","excerpt":"","text":"Prometheus 包含一个存储在本地磁盘的时间序列数据库，同时也支持与远程存储系统集成，比如grafana cloud 提供的免费云存储API，只需将remote_write接口信息填写在Prometheus配置文件即可。 本文不涉及远程存储接口内容，主要介绍Prometheus 时序数据的本地存储实现原理。 什么是时序数据？ 在学习Prometheus TSDB存储原理之前，我们先来认识一下Prometheus TSDB、InfluxDB这类时序数据库的时序数据指的是什么？ 时序数据通常以(key,value)的形式出现，在时间序列采集点上所对应值的集，即每个数据点都是一个由时间戳和值组成的元组。 1identifier-&gt;(t0,v0),(t1,v1),(t2,v2)... Prometheus TSDB的数据模型 1&lt;metric name&gt;&#123;&lt;label name&gt;=&lt;label value&gt;, ...&#125; 具体到某个实例中 1requests_total&#123;method=&quot;POST&quot;, handler=&quot;/messages&quot;&#125; 在存储时可以通过name label来标记metric name，再通过标识符@来标识时间，这样构成了一个完整的时序数据样本。 12 ----------------------------------------key-----------------------------------------------value---------&#123;__name__=&quot;requests_total&quot;,method=&quot;POST&quot;, handler=&quot;/messages&quot;&#125; @1649483597.197 52 一个时间序列是一组时间上严格单调递增的数据点序列，它可以通过metric来寻址。抽象成二维平面来看，二维平面的横轴代表单调递增的时间，metrics 遍及整个纵轴。在提取样本数据时只要给定时间窗口和metric就可以得到value 时序数据如何在Prometheus TSDB存储？ 上面我们简单了解了时序数据，接下来我们展开Prometheus TSDB存储（V3引擎） Prometheus TSDB 概览 在上图中，Head 块是TSDB的内存块，灰色块Block是磁盘上的持久块。 首先传入的样本（t,v）进入 Head 块，为了防止内存数据丢失先做一次**预写日志 (WAL)，并在内存中停留一段时间，然后刷新到磁盘并进行内存映射(M-map)**。当这些内存映射的块或内存中的块老化到某个时间点时，会作为持久块Block存储到磁盘。接下来多个Block在它们变旧时被合并，并在超过保留期限后被清理。 Head中样本的生命周期 当一个样本传入时，它会被加载到Head中的active chunk（红色块），这是唯一一个可以主动写入数据的单元，为了防止内存数据丢失还会做一次**预写日志 (WAL)**。 一旦active chunk被填满时（超过2小时或120样本），将旧的数据截断为head_chunk1。 head_chunk1被刷新到磁盘然后进行内存映射。active chunk继续写入数据、截断数据、写入到内存映射，如此反复。 内存映射应该只加载最新的、最被频繁使用的数据，所以Prometheus TSDB将就是旧数据刷新到磁盘持久化存储Block，如上1-4为旧数据被写入到下图的Block中。 此时我们再来看一下Prometheus TSDB 数据目录基本结构，好像更清晰了一些。 123456789101112131415./data├── 01BKGV7JBM69T2G1BGBGM6KB12 │ └── meta.json├── 01BKGTZQ1SYQJTR4PB43C8PD98 # block ID│ ├── chunks # Block中的chunk文件│ │ └── 000001 │ ├── tombstones # 数据删除记录文件│ ├── index # 索引│ └── meta.json # bolck元信息├── chunks_head # head内存映射│ └── 000001 └── wal # 预写日志 ├── 000000002 └── checkpoint.00000001 └── 00000000 WAL 中checkpoint的作用我们需要定期删除旧的 wal 数据，否则磁盘最终会被填满，并且在TSDB重启时 replay wal 事件时会占用大量时间，所以wal中任何不再需要的数据，都需要被清理。而checkpoint会将wal 清理过后的数据做过滤写成新的段。 如下有6个wal数据段 12345678data└── wal ├── 000000 ├── 000001 ├── 000002 ├── 000003 ├── 000004 └── 000005 现在我们要清理时间点T之前的样本数据，假设为前4个数据段： 检查点操作将按000000 000001 000002 000003顺序遍历所有记录，并且： 删除不再在 Head 中的所有序列记录。 丢弃所有 time 在T之前的样本。 删除T之前的所有 tombstone 记录。 重写剩余的序列、样本和tombstone记录（与它们在 WAL 中出现的顺序相同）。 checkpoint被命名为创建checkpoint的最后一个段号checkpoint.X 这样我们得到了新的wal数据，当wal在replay时先找checkpoint，先从checkpoint中的数据段回放，然后是checkpoint.000003的下一个数据段000004 1234567data└── wal ├── checkpoint.000003 | ├── 000000 | └── 000001 ├── 000004 └── 000005 Block的持久化存储上面我们认识了wal和chunks_head的存储构造，接下来是Block，什么是持久化Block？在什么时候创建？为啥要合并Block? Block的目录结构 123456├── 01BKGTZQ1SYQJTR4PB43C8PD98 # block ID│ ├── chunks # Block中的chunk文件│ │ └── 000001 │ ├── tombstones # 数据删除记录文件│ ├── index # 索引│ └── meta.json # bolck元信息 磁盘上的Block是固定时间范围内的chunk的集合，由它自己的索引组成。其中包含多个文件的目录。每个Block都有一个唯一的 ID（ULID），他这个ID是可排序的。当我们需要更新、修改Block中的一些样本时，Prometheus TSDB只能重写整个Block，并且新块具有新的 ID（为了实现后面提到的索引）。如果需要删除的话Prometheus TSDB通过tombstones 实现了在不触及原始样本的情况下进行清理。 tombstones 可以认为是一个删除标记，它记载了我们在读取序列期间要忽略哪些时间范围。tombstones 是Block中唯一在写入数据后用于存储删除请求所创建和修改的文件。 tombstones中的记录数据结构如下，分别对应需要忽略的序列、开始和结束时间。 123┌────────────────────────┬─────────────────┬─────────────────┐│ series ref &lt;uvarint64&gt; │ mint &lt;varint64&gt; │ maxt &lt;varint64&gt; │└────────────────────────┴─────────────────┴─────────────────┘ meta.json meta.json包含了整个Block的所有元数据 123456789101112131415161718&#123; &quot;ulid&quot;: &quot;01EM6Q6A1YPX4G9TEB20J22B2R&quot;, &quot;minTime&quot;: 1602237600000, &quot;maxTime&quot;: 1602244800000, &quot;stats&quot;: &#123; &quot;numSamples&quot;: 553673232, &quot;numSeries&quot;: 1346066, &quot;numChunks&quot;: 4440437 &#125;, &quot;compaction&quot;: &#123; &quot;level&quot;: 1, &quot;sources&quot;: [ &quot;01EM65SHSX4VARXBBHBF0M0FDS&quot;, &quot;01EM6GAJSYWSQQRDY782EA5ZPN&quot; ] &#125;, &quot;version&quot;: 1&#125; 记录了人类可读的chunks的开始和结束时间，样本、序列、chunks数量以及合并信息。version告诉Prometheus如何解析metadata Block合并 我们可以从之前的图中看到当内存映射中chunk跨越2小时（默认）后第一个Block就被创建了，当 Prometheus 创建了一堆Block时，我们需要定期对这些块进行维护，以有效利用磁盘并保持查询的性能。 Block合并的主要工作是将一个或多个现有块（source blocks or parent blocks）写入一个新块，最后，删除源块并使用新的合并后的Block代替这些源块。 为什么需要对Block进行合并？ 上面对tombstones介绍我们知道Prometheus在对数据的删除操作会记录在单独文件stombstone中，而数据仍保留在磁盘上。因此，当stombstone序列超过某些百分比时，需要从磁盘中删除该数据。 如果样本数据值波动非常小，相邻两个Block中的大部分数据是相同的。对这些Block做合并的话可以减少重复数据，从而节省磁盘空间。 当查询命中大于1个Block时，必须合并每个块的结果，这可能会产生一些额外的开销。 如果有重叠的Block（在时间上重叠），查询它们还要对Block之间的样本进行重复数据删除，合并这些重叠块避免了重复数据删除的需要。 如上图示例所示，我们有一组顺序的Block[1, 2, 3, 4]。数据块1，2，和3可以被合并形成的新的块是[1, 4]。或者成对压缩为[1，3]。 所有的时间序列数据仍然存在，但是现在总体的数据块更少。 这显著降低了查询成本。 Block是如何删除的？ 对于源数据的删除Prometheus TSDB采用了一种简单的方式：即删除该目录下不在我们保留时间窗口的块。 如下图所示，块1可以安全地被删除，而2必须保留到完全落在边界之后 因为Block合并的存在，意味着获取越旧的数据，数据块可能就变得越大。 因此必须得有一个合并的上限，，这样块就不会增长到跨越整个数据库。通常我们可以根据保留窗口设置百分比。 如何从大量的series中检索出数据？ 在Prometheus TSDB V3引擎中使用了倒排索引，倒排索引基于它们内容的子集提供对数据项的快速查找，例如我们要找出所有带有标签app =&quot;nginx&quot;的序列，而无需遍历每一个序列然后再检查它是否包含该标签。 首先我们给每个序列分配一个唯一ID，查询ID的复杂度是O(1)，然后给每个标签建一个倒排ID表。比如包含app =&quot;nginx&quot;标签的ID为1,11,111那么标签”nginx”的倒排序索引为[1,11,111]，这样一来如果n是我们的序列总数，m是查询的结果大小，那么使用倒排索引的查询复杂度是O(m)，也就是说查询的复杂度由m的数量决定。但是在最坏的情况下，比如我们每个序列都有一个“nginx”的标签，显然此时的复杂度变为O(n)了，如果是个别标签的话无可厚非，只能稍加等待了，但是现实并非如此。 标签被关联到数百万序列是很常见的，并且往往每次查询会检索多个标签，比如我们要查询这样一个序列app =“dev”AND app =“ops” 在最坏情况下复杂度是O(n^2)，接着更多标签复杂度指数增长到O(n^3)、O(n^4)、O(n^5)… 这是不可接受的。那咋办呢？ 如果我们将倒排表进行排序会怎么样？ 12&quot;app=dev&quot; -&gt; [100,1500,20000,51166]&quot;app=ops&quot; -&gt; [2,4,8,10,50,100,20000] 他们的交集为[100,20000]，要快速实现这一点，我们可以通过2个游标从列表值较小的一端率先推进，当值相等时就是可以加入到结果集合当中。这样的搜索成本显然更低，在k个倒排表搜索的复杂度为O(k*n)而非最坏情况下O(n^k) 剩下就是维护这个索引，通过维护时间线与ID、标签与倒排表的映射关系，可以保证查询的高效率。 以上我们从较浅的层面了解一下Prometheus TSDB存储相关的内容，本文仍然有很多细节没有提及，比如wal如何做压缩与回放，mmap的原理，TSDB存储文件的数据结构等等，如果你需要进一步学习可移步参考文章。通过博客阅读：iqsing.github.io 本文参考于： Prometheus维护者Ganesh Vernekar的系列博客Prometheus TSDB Prometheus维护者Fabian的博客文章Writing a Time Series Database from Scratch（原文已失效） PromCon 2017: Storing 16 Bytes at Scale - Fabian Reinartz","categories":[{"name":"prometheus","slug":"prometheus","permalink":"https://iqsing.github.io/categories/prometheus/"}],"tags":[{"name":"tsdb","slug":"tsdb","permalink":"https://iqsing.github.io/tags/tsdb/"}]},{"title":"PromQL全解析","slug":"promql 全解析","date":"2022-03-07T05:16:21.000Z","updated":"2022-03-07T06:00:12.034Z","comments":true,"path":"2022/03/07/promql 全解析/","link":"","permalink":"https://iqsing.github.io/2022/03/07/promql%20%E5%85%A8%E8%A7%A3%E6%9E%90/","excerpt":"","text":"PromQL（Prometheus Query Language）为Prometheus tsdb的查询语言。是结合grafana进行数据展示和告警规则的配置的关键部分。 本文默认您已了解Prometheus的四种指标类型： counter（计数器） gauge （仪表类型） histogram（直方图类型） summary （摘要类型） 便于读者实践，本文大部分样本数据target： Prometheus node_exporter 表达式数据类型 PromQL查询语句即表达式，实现的四种数据类型： Instant vector Instance vector（瞬时向量）表示一个时间序列的集合，但是每个时序只有最近的一个点，而不是线。 Range vector Range vector（范围向量）表示一段时间范围里的时序，每个时序可包含多个点 sources：Understanding Prometheus Range Vectors Scalar Scalar（标量）通常为数值，可以将只有一个时序的Instance vector转换成Scalar。 String 简单字符串值，目前未被使用。 选择器 标签选择器查询Prometheus http状态码为400的请求数量。 1prometheus_http_requests_total&#123;code=&quot;400&quot;&#125; 标签匹配运算符: =：与字符串匹配 !=：与字符串不匹配 =~：与正则匹配 !~：与正则不匹配 查询Prometheus http状态码为4xx或5xx并且handler为/api/v1/query的请求数量 1prometheus_http_requests_total&#123;code=~&quot;4.*|5.*&quot;,handler=&quot;/api/v1/query&quot;&#125; 内部标签__name__用来匹配指标名称，下面的表达式与上一条等价 1&#123;code=~&quot;4.*|5.*&quot;,handler=&quot;/api/v1/query&quot;,__name__=&quot;prometheus_http_requests_total&quot;&#125; 范围选择器查询过去5分钟Prometheus健康检查的采样记录。 1prometheus_http_requests_total&#123;code=&quot;200&quot;,handler=&quot;/-/healthy&quot;&#125;[5m] 单位：ms、s、m、h、d、w、y 时间串联：[1h5m]一小时5分钟 时间偏移 通过offset通过offset将时间倒退5分钟，即查询5分钟之前的数据。 1prometheus_http_requests_total&#123;code=&quot;200&quot;&#125; offset 5m 同样支持查询range vector 1prometheus_http_requests_total&#123;code=&quot;200&quot;&#125;[3m] offset 5m @修饰符还可以通过@ 直接跳转到某个uinx时间戳，需开启启动参数--enable-feature=promql-at-modifier 1prometheus_http_requests_total&#123;code=&quot;200&quot;&#125; @ 1646089826 运算符 Prometheus中的运算符与各类编程语言中的基本一致。 数学运算符Prometheus 中存在以下数学运算符： +（加法） -（减法） *（乘法） /（除法） %（取模） ^（幂） 两个标量之间的计算 110/3 瞬时向量与标量计算，由于计算后值意义与原指标名有差异，Prometheus很贴心的帮我们移除了指标名称。 1prometheus_http_response_size_bytes_sum / 1024 两个瞬时向量间的计算，如下计算node的内存使用率 123456(1 -node_memory_MemAvailable_bytes&#123;job=&quot;node&quot;,instance=&quot;localhost:9100&quot;&#125; / node_memory_MemTotal_bytes&#123;job=&quot;node&quot;,instance=&quot;localhost:9100&quot;&#125;)* 100 如果两个瞬时向量标签不一致可通过ignoring忽略多余标签 输入示例： 12345method_code:http_errors:rate5m&#123;method=&quot;get&quot;, code=&quot;500&quot;&#125; 24method_code:http_errors:rate5m&#123;method=&quot;post&quot;, code=&quot;500&quot;&#125; 6method:http_requests:rate5m&#123;method=&quot;get&quot;&#125; 600method:http_requests:rate5m&#123;method=&quot;post&quot;&#125; 120 查询示例： 1method_code:http_errors:rate5m&#123;code=&quot;500&quot;&#125; / ignoring(code) method:http_requests:rate5m 结果示例： 12&#123;method=&quot;get&quot;&#125; 0.04 // 24 / 600&#123;method=&quot;post&quot;&#125; 0.05 // 6 / 120 如果两个瞬时向量数量不一致时可通过group_left、group_right指定以那一侧为准 输入示例： 123456789method_code:http_errors:rate5m&#123;method=&quot;get&quot;, code=&quot;500&quot;&#125; 24method_code:http_errors:rate5m&#123;method=&quot;get&quot;, code=&quot;404&quot;&#125; 30method_code:http_errors:rate5m&#123;method=&quot;put&quot;, code=&quot;501&quot;&#125; 3method_code:http_errors:rate5m&#123;method=&quot;post&quot;, code=&quot;500&quot;&#125; 6method_code:http_errors:rate5m&#123;method=&quot;post&quot;, code=&quot;404&quot;&#125; 21method:http_requests:rate5m&#123;method=&quot;get&quot;&#125; 600method:http_requests:rate5m&#123;method=&quot;del&quot;&#125; 34method:http_requests:rate5m&#123;method=&quot;post&quot;&#125; 120 查询示例： group_left以左侧为准 1method_code:http_errors:rate5m / ignoring(code) group_left method:http_requests:rate5m 结果示例： 1234&#123;method=&quot;get&quot;, code=&quot;500&quot;&#125; 0.04 // 24 / 600&#123;method=&quot;get&quot;, code=&quot;404&quot;&#125; 0.05 // 30 / 600&#123;method=&quot;post&quot;, code=&quot;500&quot;&#125; 0.05 // 6 / 120&#123;method=&quot;post&quot;, code=&quot;404&quot;&#125; 0.175 // 21 / 120 比较运算符Prometheus 中存在以下比较运算符： ==（相等） !=（不相等） &gt;（大于） &lt;（小于） &gt;=（大于或等于） &lt;=（小于或等于） 两个标量之间比较，在运算符后跟bool修饰，结果0( false) 或1 ( true) 110 &lt; bool 5 瞬时向量与标量比较，查询node状态 1up&#123;job=&quot;node&quot;&#125; == bool 1 两个瞬时向量比较，查看消息队列容量状态 1prometheus_notifications_queue_length &lt; bool prometheus_notifications_queue_capacity 逻辑运算符Prometheus 中存在以下逻辑运算符： and（与） or（或） unless（非） 逻辑运算仅适用于向量 如下我们有4个target，进行相应的逻辑运算，实现和标签选择相似效果。 1up&#123;instance!=&quot;192.168.1.123:9091&quot;&#125; and up&#123;job!=&quot;alertmanager&quot;&#125; 1up&#123;instance=&quot;192.168.1.123:9091&quot;&#125; or up&#123;job=&quot;alertmanager&quot;&#125; 1up unless up&#123;job=&quot;alertmanager&quot;&#125; Prometheus 中二元运算符的优先级，从高到低。 ^ *, /, %,atan2 +,- ==, !=, &lt;=, &lt;, &gt;=,&gt; and,unless or 相同优先级的运算符是左结合的 聚合运算符Prometheus 支持以下内置聚合运算符，可用于聚合单个瞬时向量，生成新的向量： sum（总和） min（最小） max（最大） avg（平均值） group（分组） stddev（标准偏差） stdvar（标准方差） count（计算向量中的元素个数） count_values（计算具有相同值的元素个数） bottomk（样本值的最小 k 个元素） topk（按样本值计算的最大 k 个元素） quantile（分位数计算 φ-quantile (0 ≤ φ ≤ 1) 聚合运算符可通过 without、by 根据标签扩展 sum、min、max、avg： 计算http请求的总和，最大、最小请求的url的数量，平均数量 1sum(prometheus_http_requests_total) 通过状态码分别统计 group: 类uniq的用法 stddev、stdvar： 反映一组数据离散程度，用以衡量数据值偏离算术平均值的程度。标准偏差为方差的开平方，标准偏差越小，这些值偏离平均值就越少，反之亦然。 通过标准差来反映网络波动 1stddev(rate(node_network_transmit_bytes_total[5m])) rate计算某段时间的速率 count、count_values: 统计总共有几个时序 1count(prometheus_http_requests_total) 计算每个value的数量 1count_values(&quot;value&quot;,prometheus_http_requests_total) bottomk、topk 计算value中最小的5个时序 1bottomk(5,prometheus_http_requests_total) quantile:求数据的分位数 我们现在要找出K8s集群中所有node节点的内存使用率的分布情况 123456789quantile(0.8,(1 -node_memory_MemAvailable_bytes&#123;job=&quot;kubernetes-service-endpoints&quot;&#125; / node_memory_MemTotal_bytes&#123;job=&quot;kubernetes-service-endpoints&quot;&#125;)* 100) 直接可以看出80%的节点内存使用率在68%以下 函数 值取整ceil() ceil(v instant-vector)样本数据向上取整。 1ceil(node_load1) #1.2--&gt;2 floor() floor(v instant-vector)与ceil()相反，floor()样本值向下取整。 round() round(v instant-vector, to_nearest=1 scalar) 对样本值四舍五入取整。to_nearest 参数是可选的,默认为 1,表示样本返回的是最接近 1 的整数倍的值，参数可以为分数。 取整 1round(prometheus_engine_query_duration_seconds_sum) 取整到最近的5的倍数 1round(prometheus_engine_query_duration_seconds_sum,5) 值截取clamp() clamp(v instant-vector, min scalar, max scalar) 截取所有元素的样本值在 [min,max]集合内的样本,如果min&gt;max返回NaN 放回样本值在10到20的样本 1clamp(prometheus_http_requests_total,10,20) clamp_max() clamp_max(v instant-vector, max scalar) 同clamp()，不过只限定样本最大值 clamp_min() clamp_min(v instant-vector, min scalar) 同clamp()，不过只限定样本最小值 值变化统计changes() changes(v range-vector)返回某段时间内样本值改变的次数 1changes(node_load1[1m]) 复位统计resets() resets(v range-vector) 返回样本范围时间内的复位次数。与counter使用，两个连续样本之间值如有减少则被视为计数器复位。 查看上下文交换次数计数器在5分钟内复位次数 1resets(node_context_switches_total[5m]) 日期与时间管理day_of_month() day_of_month(v=vector(time()) instant-vector)如果样本值是utc时间，则返回这个时间所属月份中的日期（1-31） v=vector(time()) 为默认参数 1day_of_month(node_boot_time_seconds) day_of_week() day_of_week(v=vector(time()) instant-vector) 同上，如果样本值是utc时间，则返回这个时间所属星期几（0-6） days_in_month() days_in_month(v=vector(time()) instant-vector) 如果样本值是utc时间，则返回这个时间所属月份的天数（28-31） hour() hour(v=vector(time()) instant-vector)如果样本值是utc时间，则返回这个时间所属一天中的第几个小时（1-13） minute() minute(v=vector(time()) instant-vector) 如果样本值是utc时间，则返回这个时间所属小时中的第几分钟（1-59） month() month(v=vector(time()) instant-vector)如果样本值是utc时间，则返回这个时间所属的月份（1-12） year() year(v=vector(time()) instant-vector)如果样本值是utc时间，则返回这个时间所属的年份 time() 返回自1970 年 1 月 1 日 UTC 以来的秒数，不是系统时间，而是表达式计算时那一刻的时间。 timestamp() timestamp(v instant-vector)返回每个样本值的时间戳，自 1970 年 1 月 1 日 UTC 以来的秒数。 直方图分位数histogram_quantile() histogram_quantile(φ float, b instant-vector) 从 bucket 类型的向量 b 中计算 φ (0 ≤ φ ≤ 1) 分位数的样本的最大值，与聚合运算符quantile相似。 计算80%请求的持续时间最大值。 1histogram_quantile(0.8,rate(prometheus_http_request_duration_seconds_bucket[1d])) 差异与增长率delta() delta(v range-vector)计算范围向量中每个时间序列元素的第一个值和最后一个值之间的差。与指标类型gauge一起使用 计算一天内内存可用量的变化 1delta(node_memory_MemAvailable_bytes[1d]) idelta() idelta(v range-vector)计算范围向量中最后两个样本之间的差异。与指标类型gauge一起使用 1idelta(node_memory_MemAvailable_bytes[1m]) increase() increase(v range-vector) 计算时间范围内的增量，与counter一起使用。它是速率rate(v)乘以时间范围内秒数的语法糖，主要用于人类可读性。 计算10分钟内请求增长量 1increase(prometheus_http_requests_total[10m]) rate() rate(v range-vector)计算范围向量中时间序列的平均每秒增长率。 过去10分钟请求平均每秒增长率，与counter一起使用。 1rate(prometheus_http_requests_total[10m]) irate() irate(v range-vector) 通过时间范围的最后两个点来计算每秒瞬时增长率。 1irate(prometheus_http_requests_total[10m]) label管理label_join() label_join(v instant-vector, dst_label string, separator string, src_label_1 string, src_label_2 string, ...)为每个时间序列添加一个label，值为指定旧label的value连接 1label_join(up&#123;instance=&quot;localhost:9100&quot;, job=&quot;node&quot;&#125;,&quot;new_label&quot;,&quot;-&quot;,&quot;instance&quot;,&quot;job&quot;) 结果： 1up&#123;instance=&quot;localhost:9100&quot;, job=&quot;node&quot;, new_label=&quot;localhost:9100-node&quot;&#125; 1 label_replace() label_replace(v instant-vector, dst_label string, replacement string, src_label string, regex string)从源label中获取value元素用于添加新的label $1 获取正则匹配，匹配值添加到hello标签中 1label_replace(up&#123;instance=&quot;localhost:9100&quot;, job=&quot;node&quot;&#125;,&quot;hello&quot;,&quot;$1&quot;,&quot;job&quot;,&quot;(.*)&quot;) 结果： 12up&#123;hello=&quot;node&quot;, instance=&quot;localhost:9100&quot;, job=&quot;node&quot;&#125; 1 预测predict_linear() predict_linear(v range-vector, t scalar) 通过简单线性回归预测t秒后的样本值，与gauge一起使用。 根据过去1小时的文件系统剩余空间量，预测1小时之后的剩余空间 1predict_linear(node_filesystem_free_bytes[1h],3600) 转换absent() absent(v instant-vector)如果向量有元素，则返回一个空向量；如果向量没有元素，则返回值为 1。 设置如下告警表达式： 1absent(up&#123;job=&quot;node&quot;&#125; == 1) 由于up&#123;job=&quot;node&quot;&#125; 不存在或值不为1则告警表达式的值为1 产生告警 absent_over_time() absent_over_time(v range-vector)如果范围向量有元素，则返回一个空向量；如果范围向量没有元素，则返回值为 1。 如果up{job=”node1”}在某段时间不存在则返回1 1absent_over_time(up&#123;job=&quot;node1&quot;&#125;[1h]) scalar() scalar(v instant-vector)以标量形式返回该单元素的样本值,如果输入向量不是正好一个元素，scalar将返回NaN. vector() vector(s scalar)将标量作为没有标签的向量返回。 sgn() sgn(v instant-vector)返回一个向量，其中所有样本值都转换为1或-1或0 定义如下： 如果 v 为正，则为 1 如果 v 为负，则为 -1 如果 v 等于 0，则为 0。 排序sort() sort(v instant-vector)返回按样本值升序排序的向量元素。 sort_desc() 与sort()相反，按降序排序。 _over_time()下面的函数列表允许传入一个范围向量，返回一个带有聚合的瞬时向量： avg_over_time(range-vector): 区间向量内每个度量指标的平均值。 min_over_time(range-vector): 区间向量内每个度量指标的最小值。 max_over_time(range-vector): 区间向量内每个度量指标的最大值。 sum_over_time(range-vector): 区间向量内每个度量指标的求和值。 count_over_time(range-vector): 区间向量内每个度量指标的样本数据个数。 quantile_over_time(scalar, range-vector): 区间向量内每个度量指标的样本数据值分位数，φ-quantile (0 ≤ φ ≤ 1) stddev_over_time(range-vector): 区间向量内每个度量指标的总体标准偏差。 stdvar_over_time(range-vector): 区间向量内每个度量指标的总体标准方差 数学函数abs() abs(v instant-vector) 返回样本的绝对值。 sqrt() sqrt(v instant-vector)计算样本值的平方根。 deriv() deriv(v range-vector) 使用简单线性回归计算时间序列在范围向量中的每秒导数。与指标类型gauge一起使用 exp() exp(v instant-vector)计算样本值的指数函数。 特殊情况： Exp(+Inf) = +Inf Exp(NaN) = NaN ln()、log2()、log10() ln/log2/log10(v instant-vector) 计算样本值对数 特殊情况（同适用于log2/log10）： ln(+Inf) = +Inf ln(0) = -Inf ln(x &lt; 0) = NaN ln(NaN) = NaN holt_winters() holt_winters(v range-vector, sf scalar, tf scalar)基于访问向量v，生成时间序列数据平滑数据值。平滑因子sf越低, 对旧数据越重要。趋势因子tf越高，更关心趋势数据。0&lt;sf,tf&lt;=1。 与gauge一起使用 三角函数、弧度 acos(v instant-vector) acosh(v instant-vector) asin(v instant-vector) asinh(v instant-vector) atan(v instant-vector) atanh(v instant-vector) cos(v instant-vector) cosh(v instant-vector) sin(v instant-vector) sinh(v instant-vector) tan(v instant-vector) tanh(v instant-vector) 角度、弧度转化 deg(v instant-vector) pi() rad(v instant-vector)","categories":[{"name":"prometheus","slug":"prometheus","permalink":"https://iqsing.github.io/categories/prometheus/"}],"tags":[{"name":"PromQL","slug":"PromQL","permalink":"https://iqsing.github.io/tags/PromQL/"}]},{"title":"prometheus k8s服务发现","slug":"prometheus k8s服务发现","date":"2022-03-03T12:16:21.000Z","updated":"2022-04-14T16:53:54.986Z","comments":true,"path":"2022/03/03/prometheus k8s服务发现/","link":"","permalink":"https://iqsing.github.io/2022/03/03/prometheus%20k8s%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0/","excerpt":"","text":"Prometheus的服务发现在解决什么问题？ 被监控的目标（target）是整个监控体系中重要组成部分，传统监控系统zabbix通过 网络发现的机制自动创建主机到zabbix-server，进而快速的对目标进行监控。同样在Prometheus监控中存在一个叫服务发现的机制，在k8s容器环境中由于集群内实例网络地址是动态的，我们不可能每次创建或修改实例都将实例IP写入Prometheus的target中，借助服务发现我们可以快速的将集群内的资源注册到Prometheus-server中。 Prometheus 中的 scrape_config 是什么？ Prometheus通过yml文件来存储配置文件，通过scrape_config（抓取配置）域来配置抓取目标和抓取服务发现方式。 scrape_config指定了一组target和抓取参数。在一般情况下，一个scrape_config指定一个作业。 如下指定了两个静态服务发现prometheus、kube-state-metrics， 123456789scrape_configs:- job_name: prometheus static_configs: - targets: - localhost:9090- job_name: kube-state-metrics static_configs: - targets: - prometheus-kube-state-metrics.monitoring.svc:8080 Prometheus支持的服务发现非常多： static_configs: 静态服务发现 dns_sd_configs: DNS 服务发现 file_sd_configs: 文件服务发现 kubernetes_sd_configs: Kubernetes 服务发现 gce_sd_configs: GCE 服务发现 ec2_sd_configs: EC2 服务发现 openstack_sd_configs: OpenStack 服务发现 azure_sd_configs: Azure 服务发现 前面4个是比较常用的，这里我们主要介绍kubernetes_sd_configs，其他的比较简单可查看Prometheus官方文档 prometheus configuration 什么是 Kubernetes_sd_configs？ Prometheus中k8s服务发现的原理是通过 Kubernetes 的REST API 检索抓取目标，并始终与集群状态保持同步。所以我们需要配置Kubernetes_sd_configs来访问K8s API 比如我们要抓取k8s ingress，应为Prometheus指定用于RBAC认证证书和serviceaccount的token 12345678- job_name: &#x27;kubernetes-ingress&#x27; scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt insecure_skip_verify: true bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: ingress 这里的role为k8s中资源实体如 endpoints、service,、pod,、node或 ingress 当指定ingress时，Prometheus将每个入口地址发现为一个目标。 重载配置文件后可以在Prometheus Service Discovery查看发现的target 发现apiserver配置 123456789101112131415- job_name: kubernetes-apiservers scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt insecure_skip_verify: true bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: endpoints relabel_configs: - action: keep regex: default;kubernetes;https source_labels: - __meta_kubernetes_namespace - __meta_kubernetes_service_name - __meta_kubernetes_endpoint_port_name 这里我们用到了relabel_configs即重新打标，动作为keep 啥意思呢？ 首先我们通过k8s API获取到所有endpoints，将endpoints中的含元数据 namespace、service_name、endpoint_port_name的实例和regex匹配，如果匹配成功就保留。这用来过滤一下不需要的实例时很有用。 通过kubectl 查看的kubernetes这个endpoints的信息 123456789101112# kubectl describe endpoints kubernetesName: kubernetesNamespace: defaultLabels: &lt;none&gt;Annotations: &lt;none&gt;Subsets: Addresses: 192.168.1.82,192.168.1.83,192.168.1.84 NotReadyAddresses: &lt;none&gt; Ports: Name Port Protocol ---- ---- -------- https 6443 TCP 发出来的target如下 这里有一个隐藏点，Prometheus会把元数据中的__address__ 和__metrics_path__作为endpoint，下面我们来看一个替换元数据的node实例 发现node配置 123456789101112131415161718- job_name: kubernetes-nodes scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt insecure_skip_verify: true bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: node relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - replacement: kubernetes.default.svc:443 target_label: __address__ - regex: (.+) replacement: /api/v1/nodes/$1/proxy/metrics source_labels: - __meta_kubernetes_node_name target_label: __metrics_path__ 这里的动作为labelmap,可用于标签替换。首先获取所有node，对元数据__address__中的value替换为replacement的值kubernetes.default.svc:443 在replacement的值中可以通过$1,$2,$3…的方式引用source_labels的key-value，所以元数据__metrics_path__的值将会被/api/v1/nodes/{node_name}/proxy/metrics替换。 发现出的node如下所示，此时target的address和metrics_path已被替换了。 以上通过kubernetes-apiservers、kubernetes-nodes的实例简单介绍了Prometheus中如何实现k8s集群资源的服务发现以及相应的配置和操作。亦可参考Prometheus示例配置prometheus-kubernetes 希望小作文对你有些许帮助，如果内容有误请指正。通过博客阅读：iqsing.github.io","categories":[{"name":"prometheus","slug":"prometheus","permalink":"https://iqsing.github.io/categories/prometheus/"}],"tags":[{"name":"service-discovery","slug":"service-discovery","permalink":"https://iqsing.github.io/tags/service-discovery/"}]},{"title":"k8s 通过helm发布应用","slug":"k8s 通过包管理器helm发布应用","date":"2022-02-13T16:47:21.000Z","updated":"2022-02-13T18:45:16.267Z","comments":true,"path":"2022/02/14/k8s 通过包管理器helm发布应用/","link":"","permalink":"https://iqsing.github.io/2022/02/14/k8s%20%E9%80%9A%E8%BF%87%E5%8C%85%E7%AE%A1%E7%90%86%E5%99%A8helm%E5%8F%91%E5%B8%83%E5%BA%94%E7%94%A8/","excerpt":"","text":"什么是helm？ Helm 是 Kubernetes 的包管理器。Helm 是查找、分享和使用软件构建 Kubernetes 的最优方式。 在红帽系的Linux中我们使用yum来管理RPM包，类似的，在K8s中我们可以使用helm来管理资源对象（Deployment、Service、Ingress…）实现K8s中应用的快速发布、升级、维护和分享。helm官方文档 helm中的几个关键概念 Chart 是Helm 中的包。包含一组用于部署应用程序的 K8s 资源对象定义（即资源清单的集合）。 Repository 即chart图表的仓库。我们可以从网络仓库中搜索、下载和安装chart。 Release 即chart部署后的实例。通过helm install命令，在 Kubernetes 集群上安装该chart的新版本。 helm实现哪些功能？ Helm (v3版本)为 K8s 提供的功能包括： 通过单个 CLI 命令部署 Kubernetes 应用（chart）。实现本地chart的创建、管理和发布。 Helm 将chart中资源对象配置文件模板化，实现在多个集群环境中重用一个 Helm chart，同时可打包进行网络共享。 Helm 通过自动维护发布的所有版本来简化 Kubernetes 应用程序的回滚，防止部署问题。 通过helm轻松实现 Kubernetes 中工作负载的 CI/CD 管道。 helm 基本使用 Helm可以用源码或构建的二进制版本安装。参考：安装Helm Artifact Hub 是一个开源项目,我们通过它来查找、安装或发布k8s应用。 除了通过web搜索，也可以通过helm命令行方式： 12345#helm search hub redisURL CHART VERSION APP VERSION DESCRIPTIONhttps://hub.helm.sh/charts/bitnami/redis 16.4.0 6.2.6 Redis(TM) is an opensource, advanced key-value...https://hub.helm.sh/charts/wenerme/redis 16.4.0 6.2.6 Redis(TM) is an opensource, advanced key-value...... 找到redis版本为6.2.6，chart版本16.4.0的包，访问 url https://hub.helm.sh/charts/bitnami/redis 新版本已被重定向到artifacthub.io 由图上信息可以知redis是一个来自Bitnami仓库（由VMware主导的开源软件仓库），通过验证的版本，仓库地址https://charts.bitnami.com/bitnami 要安装这个应用我们应先将Bitnami仓库添加到本地配置中。 1#helm repo add bitnami https://charts.bitnami.com/bitnami 安装redis，release名称为redis-dev 1234567891011# helm install redis-dev bitnami/redisNAME: redis-devLAST DEPLOYED: Sun Feb 13 20:09:30 2022NAMESPACE: defaultSTATUS: deployedREVISION: 1TEST SUITE: NoneNOTES:CHART NAME: redisCHART VERSION: 16.4.0APP VERSION: 6.2.6 这样我们可以轻松发布一个一主三从的redis集群到k8s中 123# helm listNAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSIONredis-dev default 1 2022-02-13 20:09:30.755534484 +0800 CST deployed redis-16.4.0 Helm 通过向资源对象中添加标签来跟踪安装在 Kubernetes 集群上的chart。这些标签看起来像app.kubernetes.io/managed-by=Helm和app.kubernetes.io/instance: myapp。 123456789101112131415# kubectl get all -l app.kubernetes.io/instance=redis-devNAME READY STATUS RESTARTS AGEpod/redis-dev-master-0 1/1 Running 0 27mpod/redis-dev-replicas-0 1/1 Running 0 27mpod/redis-dev-replicas-1 1/1 Running 0 24mpod/redis-dev-replicas-2 1/1 Running 0 23mNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/redis-dev-headless ClusterIP None &lt;none&gt; 6379/TCP 27mservice/redis-dev-master ClusterIP 10.96.52.104 &lt;none&gt; 6379/TCP 27mservice/redis-dev-replicas ClusterIP 10.96.230.162 &lt;none&gt; 6379/TCP 27mNAME READY AGEstatefulset.apps/redis-dev-master 1/1 27mstatefulset.apps/redis-dev-replicas 3/3 27m 删除redis-dev的发布，将会移除标签跟踪的所有资源对象。 12# helm uninstall redis-devrelease &quot;redis-dev&quot; uninstalled 创建自己的helm chart 显然大多数时候我们更想发布自己的应用到K8s中或者需要对将要发布的开源软件做一些配置上的修改，所以我们可以通过helm自己构建一个chart或者使用helm pull下载一个chart做修改后再上传的内部或外部仓库中。 下面来创建一个简易的nginx chart 12# helm create chart-nginxCreating chart-nginx chart的目录结构，你可以删除模板中的所有文件自建或使用默认模板 1234567891011121314151617# tree chart-nginx/chart-nginx/├── charts #依赖的chart目录├── Chart.yaml #chart版本信息├── templates #资源对象模板目录│ ├── deployment.yaml│ ├── _helpers.tpl│ ├── hpa.yaml│ ├── ingress.yaml│ ├── NOTES.txt #提示信息│ ├── serviceaccount.yaml│ ├── service.yaml│ └── tests│ └── test-connection.yaml└── values.yaml #模板值3 directories, 10 files Chart.yaml声明了版本信息，我们可以进行自定义 1234567# Chart.yamlapiVersion: v2name: chart-nginxdescription: A Helm chart for Kubernetestype: applicationversion: 0.1.0 #chart版本appVersion: 1.0.0 #app版本 helm默认创建的模板文件deployment.yaml如下： helm 采用go模板，官方文档Chart模板 通过deployment模板中可以看到image的值会引用value文件中定义的image.repository和tag，如果tag值为空则返回默认引用Chart.appVersion的值。 接着根据需要更新value.yaml文件中image和service等相关信息，同时关闭serviceAccount、ingress、hpa的创建。 模板文件service.yaml定义好了type和pod的引用。 一个基本的nginx的chart创建好了。通过helm template 命令渲染模板查看一下 123456789101112131415161718192021222324252627282930313233# helm template chart-nginx---# Source: chart-nginx/templates/service.yamlapiVersion: v1kind: Servicemetadata: name: RELEASE-NAME-chart-nginx labels: helm.sh/chart: chart-nginx-0.1.0 app.kubernetes.io/name: chart-nginx app.kubernetes.io/instance: RELEASE-NAME app.kubernetes.io/version: &quot;1.0.0&quot; app.kubernetes.io/managed-by: Helmspec: type: NodePort ports: - port: 80 targetPort: http protocol: TCP name: http selector: app.kubernetes.io/name: chart-nginx app.kubernetes.io/instance: RELEASE-NAME---# Source: chart-nginx/templates/deployment.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: RELEASE-NAME-chart-nginx labels: helm.sh/chart: chart-nginx-0.1.0 app.kubernetes.io/name: chart-nginx... 再通过helm lint检查语法 1234==&gt; Linting chart-nginx[INFO] Chart.yaml: icon is recommended1 chart(s) linted, 0 chart(s) failed ok，通过helm install发布到k8s，参照NOTES说明可进行访问。 1234567891011# helm install chart-nginx --generate-nameNAME: chart-nginx-1644771770LAST DEPLOYED: Mon Feb 14 01:02:50 2022NAMESPACE: defaultSTATUS: deployedREVISION: 1NOTES:1. Get the application URL by running these commands: export NODE_PORT=$(kubectl get --namespace default -o jsonpath=&quot;&#123;.spec.ports[0].nodePort&#125;&quot; services chart-nginx-1644771770) export NODE_IP=$(kubectl get nodes --namespace default -o jsonpath=&quot;&#123;.items[0].status.addresses[0].address&#125;&quot;) echo http://$NODE_IP:$NODE_PORT 查看资源正常。 12345678910111213# kubectl get all -l app.kubernetes.io/name=chart-nginxNAME READY STATUS RESTARTS AGEpod/chart-nginx-1644771770-69bbb4fdf8-gqdk7 1/1 Running 0 3m59spod/chart-nginx-1644771770-69bbb4fdf8-wwxw2 1/1 Running 0 3m59sNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/chart-nginx-1644771770 NodePort 10.96.231.61 &lt;none&gt; 80:32631/TCP 3m59sNAME READY UP-TO-DATE AVAILABLE AGEdeployment.apps/chart-nginx-1644771770 2/2 2 2 3m59sNAME DESIRED CURRENT READY AGEreplicaset.apps/chart-nginx-1644771770-69bbb4fdf8 2 2 2 3m59s 通过仓库分发应用 首先通过helm packge将chart-nginx打包 12# helm package chart-nginxSuccessfully packaged chart and saved it to: .../../chart-nginx-0.1.0.tgz 建立chart私有仓库，可参考开源项目chartmuseum，如有必要你也可将仓库提交至artifacthub发布到互联网。 将chart-nginx-0.1.0.tgz上传至仓库后，通过curl列出chart信息如下： 123456789101112131415161718# curl http://192.168.1.123:8088/api/charts |python -m json.tool&#123; &quot;chart-nginx&quot;: [ &#123; &quot;apiVersion&quot;: &quot;v2&quot;, &quot;appVersion&quot;: &quot;1.0.0&quot;, &quot;created&quot;: &quot;2022-02-13T17:37:43.653117345Z&quot;, &quot;description&quot;: &quot;A Helm chart for Kubernetes&quot;, &quot;digest&quot;: &quot;58a687be62a2a2a2b1dd177675bbc5aa49ac754df2219149bb4798636662b57c&quot;, &quot;name&quot;: &quot;chart-nginx&quot;, &quot;type&quot;: &quot;application&quot;, &quot;urls&quot;: [ &quot;charts/chart-nginx-0.1.0.tgz&quot; ], &quot;version&quot;: &quot;0.1.0&quot; &#125; ]&#125; 将仓库添加到你的其他k8s集群helm中，实现应用共享和发布。 12# helm repo add chartmuseum http://192.168.1.123:8088&quot;chartmuseum&quot; has been added to your repositories 搜索chart-nginx 123# helm search repo chart-nginxNAME CHART VERSION APP VERSION DESCRIPTIONchartmuseum/chart-nginx 0.1.0 1.0.0 A Helm chart for Kubernetes 通过仓库安装chart-nginx 1234567# helm install my-chart-nginx chartmuseum/chart-nginxNAME: my-chart-nginxLAST DEPLOYED: Mon Feb 14 01:54:34 2022NAMESPACE: defaultSTATUS: deployedREVISION: 1... 以上我们对helm进行了基本介绍以及如何创建一个自己的helm chart,如何结合私有仓库chartmuseum在K8s中发布应用。 希望小作文对你有些许帮助，如果内容有误请指正。通过博客阅读：iqsing.github.io","categories":[{"name":"k8s","slug":"k8s","permalink":"https://iqsing.github.io/categories/k8s/"}],"tags":[{"name":"helm","slug":"helm","permalink":"https://iqsing.github.io/tags/helm/"}]},{"title":"理解https中的安全及其实现原理","slug":"理解https中的安全及其实现原理","date":"2022-02-06T16:47:21.000Z","updated":"2022-02-13T18:56:21.177Z","comments":true,"path":"2022/02/07/理解https中的安全及其实现原理/","link":"","permalink":"https://iqsing.github.io/2022/02/07/%E7%90%86%E8%A7%A3https%E4%B8%AD%E7%9A%84%E5%AE%89%E5%85%A8%E5%8F%8A%E5%85%B6%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/","excerpt":"","text":"Google的一份网络上的 HTTPS 加密透明报告（数据截至2022年1月）中指出HTTPS 连接的普及率在过去几年激增，互联网上排名前 100 位的非 Google 网站HTTPS 使用情况为：97%的站点默认启用HTTPS，100%的站点支持HTTPS。 Chrome 中的 HTTPS 浏览时间所占的百分比（按平台） Chrome 中通过 HTTPS 加载的网页所占的百分比（按国家/地区） 如此流行的HTTPS我们应当对其有所了解，通过阅读本文你可能能更进一步了解HTTPS相关的安全实现。 HTTPS(超文本传输安全协议)使用HTTP进行通信，但利用SSL/TLS来加密数据包，所以它也有另外一种称呼HTTP over TLS/SSL，说HTTPS安全其实说的就是TLS/SSL协议。HTTP以明文的方式在网络中交换数据，攻击者可以轻易通过监听或中间人攻击等手段，获取网站帐户和敏感信息等，而HTTPS可以做到如下几个特性： 保密性。 客户端的连接被加密，隐藏了 URL、cookie 和其他敏感元数据。 **真实性。 ** 确保客户端正在与“真实”的服务端通信，而非中间人。 准确性。 客户端与服务端之间发送的数据没有被篡改。 保密性–对称加密、非对称加密 我们说http是明文传输，所以https首要解决的问题就是它的通信加密，达到保密性。 对称加密对称加密是最简单、最常见的加密方式。 通信双方持有相同的密钥key，加密和解密都是使用同一个密钥。当客户端要发送数据时先用key对数据进行加密生成secret data，接着传输到服务端。服务端接收数据时，通过key将数据解密为data。反之客户端接收数据也是如此。 这样即使数据被截获，由于不知道key数据也无法被解密。常见的对称加密算法有 DES、 AES 等。对称加密速度快、效率高，能够使用较小的计算量完成加密。 对称加密有一个核心问题是如何在互联网上传输密钥？它不能像现实中一样我打个车就过去你家把密钥塞给你，万一密钥被截胡了不就白搭了。所以需要非对称加密来解决这个问题。 非对称加密非对称加密就是加密和解密使用两个不同的密钥，密钥对包含一个公钥（public key）和一个私钥（private key）。其中公钥只能用于加密，私钥只用于解密。 首先客户端请求服务端，服务端将自己的公钥返回，客户端拿到公钥后就可以用它来加密要传输的数据data ，将加密数据secret data发送到服务端后通过服务端的私钥来解密，以此完成加密传输。 有了非对称加密，只要我们将其中的data换成随机码key，这个key作为对称加密中密钥。密钥传输问题就解决了，同时很好地利用了对称加密的高效率。 如下所示： 这样HTTPS中通信的数据加密已经完成了。 一个http请求： 一个加密的https请求： 只要我们的私钥不被破解，即使通信被监听也得不到其中的敏感加密数据。 真实性、准确性–数字证书、签名 上面我们忽略了一个重要的问题，在通信中如何保证所连接的服务端真实性呢？如下图我们的通信已经被中间人截胡了，client此时通信对象为hacker。 在HTTPS中是如何防止这种中间人攻击的呢？让我们请出数字证书！ 数字证书所谓证书就是第三方（自签证书没有公证效应）颁发的认证，比如我们的学位证是由教育局颁发的一种学历认证，由教育局来认证此人获得了某个学位。同样在HTTPS中存在一种认证机构即CA（Certification Authority），由它来证明你所连接的服务端就是你想要连接的server，即保证服务端真实性。 要获取学位证你需要花钱上学、学习，而获取数字证书你只需要花钱。 首先站点的所有者生成一个密钥对，然后掏钱将站点的信息如域名、组织信息等以及公钥提交给CA机构审核，即**证书签名请求 (CSR)**。 CA机构审核通过后，用它独有的私钥对CSR信息（其实是CSR信息的hash值，用于加速加、解密）进行加密，即形成数字签名，用于验证证书是否被篡改，经过签名后一个完整的数字证书就成了其中包含站点信息、数字签名。 如下图所示（图源:what-is-a-certificate-authority）： ok，申请到了数字证书，给安装到server中。 当client请求时server返回数字证书，先查看证书认证的域名或所有者是谁？如果与你访问的域名不一致毫无疑问你正遭受中间人攻击，这是一个假站点请停止访问。 如果一致，接着client查看证书的签发CA机构是谁？找到浏览器或操作系统中对应的内置CA公钥，找不到？对不起，这个站点不安全（这其实也是垄断和付费的根源），如果找到则使用公钥解密签名得到hash值和此时证书中CSR信息的hash值做对比，如果一致，则这个证书没有被修改，你访问的站点很安全，取出证书中公钥来做加密通信吧。 如下图所示： HTTPS不保护的信息？ 虽然 HTTPS 对整个 HTTP 请求和响应进行加密，但 DNS 解析和连接监听仍然可以获得一些其他信息，例如完整的域名或子域以及原始 IP 地址。 别有用心者还可能通过分析加密的 HTTPS 流量以获取特殊信息比如在网站上花费的时间，或用户数据包相对大小。 攻击 HTTPS 连接的有多难？ 对 HTTPS 连接的攻击通常分为 3 类： 通过密码分析或其他协议的弱点破坏 HTTPS 连接的质量。 黑掉客户端，将恶意根证书安装到系统或浏览器信任库中。 获得浏览器信任的“流氓”证书，即通过操纵或破坏证书颁发机构。 以上是对HTTPS安全及其实现原理的学习，其中没有提到TLS/SSL版本、加密算法相关的内容，有兴趣的可以自行检索。 希望小作文对你有些许帮助，如果内容有误请指正。 您可以随意转载、修改、发布本文，无需经过本人同意。通过博客阅读：iqsing.github.io 参考： what-is-a-certificate-authority The HTTPS-Only Standard","categories":[{"name":"network","slug":"network","permalink":"https://iqsing.github.io/categories/network/"}],"tags":[{"name":"https","slug":"https","permalink":"https://iqsing.github.io/tags/https/"}]},{"title":"k8s 基于RBAC的认证、授权介绍和实践","slug":"k8s 基于RBAC的认证、授权介绍和实践","date":"2022-01-24T16:47:21.000Z","updated":"2022-02-04T05:33:35.799Z","comments":true,"path":"2022/01/25/k8s 基于RBAC的认证、授权介绍和实践/","link":"","permalink":"https://iqsing.github.io/2022/01/25/k8s%20%E5%9F%BA%E4%BA%8ERBAC%E7%9A%84%E8%AE%A4%E8%AF%81%E3%80%81%E6%8E%88%E6%9D%83%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%AE%9E%E8%B7%B5/","excerpt":"","text":"在K8S中，当我们试图通过API与集群资源交互时，必定经过集群资源管理对象入口kube-apiserver。显然不是随随便便来一个请求它都欢迎的，每个请求都需要经过合规检查，包括Authentication(身份验证)、Authorization(授权)和Admission Control(准入控制)。通过一系列验证后才能完成交互。 Kubernetes API 请求从发起到持久化到ETCD数据库中的过程如下： “三个A”我们可以简单理解为： Authentication：你是谁？你能登录系统么？ Authorization：你想做什么？你有相应的权限么？ Admission Control： 在apiserver中准入控制会以控制器插件的方式存在，类似于各类web框架中的中间件，可以在kube-apiserver的yml中添加控制器插件--enable-admission-plugins开启。 这篇小作文我们主要来学习K8S中关于认证与授权相关的知识，看看他们是如何实现的。包含如下内容： K8S 通过证书认证 K8S 通过RBAC 授权 一、 K8S 通过证书认证 Authentication(身份认证)，即核查用户能否进入K8s集群。一般来说k8s中有两类用户，普通用户和服务账户(Service Account)。 普通用户，使用者是人，即用户可以通过 kubectl 命令、或通过REST请求访问 API，但是请注意K8s不提供普通用户管理的资源对象，那所谓的普通用户哪里的？很简单只要你能通过k8s身份认证策略那么你就是一个普通用户。而Service Account 则是针对运行在 Pod 中的进程而言的。 K8S的几种验证方式： Certificate Token OpenID Web Hook 其中Certificate(证书)是在普通用户（客户端）中被广泛使用的验证方式。通过客户端证书进行身份验证时，客户端必须先获得一个有效的 x509 客户端证书，然后Kubernetes API服务器通过验证这个证书来验证你的身份。当然你的X509证书必须由集群 CA 证书签名。这其实就是HTTPS加密中的一部分，只不过是CA是K8S自签名的CA证书。 首先我们通过openssl创建一个用户私钥 1openssl genrsa -out develop1.key 2048 通过user.key 生成CSR（证书签名请求）,Kubernetes 使用证书中的 ‘subject’ 的通用名称（Common Name）字段来确定用户名,Organization Name 作为组。 1openssl req -new -key develop1.key -out develop1.csr -subj &quot;/CN=develop1/O=devops&quot; 有了CSR，我们就可以把它交给K8S admin通过集群CA签署客户端证书。kubeadm创建的集群证书对存储在master节点的 /etc/Kubernetes/pki/ 目录中，（当然如果你是admin，也可以直接通过API的方式签署证书）集群包含一个根 CA，用它签署所有集群组件相互通信所需的证书。 12openssl x509 -req -in develop1.csr -CA /etc/kubernetes/pki/ca.crt \\-CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out develop1.crt -days 30 这样我们获得了一个被集群CA签署过的证书develop1.crt 1develop1.crt develop1.csr develop1.key 查看证书内容openssl x509 -noout -text -in develop1.crt包含CN/O，以及证书过期时间。 好了，有了证书之后，下一步我们需要配置kubecofnig使kubectl可以正常访问apiserver，关于kubeconfig可参考官方文档organize-cluster-access-kubeconfig 这里我们以配置为主。 默认情况下，kubectl 读取 $HOME/.kube/config 作为配置文件。也可以通过两种方式为 kubectl 指定配置文件： 环境变量 KUBECONFIG 命令行参数 --kubeconfig 现在通过kubectl来创建config中的集群入口 1234kubectl config set-cluster kubernetes \\ --server=https://apiserver.cluster.local:6443 \\ --certificate-authority=/etc/kubernetes/pki/ca.crt \\ --embed-certs=true 创建用户入口 1234kubectl config set-credentials develop1 \\ --client-certificate=$HOME/private_key/develop1.crt \\ --client-key=$HOME/private_key/develop1.key \\ --embed-certs=true 创建上下文 123kubectl config set-context develop1 \\ --cluster=kubernetes \\ --user=develop1 指定当前context 12345$ kubectl config set current-context develop1Property &quot;current-context&quot; set.#查看当前context，已绑定develop1$ kubectl config current-contextdevelop1 通过kubectl config view查看当前的config 这样我们kubectl已经配置完毕，但是此时我们只完成了Authentication，并没有获得权限 12$ kubectl get podError from server (Forbidden): pods is forbidden: User &quot;develop1&quot; cannot list resource &quot;pods&quot; in API group &quot;&quot; in thenamespace &quot;default&quot; 可以看到develop1没有对命名空间default的list权限。所以接下来我们来学习Authorization授权相关内容。 二、K8S 通过RBAC 授权 RBAC(Role-Based Access Control)即基于角色的访问控制，在各类大型系统如虚拟化Vcenter、各类云服务以及众多toB软件访问控制中被大量使用。关于RBAC可参考一篇译文：[译] 基于角色的访问控制（RBAC）：演进历史、设计理念及简洁实现（Tailscale, 2021） k8s作为企业内部重要云基础设施并不希望每个使用平台的用户都可以不受限制的创建、修改和删除资源。同时伴随着集群节点、应用程序和团队数量的增加，你需要一种安全措施将用户或应用权限控制在某个范围内，这就K8S 在V1.8正式引入RBAC所要做的事（其他鉴权机制本文不涉及）。 K8S的RBAC 主要由Role、ClusterRole、RoleBinding 和 ClusterRoleBinding 等资源实现。模型如下： Role、ClusterRole角色是一组权限规则的集合，Role 用来定义某个命名空间内的访问权限，而ClusterRole 则是一个集群作用域的资源。为啥要用两个资源？因为Kubernetes 对象的作用域已经被划分为集群和命名空间两部分了。需要注意：角色只有授权没有禁止的操作。 构成一个Rule需要声明三部分： apiGroups：资源所属的API组：&quot;&quot; 缺省为 core 组资源，如：extensions、apps、batch等。Kubernetes API 参考文档 resources：资源，如： pods、deployments、services、secrets 等。 verbs：动作，如： get、list、watch、create、delete、update 等。 现在我们来创建一个可以读取默认命名空间default的Role，它的api版本为：rbac.authorization.k8s.io/v1 123456789apiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata: namespace: default name: develop-defualtrules:- apiGroups: [&quot;&quot;] #core api组 resources: [&quot;pods&quot;] verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;] 此时还没有api交互权限，所以应该通过kube-admin来创建 12# kubectl apply -f role.ymlrole.rbac.authorization.k8s.io/develop-defualt created RoleBinding、ClusterRoleBinding上面我们已经创建了一个带权限的角色，下一步就了解如何将角色关联到用户。角色绑定是将我们角色中定义好的权限赋予一个或者一组用户，即上图Sujbect。RoleBinding 在指定的名字空间中执行授权，而 ClusterRoleBinding 在集群范围执行授权。 图中展示了三种绑定方式，除了常规的绑定各自作用域的角色外，RoleBinding还可以绑定集群级别的ClusterRole。有啥用呢？当我们要对namespace做授权时，通常可以创建namespace中的Role进行绑定，如果管理几百个NS则需创建相应数量的NS Role，显然不是很棒，所以我们将RoleBinding绑定到集群的ClusterRole，只需几个ClusterRole就可以将几百个NS做访问控制了。 我们将上面创建的develop-defualt角色做绑定： 12345678910111213apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: name: develop-rolebinding namespace: default #授权的命名空间为defaultsubjects:- kind: User name: develop1 # 绑定develop1用户 apiGroup: rbac.authorization.k8s.ioroleRef: kind: Role name: develop-defualt #绑定Role apiGroup: rbac.authorization.k8s.io 通过admin创建RoleBinding 12# kubectl apply -f role-binding.ymlrolebinding.rbac.authorization.k8s.io/develop-rolebinding created ok，此时在使用kubectl get pod时，就能获得结果了。 1234$ kubectl get podNAME READY STATUS RESTARTS AGEweb-85549dcb84-nb67c 1/1 Running 0 12dweb-85549dcb84-z95sj 1/1 Running 0 12d ServiceAccount授权和普通用户相似，这里不再赘述。有兴趣的读者可以参考官方文档学习。 以上我们对K8S中认证和授权做了基本介绍，以及对创建一个用户并授权pod读取权限做了实践。 希望小作文对你有些许帮助，如果内容有误请指正。 您可以随意转载、修改、发布本文，无需经过本人同意。通过博客阅读：iqsing.github.io","categories":[{"name":"k8s","slug":"k8s","permalink":"https://iqsing.github.io/categories/k8s/"}],"tags":[{"name":"RBAC","slug":"RBAC","permalink":"https://iqsing.github.io/tags/RBAC/"}]},{"title":"k8s loadbalancer与ingress实践","slug":"k8s loadbalancer与ingress实践","date":"2022-01-19T16:47:21.000Z","updated":"2022-02-04T05:29:09.413Z","comments":true,"path":"2022/01/20/k8s loadbalancer与ingress实践/","link":"","permalink":"https://iqsing.github.io/2022/01/20/k8s%20loadbalancer%E4%B8%8Eingress%E5%AE%9E%E8%B7%B5/","excerpt":"","text":"k8s可以通过三种方式将集群内服务暴露到外网，分别是NodePort、LoadBalancer、Ingress，其中NodePort作为基础通信形式我们在《k8s网络模型与集群通信》中进行了介绍，这里我们主要关注LoadBalancer和Ingress LoadBalancer loadbalancer是服务暴露到因特网的标准形式，和nodeport一样我们只需在创建service是指定type为loadbalancer即可，接着Service 的通过status.loadBalancer字段将需要创建的负载均衡器信息发布供负载均衡服务创建。不过loadbalancer是云服务商”专属“，像腾讯云CLB、阿里云SLB，这样在创建service时会自动帮我们创建一个负载均衡器。 大多数云上负载均衡也是基于nodeport，他们的结构如下： 如果要在本地创建一个负载均衡器如何实现呢？ MetalLB，一个CNCF沙箱项目，使用标准路由协议(ARP/BGP)，实现裸机K8s集群的负载均衡器。 安装方式可参考官方文档：installation L2（子网）模式的结构，图源 安装后我们获得如下两个组件： metallb-system/controller deployment。用于处理IP分配的控制器。 metallb-system/speakerdaemonset。集群中每个节点启动一个协议服务守护进程。 接着添加一个configmap配置metallb IP池。 123456789101112apiVersion: v1kind: ConfigMapmetadata: namespace: metallb-system name: configdata: config: | address-pools: - name: default protocol: layer2 addresses: - 192.168.1.240-192.168.1.250 这样当我们创建一个loadbalancer类型的service时,EXTERNAL-IP将会从地址池中获取一个用于外部访问的IP 192.168.1.243 当外部流量进入时，ARP将我们的请求地址广播获取所属的service，接着k8s内部 通过iptables 规则和 kube-proxy，将流量从服务端点引导到后端。 12345678910111213141516171819202122232425262728293031323334353637#nginx_deployment_service.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: nginx namespace: metallb-systemspec: selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:latest ports: - name: http containerPort: 80---apiVersion: v1kind: Servicemetadata: namespace: metallb-system name: nginxspec: ports: - name: http port: 80 protocol: TCP targetPort: 80 selector: app: nginx type: LoadBalancer 查看service kubectl get svc 12NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEnginx LoadBalancer 10.96.243.159 192.168.1.243 80:31052/TCP 40h 测试访问：curl 192.168.1.243 1234567891011# curl 192.168.1.243&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt;html &#123; color-scheme: light dark; &#125;body &#123; width: 35em; margin: 0 auto;font-family: Tahoma, Verdana, Arial, sans-serif; &#125;&lt;/style&gt;&lt;/head&gt; 负载均衡可以建立在 OSI 网络模型的不同级别上，主要是在 L4（传输层，例如 TCP/UDP）和 L7（应用层，例如 HTTP）上。在 Kubernetes 中，Services是 L4 的抽象，LoadBalancer类型负载均衡依然有局限性，同时我们看到每创建一个service对应的负载均衡器都会消耗一个静态IP，这并不合理。当然k8s中的另一种资源对象ingress可工作在 L7 层实现应用程序协议（HTTP/HTTPS）的负载均衡。 Ingress Ingress 公开了从集群外部到集群内服务的 HTTP 和 HTTPS 路由。 流量路由由 Ingress 资源上定义的规则控制。我们可以将 Ingress 配置为服务提供外部可访问的 URL、负载均衡流量、终止 SSL/TLS，以及提供基于名称的虚拟主机等能力。 我们所说的Ingress包含两个部分： ingress k8s资源对象：流量路由规则的控制 ingress-controller控制器：控制器的实现有非常多，可参考官方文档中列表Ingress 控制器，这里我们使用k8s官方维护的控制器NGINX Ingress Controller 外部流量进入集群时先经过ingress-controller，然后根据ingress配置的路由规则将请求转发到后端service。 ingress-controlleringress-controller其实就是守护进程加一个反向代理的应用，守护进程不断监听集群中资源的变化，将ingress中的配置信息生成反向代理配置。在nginx-ingress controller中即生成nginx.conf的配置文件。 在本文中因为我们上面已经配置好了loadbalancer的服务，这样我们创建一个type为LoadBalancer的service关联这组pod，再把域名解析指向该地址，就实现了集群服务的对外暴露。当然你也可以使用NodePort、Hostnetwork的方式，感兴趣的小伙伴可以进行测试。 ingress-controller不是k8s内部组件，可以通过helm或资源清单方式安装,可查看ingress-nginx deploy 1kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.1.0/deploy/static/provider/cloud/deploy.yaml 然后我们编辑service 1kubectl edit service/ingress-nginx-controller -n ingress-nginx 修改spec.type为LoadBalancer即可。 这样我们创建好了nginx-ingress controller，下一步就要配置ingress路由规则。 ingress规则host：k8s.com 基于url的路由： /api/v1 /api/v2 这两个url分别路由到不同的service中 1234567891011121314151617181920apiVersion: extensions/v1beta1kind: Ingressmetadata: name: test namespace: training annotations: ingress.kubernetes.io/rewrite-target: /spec: rules: - host: k8s.com http: paths: - path: /api/v1 backend: serviceName: service-apiv1 servicePort: 80 - path: /api/v2 backend: serviceName: service-apiv2 servicePort: 80 ingress.kubernetes.io/rewrite-target是nginx-ingress controller的一个注解，当后端服务中暴露的 URL 与 Ingress 规则中指定的路径不同时可以通过此重定向。 查看svc可以看到此时控制器已经获得了一个EXTERNAL-IP 1234#kubectl get svc -n ingress-nginxNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEingress-nginx-controller LoadBalancer 10.96.87.23 192.168.1.245 80:32603/TCP,443:31906/TCP 621dingress-nginx-controller-admission ClusterIP 10.96.109.70 &lt;none&gt; 443/TCP 621d 现在nginx-ingress controller和ingress路由规则都有了。 我们可以进入到nginx-ingress controller pod中查看nginx.conf可以看到此时我们的ingress配置已经被生成为路由规则。 接下来就是指定我们的backend，即上面的server-apiv1/2 我们添加两个用于暴露的service和deployment，和loadbalancer中测试清单一样，我们稍稍修改一下名称即可。 123456789101112131415161718192021222324252627282930313233343536apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-apiv1 namespace: trainingspec: selector: matchLabels: app: nginx-apiv1 template: metadata: labels: app: nginx-apiv1 spec: containers: - name: nginx-apiv1 image: nginx:latest ports: - name: http containerPort: 80---apiVersion: v1kind: Servicemetadata: namespace: training name: service-apiv1spec: ports: - name: http port: 80 protocol: TCP targetPort: 80 selector: app: nginx-apiv1 type: NodePort 将nginx-apiv1换成nginx-apiv2创建出另一个service和deployment。 最后修改hosts解析k8s.com 1192.168.1.245 k8s.com 使用curl命令测试url路由（记得在pod中添加测试文件，否则虽然url进行了路由但会出现404）。 1234# curl k8s.com/api/v1/index.htmlapi v1# curl k8s.com/api/v2/index.htmlapi v2 这样我们对ingress有了初步了解，ingress的路由规则可自定项较多也比较繁杂，可通过官方文档进一步学习。 希望小作文对你有些许帮助，如果内容有误请指正。 您可以随意转载、修改、发布本文，无需经过本人同意。","categories":[{"name":"k8s","slug":"k8s","permalink":"https://iqsing.github.io/categories/k8s/"}],"tags":[{"name":"loadBalancer","slug":"loadBalancer","permalink":"https://iqsing.github.io/tags/loadBalancer/"},{"name":"ingress","slug":"ingress","permalink":"https://iqsing.github.io/tags/ingress/"}]},{"title":"k8s 理解Service工作原理","slug":"k8s 理解Service工作原理","date":"2022-01-14T16:47:21.000Z","updated":"2022-02-04T05:29:18.023Z","comments":true,"path":"2022/01/15/k8s 理解Service工作原理/","link":"","permalink":"https://iqsing.github.io/2022/01/15/k8s%20%E7%90%86%E8%A7%A3Service%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/","excerpt":"","text":"什么是service？ Service是将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。 简单来说K8s提供了service对象来访问pod。我们在《k8s网络模型与集群通信》中也说过k8s集群中的每一个Pod（最小调度单位）都有自己的IP地址，都有IP了访问起来还不简单？ 其实不然，一是k8s中pod不是持久性的，摧毁重建将获得新的IP，客户端通过变更IP来访问显然不合理。二是需要多个副本间的负载均衡。所以此时Service就冒出来了。 那么今天我们就来学习一下service，看看它是如何工作的。 Service与endpoints、pod 当我们通过API创建/修改service对象时，endpoints控制器的informer机制 Listen到service对象，然后根据service的配置的选择器创建一个endpoints对象，此对象将pod的IP、容器端口做记录并存储到etcd，这样service只要看一下自己名下的endpoints就可以知道所对应pod信息了。 且看下图： 我们在实例来看一下，先稀疏平常创建一个Deployment 123456789101112131415161718192021#deployment.ymlapiVersion: apps/v1kind: Deploymentmetadata: name: deployment-demospec: selector: matchLabels: app: nginx replicas: 3 template: metadata: labels: app: nginx spec: containers: - name: nginx image: mirrorgooglecontainers/serve_hostname ports: - containerPort: 9376 protocol: TCP serve_hostname是k8s官方提供的debug镜像，一个返回hostname的web server。这样我们创建出了标签为app=nginx的三个pod，当我们访问pod的9376时会返回hostname。 接着是service清单，我们在service中指定了选择器为app=nginx 123456789101112131415#service.yamlapiVersion: v1kind: Servicemetadata: name: service-demospec: selector: app: nginx ports: - name: default protocol: TCP #service port port: 80 #container port targetPort: 9376 这样我们获得不变的CLUSTER-IP 10.96.148.206的service 如果pod启动成功，则自动创建和service同名的endpoints记录下了三个pod的数据 service中选择器未指定标签时endpoints需要手动创建映射到service的网络地址如下： 123456789apiVersion: v1kind: Endpointsmetadata: name: servicesubsets: - addresses: - ip: 10.96.148.206 ports: - port: 9376 此时当我们不断访问service的CLUSTER-IP时： 12345678# curl 10.96.148.206:80deployment-demo-7d94cbb55f-8mmxb# curl 10.96.148.206:80deployment-demo-7d94cbb55f-674ns# curl 10.96.148.206:80deployment-demo-7d94cbb55f-lfrm8# curl 10.96.148.206:80deployment-demo-7d94cbb55f-8mmxb 可以看到此时请求已被路由到后端pod，返回hostname，并且负载均衡方式是Round Robin即轮询模式。 通过上面介绍我们好像摸到了Service其中的门道，接下来是流量到底如何通过service进入pod的？ Service与kube-proxy 涉及到流量当然是kube-proxy登场了！ kube-proxy 是集群中每个节点上运行的网络代理， 实现 Kubernetes 服务（Service） 概念的一部分。用于处理单个主机子网划分并向外部世界公开服务。它跨集群中的各种隔离网络将请求转发到正确的 pod/容器。 kube-proxy 维护节点上的网络规则。这些网络规则允许从集群内部或外部的网络会话与 Pod 进行网络通信。 如下图所示： kube-proxy 通过 Informer知道了Service、endpoints对象的创建，然后把service身上的CLUSTER-IP 和端口已经端点信息拿出来，创建iptable NAT规则做转发或通过ipvs模块创建VS服务器，这样经过CLUSTER-IP的流量都被转发到后端pod。 iptables模式我们先查看nat表的OUTPUT链，存在kube-proxy创建的KUBE-SERVICE链 iptables -nvL OUTPUT -t nat 在KUBE-SERVICES链中有一条目的地为10.96.148.206即CLUSTER-IP地址跳转到KUBE-SVC-EJUV4ZBKPDWOZNF4 iptables -nvL KUBE-SERVICES -t nat |grep service-demo 接着是查看这条链，以1/3的概率跳转到其中一条 iptables -nvL KUBE-SVC-EJUV4ZBKPDWOZNF4 -t nat 最后KUBE-SEP-BTFJGISFGMEBGVUF链终于找到了DNAT规则 iptables -nvL KUBE-SEP-BTFJGISFGMEBGVUF -t nat 即将请求通过DNAT发送到地址100.101.184.61:9376也就是我们其中一个Pod。 IPVS模式与iptalbes模式相比，IPVS模式工作在内核态，在同步代理规则时具有更好的性能，同时提高网络吞吐量为大型集群提供了更好的可扩展性。 IPVS 模式在工作时，当我们创建了前面的 Service 之后，kube-proxy 首先会在宿主机上创建一个虚拟网卡kube-ipvs0，并为它分配 Service VIP 作为 IP 地址，如图 接着kube-proxy通过Linux的IPVS模块为这个 IP 地址添加三个 IPVS 虚拟主机，并设置这三个虚拟主机之间使用轮询模式 来作为负载均衡策略。 通过ipvsadm查看 ipvsadm -ln |grep -C 5 10.96.148.206 可以看到虚拟server的IP即是Pod的地址，这样流量即向了目的地Pod。 以上我们先认识了Service这个API对象，接着讲到了service与endpoints和pod的关联，然后是service与kube-proxy的关系，以及kube-proxy的两种模式如何通过service的IP创建iptables、IPVS规则将流量转发到Pod。 希望小作文对你有些许帮助，如果内容有误请指正。 您可以随意转载、修改、发布本文，无需经过本人同意。通过博客阅读：iqsing.github.io","categories":[{"name":"k8s","slug":"k8s","permalink":"https://iqsing.github.io/categories/k8s/"}],"tags":[{"name":"service","slug":"service","permalink":"https://iqsing.github.io/tags/service/"}]},{"title":"k8s env、configmap、secret外部数据加载配置","slug":"k8s env、configmap、secret外部数据加载配置","date":"2022-01-09T16:47:21.000Z","updated":"2022-02-04T05:28:23.080Z","comments":true,"path":"2022/01/10/k8s env、configmap、secret外部数据加载配置/","link":"","permalink":"https://iqsing.github.io/2022/01/10/k8s%20env%E3%80%81configmap%E3%80%81secret%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E9%85%8D%E7%BD%AE/","excerpt":"","text":"K8s提供了多种外部数据注入容器的方式，今天我们主要学习环境变量、ConfigMap以及Secret的使用和配置。 环境变量 在docker项目中，对一个容器添加环境变量可以在容器创建时通过-e ENV=name方式加载。而k8s在创建 Pod 时，也提供了其下容器环境变量配置的能力。 我们可以通过配置清单中的 env 及 envFrom（来自外部配置） 字段来设置环境变量。 比如如下的yaml 12345678910111213141516171819202122232425262728293031323334#busybox-deployment.ymlapiVersion: apps/v1kind: Deploymentmetadata: name: busybox-deploymentspec: selector: matchLabels: app: busybox replicas: 1 template: metadata: labels: app: busybox spec: containers: - name: busybox image: busybox:latest resources: limits: memory: 20Mi env: - name: DEMO_VERSION value: demov1 - name: DEMO_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: DEMO_CONT_MEM valueFrom: resourceFieldRef: containerName: busybox resource: limits.memory command: [&#x27;top&#x27;] 在清单中我们配置了三个环境变量： DEMO_VERSION:直接添加变量值demov1 DEMO_POD_NAME:结合valueFrom中fieldRef获取pod名称字段metadata.name DEMO_CONT_MEM:结合valueFrom中resourceFieldRef获取容器资源字段limits.memory 此时我们创建pod进入容器后通过printenv命令可以查看到环境变量已经被加载： 12345#kubectl exec busybox-deployment-5bb768546c-jbsmz -- printenvDEMO_POD_NAME=busybox-deployment-5bb768546c-jbsmzDEMO_CONT_MEM=20971520 DEMO_VERSION=demov1 valueFrom中其他字段如下待会我们会用到，需要时可参考官方API文档：envvar-v1-core 注意： 环境变量将覆盖容器镜像中指定的所有环境变量。 ConfigMap ConfigMap 是一种 API 对象，用来将非机密性的数据保存到键值对中。使用时， Pods可以将其用作环境变量、命令行参数或者存储卷中的配置文件。 1、用于环境变量Configmap 用于配置环境变量的好处是可以将环境配置信息和容器镜像解耦，便于应用配置的修改。 我们可以快速的创建出一个configmap如下： 1234567#busybox-configmap.yamlapiVersion: v1kind: ConfigMapmetadata: name: busybox-configmapdata: DEMO_VERSION: &quot;demov2&quot; configmap使用 data（UTF-8字节序列） 和 binaryData（二进制数据base64 编码的字串） 字段创建键值对做数据存储。 接着使用调整我们deployment中的envDEMO_VERSION的字段如下： 12345- name: DEMO_VERSION valueFrom: configMapKeyRef: name: busybox-configmap key: DEMO_VERSION configMapKeyRef如API所说的选择一个configmap 同样创建后查看 1234# kubectl exec pod/busybox-deployment-64c678977f-zjnhb -- printenvDEMO_VERSION=demov2... 这样我们只需要维护这个configmap即可，不过通过环境变量引用configmap时也是不支持热更新，环境变量只在容器创建时加载，所以你需要触发一次deployment的滚动更新。 2、挂载配置信息显然从名字上可以看出configmap并不是为环境变量而生。我们可以将configmap中key作文文件挂载到容器中，我们创建如下清单： 1234567891011121314151617181920apiVersion: v1kind: ConfigMapmetadata: name: busybox-configmapdata: DEMO_VERSION: &quot;demov3&quot; game.properties: | enemies=aliens lives=3 enemies.cheat=true enemies.cheat.level=noGoodRotten secret.code.passphrase=UUDDLRLRBABAS secret.code.allowed=true secret.code.lives=30 ui.properties: | color.good=purple color.bad=yellow allow.textmode=true how.nice.to.look=fairlyNice 相当于此时我们获得三个key文件，接下来我们就可以通过volume挂载了。 123456789...volumeMounts:- name: config-volume mountPath: /etc/configvolumes:- name: config-volume configMap: name: busybox-configmap... 在volume中configmap字段指定我们的busybox-configmap，创建后查看/etc/config 1234$ kubectl exec busybox-deployment-87b6c7bd7-ljcfr -- ls /etc/config/DEMO_VERSIONgame.propertiesui.properties 当卷中使用的 ConfigMap 被更新时，所投射的键最终也会被更新。 kubelet 组件会在每次周期性同步时检查所挂载的 ConfigMap 是否为最新。即k8s的watch机制。 Secret 与ConfigMap类似，k8s提供了另一种API对象Secret用于存储机密信息，我们可以使用Secret对象存储敏感信息例如密码、令牌或密钥，这样在应用程序代码中解耦机密数据。 创建一个Sercet 123456789apiVersion: v1kind: Secretmetadata: name: mysecrettype: Opaquedata: password: cGFzc3dkstringData: username: k8s data 字段用来存储 base64 编码的任意数据，我们可以通过base64命令生成编码。 stringData则允许 Secret 使用未编码的字符串，只用于写，无法直接读取明文字段。 123456789101112$ kubectl get secret mysecret -o yamlapiVersion: v1data: password: cGFzc3dk username: azhz...$ kubectl describe secret mysecret...Data====password: 6 bytesusername: 3 bytes 这样在kubectl get 和 kubectl describe 中默认不显示 Secret 的内容。 这是为了防止 Secret 意外地暴露给旁观者或者保存在终端日志中。 Kubernetes 提供若干种内置的Secret类型，用于一些常见的使用场景。 针对这些类型，Kubernetes 所执行的合法性检查操作以及对其所实施的限制各不相同。 内置类型 用法 Opaque 用户定义的任意数据 kubernetes.io/service-account-token 服务账号令牌 kubernetes.io/dockercfg ~/.dockercfg 文件的序列化形式 kubernetes.io/dockerconfigjson ~/.docker/config.json 文件的序列化形式 kubernetes.io/basic-auth 用于基本身份认证的凭据 kubernetes.io/ssh-auth 用于 SSH 身份认证的凭据 kubernetes.io/tls 用于 TLS 客户端或者服务器端的数据 bootstrap.kubernetes.io/token 启动引导令牌数据 类型说明可参考官方文档：secret，当然也可以通过Opaque自定义的实现内置类型。 这里我们以类型kubernetes.io/ssh-auth为例尝试使用Secret,kubernetes.io/ssh-auth 用来存放 SSH 身份认证中 所需要的凭据。使用这种 Secret 类型时，我们必须在其 data （或 stringData） 字段中提供一个 ssh-privatekey 键值对，作为要使用的 SSH 凭据。 创建如下的yaml： 12345678apiVersion: v1kind: Secretmetadata: name: secret-ssh-authtype: kubernetes.io/ssh-authdata: ssh-privatekey: | PRIVATEKEY_STINGS.. #base64编码数据 创建后可以查看到类型和key名称。 1234567891011$ kubectl describe secret/secret-ssh-authName: secret-ssh-authNamespace: defaultLabels: &lt;none&gt;Annotations: &lt;none&gt;Type: kubernetes.io/ssh-authData====ssh-privatekey: 2626 bytes 接着创建用于加载secret的pod 12345678910111213141516apiVersion: v1kind: Podmetadata: name: pod-secretspec: containers: - name: pod-secret image: nginx volumeMounts: - name: secret-volume mountPath: &quot;/etc/ssh/&quot; readOnly: true volumes: - name: secret-volume secret: secretName: secret-ssh-auth 此时容器中已加载到secretName中的ssh-privatekey项 12$ kubectl exec pod/pod-secret -- ls /etc/sshssh-privatekey 这样我们可以通过此key来做ssh相关的认证。 和configmap一样，secret也可用于环境变量配置。通过secretRef字段引入secret 12345...envFrom:- secretRef: name: mysecret... 以上secret使用仅做学习，生产中请排查以下安全问题，更多secret内容参考官方文档：Secret 安全问题： 当部署与 Secret API 交互的应用程序时，应使用 鉴权策略， 例如 RBAC，来限制访问。 API 服务器上的 Secret 数据以纯文本的方式存储在 etcd 中，因此： 管理员应该为集群数据开启静态加密（要求 v1.13 或者更高版本）。 管理员应该限制只有 admin 用户能访问 etcd； API 服务器中的 Secret 数据位于 etcd 使用的磁盘上,不再使用secret应该被删除。 如果 etcd 运行在集群内，管理员应该确保 etcd 之间的通信使用 SSL/TLS 进行加密。 如果将 Secret 数据编码为 base64 的清单（JSON 或 YAML）文件，共享该文件或将其检入代码库，该密码将会被泄露。 Base64 编码不是一种加密方式，应该视同纯文本。 应用程序在从卷中读取 Secret 后仍然需要保护 Secret 的值，例如不会意外将其写入日志或发送给不信任方。 可以创建使用 Secret 的 Pod 的用户也可以看到该 Secret 的值。即使 API 服务器策略不允许用户读取 Secret 对象，用户也可以运行 Pod 导致 Secret 暴露。 希望小作文对你有些许帮助，如果内容有误请指正。 您可以随意转载、修改、发布本文，无需经过本人同意。 通过博客阅读：iqsing.github.io","categories":[{"name":"k8s","slug":"k8s","permalink":"https://iqsing.github.io/categories/k8s/"}],"tags":[{"name":"configmap","slug":"configmap","permalink":"https://iqsing.github.io/tags/configmap/"},{"name":"secret","slug":"secret","permalink":"https://iqsing.github.io/tags/secret/"}]},{"title":"k8s网络模型与集群通信","slug":"k8s网络模型与集群通信","date":"2021-11-16T04:47:21.000Z","updated":"2022-02-04T05:16:11.998Z","comments":true,"path":"2021/11/16/k8s网络模型与集群通信/","link":"","permalink":"https://iqsing.github.io/2021/11/16/k8s%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%B8%8E%E9%9B%86%E7%BE%A4%E9%80%9A%E4%BF%A1/","excerpt":"","text":"在k8s中，我们的应用会以pod的形式被调度到各个node节点上，在设计集群如何处理容器之间的网络时是一个不小的挑战，今天我们会从pod（应用）通信来展开关于k8s网络的讨论。 小作文包含如下内容： k8s网络模型与实现方案 pod内容器通信 pod与pod通信 pod与service通信 外网与service通信 k8s网络模型与实现方案k8s集群中的每一个Pod（最小调度单位）都有自己的IP地址，即ip-per-pod模型。 在ip-per-pod模型中每一个pod在集群中保持唯一性，我们不需要显式地在每个 Pod 之间创建链接， 不需要处理容器端口到主机端口之间的映射。从端口分配、命名、服务发现、 负载均衡、应用配置和迁移的角度来看，Pod 可以被视作独立虚拟机或者物理主机。 如下图，从表面上来看两个容器在docker网络与k8s网络中与client通信形式。 k8s是一套庞大的分布式系统，为了保持核心功能的精简（模块化）以及适应不同业务用户的网络环境，k8s通过CNI(Container Network Interface)即容器网络接口集成各种网络方案。这些网络方案必须符合k8s网络模型要求： 节点上的 Pod 可以不通过 NAT 和其他任何节点上的 Pod 通信 节点上的代理（比如：系统守护进程、kubelet）可以和节点上的所有Pod通信 备注：仅针对那些支持 Pods 在主机网络中运行的平台（比如：Linux）： 那些运行在节点的主机网络里的 Pod 可以不通过 NAT 和所有节点上的 Pod 通信 如此操作，是不是有点像美团？将配送业务外包（CNI）给三方公司（实现方案），骑手是通过哪种飞机大炮（网络）送餐的我不管，只要符合准时、不撒漏（模型要求）等相关规矩这就是一次合格的配送。 CNI 做两件事，容器创建时的网络分配，和当容器被删除时释放网络资源。 常用的 CNI 实现方案有 Flannel、Calico、Weave以及各种云厂商根据自身网络推出的CNI插件如华为的 CNI-Genie、阿里云Terway。关于各实现方案的原理不是本次讨论重点，有机会单独写一篇。 pod内容器通信Pod内容器非常简单，在同一个 Pod 内，所有容器共享存储、网络即使用同一个 IP 地址和端口空间，并且可以通过 localhost 发现对方。Pod 使用了一个中间容器 Infra，Infra 在 Pod 中首先被创建，而其他容器则通过 Join Network Namespace 的方式与 Infra 容器关联在一起。 我们有一个pod包含busybox、nginx这两个容器 123kubectl get pod -n trainingNAME READY STATUS RESTARTS AGEpod-localhost-765b965cfc-8sh76 2/2 Running 0 2m56s 在busybox中使用telnet连接nginx容器的 80端口看看。 1234kubectl exec -it pod-localhost-765b965cfc-8sh76 -c container-si1nrb -n training -- /bin/sh# telnet localhost 80Connected to localhost 一个pod有多个容器时可以通过-c指定进入的容器名（通过describe查看容器名称），显然通过localhost就可以轻松访问到同一个pod中的nginx容器80端口。这也是在许多关系密切的应用中通常会部署在同一个pod中。 pod与pod通信 pod在同一主机 我们通过node选择器将两个pod调度到同一个node中 1234...nodeSelector: kubernetes.io/hostname: node2... 两个容器分别获得一个IP地址，同样通过IP地址双方网络正常互通。 123456789# kubectl get pod -o wide -n training NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESpod-to-pod-64444686ff-w7c4g 1/1 Running 0 6m53s 100.82.98.206 node2 &lt;none&gt; &lt;none&gt;pod-to-pod-busybox-7b9db67bc6-tl27c 1/1 Running 0 5m3s 100.82.98.250 node2 &lt;none&gt; &lt;none&gt;# kubectl exec -it pod-to-pod-busybox-7b9db67bc6-tl27c -n training -- /bin/sh/# telnet 100.82.98.206 80Connected to 100.82.98.206 同一主机网络的pod互通和我们之前学习的docker bridge相似，通过linux网桥添加虚拟设备对veth pair连接容器和主机主机命名空间。具体可查看文章《docker容器网络bridge》。 我们把之前的图拿过来，在k8s中只不过把灰色部分替换成CNI方案实现。 pod在不同主机 此时我们的pod分布如下： 12345678910kubectl get pod -o wide -n training NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESpod-to-pod-64444686ff-w7c4g 1/1 Running 0 104m 100.82.98.206 node2 &lt;none&gt; pod-to-pod-busybox-node2-6476f7b7f9-mqcw9 1/1 Running 0 42s 100.91.48.208 node3 &lt;none&gt; # kubectl exec -it pod-to-pod-busybox-node2-6476f7b7f9-mqcw9 -n training -- /bin/sh/ # telnet 100.82.98.206 80Connected to 100.82.98.206 pod在不同主机的通信依赖于CNI插件，这里我们以Calico为例的做简单了解，从Calico架构图中可以看到每个node节点的自身依然采用容器网络模式，Calico在每个节点都利用Linux 内核实现了一个高效的虚拟路由器vRouter来负责数据转发。每个虚拟路由器将路由信息广播到网络中，并添加路由转发规则。同时基于iptables还提供了丰富的网络策略，实现k8s的Network Policy策略，提供容器间网络可达性限制的功能。 简单理解就是通过在主机上启动虚拟路由器(calico node)，将每个主机作为路由器使用实现互联互通的网络拓扑。 Calico节点组网时可以直接利用数据中心的网络结构(L2或者L3)，不需要额外的NAT、隧道或者Overlay Network，没有额外的封包解包，能够节约CPU运算，提高网络效率。 pod与service通信我们知道在k8s中容器随时可能被摧毁，pod的IP显然不是持久的，会随着扩展或缩小应用规模、或者应用程序崩溃以及节点重启等而消失和出现。service 设计就是来处理这个问题。service可以管理一组 Pod 的状态，允许我们跟踪一组随时间动态变化的 Pod IP 地址。而客户端只需要知道service这个不变的虚拟IP就可以了。 我们先来看看典型的service与pod使用，我们创建了一个service，标签选择器为app:nginx，将会路由到app=nginx标签的Pod上。 123# kubectl get service -n trainingNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEtraining-service ClusterIP 10.96.229.238 &lt;none&gt; 8881/TCP 10m Service对外暴露的端口8881,这样在集群的中的pod即可通过8881访问到与service 绑定的label为app=nginx的pod 12345678kubectl run -it --image nginx:alpine curl --rm /bin/sh/ # curl 10.96.229.238:8881&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt;... 其实大多数时候在自动化部署服务时并不知道service ip，所以另一种常见方式通过DNS进行域名解析后，可以使用“ServiceName:Port”访问Service，可以自己尝试一下。 service 是如何做到服务发现的？ Endpoints是k8s中的一种资源对象，k8s通过Endpoints监控到Pod的IP，service又关联Endpoints从而实现Pod的发现。大致如下图所示，service的发现机制我们会在后面文章中做深入了解。 外网与service通信其实所谓外网通信也是service的表现形式。 service几种类型和不同用途。 ClusterIP：用于在集群内部互相访问的场景，通过ClusterIP访问Service，即我们上面所说的pod与service。 NodePort：用于从集群外部访问的场景，通过节点上的端口访问Service。 LoadBalancer：用于从集群外部访问的场景，其实是NodePort的扩展，通过一个特定的LoadBalancer访问Service，这个LoadBalancer将请求转发到节点的NodePort，而外部只需要访问LoadBalancer。 None：用于Pod间的互相发现，这种类型的Service又叫Headless Service。 我们先来看NodePort： 我们在service中指定type: NodePort创建出的service将会包含一个在所有node 开放的端口30678，这样我们访问任意节点IP:30678即可访问到我们的pod 1234567891011# kubectl get service -n trainingNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEtraining-service NodePort 10.96.229.238 &lt;none&gt; 8881:30678/TCP 55m# curl 192.168.1.86:30678&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt;.... LoadBalancer类型和它名字一样，为负载均衡而生。它的结构如下图所示， LoadBalancer本身不是属于Kubernetes的组件，如果使用云厂商的容器服务。通常会提供一套他们的负载均衡服务比如阿里云ACK的SLB、华为云的ELB等等。Service是基于四层TCP和UDP协议转发的，而k8s 另外一种资源对象Ingress可以基于七层的HTTP和HTTPS协议转发，可通过域名和路径做到更细粒度的划分，这是后话。 希望小作文对你有些许帮助，如果内容有误请指正。 您可以随意转载、修改、发布本文，无需经过本人同意。 通过博客阅读：iqsing.github.io","categories":[{"name":"k8s","slug":"k8s","permalink":"https://iqsing.github.io/categories/k8s/"}],"tags":[{"name":"network","slug":"network","permalink":"https://iqsing.github.io/tags/network/"}]},{"title":"k8s关于Job与Cronjob","slug":"k8s 关于Job与Cronjob","date":"2021-10-23T16:47:21.000Z","updated":"2022-02-04T05:16:11.944Z","comments":true,"path":"2021/10/24/k8s 关于Job与Cronjob/","link":"","permalink":"https://iqsing.github.io/2021/10/24/k8s%20%E5%85%B3%E4%BA%8EJob%E4%B8%8ECronjob/","excerpt":"","text":"在Kubernetes 中通过创建工作负载资源 Job 可完成大型计算以及一些批处理任务。比如 Job 转码文件、获取部分文件和目录，机器学习中的训练任务等。这篇小作文我们一起来了解 k8s 中关于 job、cronjob 的内容。 Job创建我们可以通过API版本 batch/v1创建出一个简单的k8s Job 12345678910111213#new-job.ymlapiVersion: batch/v1kind: Jobmetadata: name: command-jobspec: template: spec: containers: - name: command-job image: busybox command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;sleep 5;echo &#x27;job one&#x27;&quot;] restartPolicy: Never Job对象 将会启动一个pod用于完成我们的工作–睡眠5s，接着输出 job one 应用job定义，查看job工作工作状态： 12$ kubectl apply -f new-job.yml job.batch/command-job created 任务完成后，pod状态被置为Completed： 通过logs查看我们的任务执行结果： Job重启与失败认定在上面我们的例子中，job pod顺利的完成了我们的任务。当pod在执行作业时，容器可能会由于一些原因启动失败，比如进程以非0代码退出或超出内存限制等。在pod模板中可以通过restartPolicy控制job pod的重启策略。 重启策略（restartPolicy）： Never：pod启动失败时不会重启，而是通过job-controller重新创建pod供节点调度。 OnFailure：pod将会在节点重启执行任务。 失败回退策略（backoffLimit）： 当Job pod 经过多次重启无果，显然我们应该认定这个Job是一个失败任务，默认失败认定重启次数为6，我们可以通过在spec中添加backoffLimit来改变这一认定。 我们调整new-job.yml如下： 1234567891011121314#new-job.ymlapiVersion: batch/v1kind: Jobmetadata: name: command-job-twospec: template: spec: containers: - name: command-job-two image: busybox command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;sleep 5;echo &#x27;job two&#x27;;exit 1&quot;] restartPolicy: Never backoffLimit: 2 我们通过describe查看创建的Job job-controller经过2次重建pod达到阈值，job-controller认定本次Job为失败工作流。 在重启策略为Never时，认定失败的Job会将pod遗留在节点上。 Job 期限与清理除了Job执行结束与重启失败认定的Job 终止外还可以通过配置活跃期限（activeDeadlineSeconds）来自动停止Job任务。 我们可以为 Job 的 .spec.activeDeadlineSeconds 设置一个秒数值。 该值适用于 Job 的整个生命期，无论 Job 创建了多少个 Pod。 一旦 Job 运行时间达到 activeDeadlineSeconds 秒，其所有运行中的 Pod 都会被终止，并且 Job 的状态更新为 type: Failed 及 reason: DeadlineExceeded。 注意 Job 的 .spec.activeDeadlineSeconds 优先级高于其 .spec.backoffLimit 设置。 因此，如果一个 Job 正在重试一个或多个失效的 Pod，该 Job 一旦到达 activeDeadlineSeconds 所设的时限即不再部署额外的 Pod，即使其重试次数还未 达到 backoffLimit 所设的限制。 调整new-job.yml如下： 12345678910111213141516#new-job.ymlapiVersion: batch/v1kind: Jobmetadata: name: command-job-threespec: template: spec: containers: - name: command-job-three image: busybox command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;sleep 50;echo &#x27;job three&#x27;&quot;] restartPolicy: Never backoffLimit: 2 activeDeadlineSeconds: 10 虽然是50s的任务，但是由于activeDeadlineSeconds的限制,Job运行10s后被终止 清理job和终止相似，我们可以通过添加spec.ttlSecondsAfterFinished使Job在任务完成后一段时间内被清理，读者感兴趣可动手尝试一下。 Job 任务类型 非并行 Job 通常只启动一个 Pod，除非该 Pod 失败，Pod中应用成功运行完成即视为Job任务为完成状态，我们上面讨论的任务即属于此类。 **并行 Job ** 指定任务数的并行 Job 通过spec.completions指定任务数，一旦所有 Pod 成功完成它的任务. 作业将完成。 我们添加一个new-jobs.yml，并指定completions为3 1234567891011121314apiVersion: batch/v1kind: Jobmetadata: name: command-jobsspec: template: spec: containers: - name: command-jobs image: busybox command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;sleep 50;echo &#x27;jobs &#x27;&quot;] restartPolicy: Never backoffLimit: 2 completions: 3 当3个Pod都运行完成时，Job状态为成功执行。 我们可以从Job pod 运行过程中看到次模式中Pod 创建存在先后顺序，即需要等待一个job完成后，开启下一个Job的运行。 工作队列式的并行 Job 一旦一个 Pod 成功终止则所有 Pod 都都终止，此时Job 成功完成。 修改new-jobs.yml，并添加parallelism使其并行数为5 123456789101112131415apiVersion: batch/v1kind: Jobmetadata: name: command-jobsspec: template: spec: containers: - name: command-jobs image: busybox command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;sleep 50;echo &#x27;jobs &#x27;&quot;] restartPolicy: Never backoffLimit: 2 parallelism: 5 此类Job Pod在同一时间创建和结束。 Cronjob周期性任务CronJob 用于执行周期性的动作，例如备份、邮件、报告生成等。 cron时间配置与linux crontab相似。 12345678910# ┌────────────────── 时区 (可选)# | ┌───────────── 分钟 (0 - 59)# | │ ┌───────────── 小时 (0 - 23)# | │ │ ┌───────────── 月的某天 (1 - 31)# | │ │ │ ┌───────────── month (1 - 12)# | │ │ │ │ ┌───────────── 周的某天 (0 - 6)（周日到周一；在某些系统上，7 也是星期日）# | │ │ │ │ │ # | │ │ │ │ │# | │ │ │ │ │# CRON_TZ=UTC * * * * * 添加cronjob.yml如下： 12345678910111213141516#cronjob.ymlapiVersion: batch/v1beta1kind: CronJobmetadata: name: cronjobspec: schedule: &quot;*/1 * * * *&quot; jobTemplate: spec: template: spec: containers: - name: cronjob image: busybox command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;date&quot;] restartPolicy: Never 我们通过cronjob没隔一分钟打印一次日期。 查看cronjob信息： 通过logs查看任务结果： 12[docker@localhost yml]$ kubectl logs cronjob-1635010680-n5gxjSat Oct 23 17:38:15 UTC 2021 cronjob可以自动清理任务，默认保留3次成功的任务，我们可以通过添加.spec.successfulJobsHistoryLimit改变保留的历史任务信息即Pod。 以上我们将k8s中Job、Cronjob涉及的大部分内容进行了介绍。 参考： Job Running Automated Tasks with a CronJob 希望小作文对你有些许帮助，如果内容有误请指正。 您可以随意转载、修改、发布本文，无需经过本人同意。亦可通过博客阅读本文：iqsing.github.io","categories":[{"name":"k8s","slug":"k8s","permalink":"https://iqsing.github.io/categories/k8s/"}],"tags":[{"name":"job","slug":"job","permalink":"https://iqsing.github.io/tags/job/"}]},{"title":"k8s DaemonSet 介绍与实例","slug":"k8s DaemonSet 介绍与实例","date":"2021-10-20T16:47:21.000Z","updated":"2022-02-04T05:16:11.918Z","comments":true,"path":"2021/10/21/k8s DaemonSet 介绍与实例/","link":"","permalink":"https://iqsing.github.io/2021/10/21/k8s%20DaemonSet%20%E4%BB%8B%E7%BB%8D%E4%B8%8E%E5%AE%9E%E4%BE%8B/","excerpt":"","text":"我们之前说k8s中使用deployment、statefulset工作负载资源来分别维护无状态和有状态应用。这篇小作文我们会学习如何使用DaemonSet来维护一个守护进程（应用）。 一、DaemonSet是什么？DaemonSet 是一个确保全部或者某些节点上必须运行一个 Pod的工作负载资源（守护进程），当有节点加入集群时， 也会为他们新增一个 Pod。 下面是常用的使用案例： 集群守护进程，如Kured、node-problem-detector 日志收集守护进程，如fluentd、logstash 监控守护进程，如promethues node-exporter 通过创建DaemonSet 可以确保 守护进程pod 被调度到每个可用节点上运行。 二、DaemonSet 如何工作？DaemonSet 是由控制器（controller manager）管理的 Kubernetes 工作资源对象。我们通过声明一个想要的daemonset状态，表明每个节点上都需要有一个特定的 Pod。协调控制回路会比较期望状态和当前观察到的状态。如果观察到的节点没有匹配的 Pod，DaemonSet controller将自动创建一个。可以参考之前《k8s工作流程详解》 在这个过程包括现有节点和所有新创建的节点。不过DaemonSet 控制器创建的 Pod 会被Kubernetes 调度器忽略，即DaemonSet Pods 由 DaemonSet 控制器创建和调度。这样带来的两个微妙的问题： Pod 行为的不一致性：正常 Pod 在被创建后等待调度时处于 Pending 状态， DaemonSet Pods 创建后不会处于 Pending 状态下。 Pod 抢占行为由默认调度器处理。启用抢占后，DaemonSet 控制器将在不考虑 Pod 优先级和抢占 的情况下制定调度决策。 所以在k8s v1.12以后DaemonSet Controller 将会向 DaemonSet 的 Pod 添加 .spec.nodeAffinity 字段，而不是 .spec.nodeName 字段，并进一步由 kubernetes 调度器将 Pod 绑定到目标节点。如果 DaemonSet 的 Pod 已经存在了 nodeAffinity 字段，该字段的值将被替换。 12345678nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchFields: - key: metadata.name operator: In values: - target-host-name daemonset pod的默认容忍规则如下： DaemonSet 默认在每个节点上创建一个 Pod。当然也可以使用节点选择器来限制可接受节点的数量。DaemonSet 控制器将仅在与 YAML 文件中预定义的nodeSelector字段匹配的节点上创建Pod。我们在下面会使用到。 三、DaemonSet实例创建DaemonSet我们只需要将前面deployment中的kind调整为DaemonSet 就可以创建出一个DaemonSet守护进程 123456789101112131415161718apiVersion: apps/v1 kind: DaemonSet metadata: name: my-daemonsetspec: selector: matchLabels: app: my-daemon template: metadata: labels: app: my-daemon spec: containers: - name: daemonset-container image: httpd ports: - containerPort : 80 通过apply应用后查看资源状态 123$ kubectl get daemonsetNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGEmy-daemonset 1 1 1 1 1 &lt;none&gt; 10m 由于我们minikube只有一个node 所以只建立了一个副本，在节点通过get查看到已创建出这个daemonset pod 123$ kubectl get pod NAME READY STATUS RESTARTS AGEmy-daemonset-97z2g 1/1 Running 0 10m 在daemonset资源状态中可以看到NODE SELECTOR的值为none，显然我们可以通过在pod模板中添加nodeSelector使DaemonSet 控制器仅在与Node 选择算符匹配的节点上创建出pod，接下来我们添加一个nodeSelector 1234567891011121314151617181920apiVersion: apps/v1 kind: DaemonSet metadata: name: my-daemonsetspec: selector: matchLabels: app: my-daemon template: metadata: labels: app: my-daemon spec: containers: - name: daemonset-container image: httpd ports: - containerPort : 80 nodeSelector: kubernetes.io/hostname: minikube 这样我们的pod只会在hostname为minikube的Node上创建DaemonSet守护进程的pod 123$ kubectl get daemonsetNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGEmy-daemonset 1 1 1 1 1 kubernetes.io/hostname=minikube 30m 除了通过nodeSelector来控制节点调度外，还可以通过上面提到的容忍策略即tolerations使daemonset pod 调度到“非正常“Node。 我们可以来看一个fluentd的官方elasticsearch daemonset 源文件地址：fluentd-daemonset-elasticsearch.yaml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182apiVersion: apps/v1kind: DaemonSetmetadata: name: fluentd namespace: kube-system labels: k8s-app: fluentd-logging version: v1spec: selector: matchLabels: k8s-app: fluentd-logging version: v1 template: metadata: labels: k8s-app: fluentd-logging version: v1 spec: tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule containers: - name: fluentd image: fluent/fluentd-kubernetes-daemonset:v1-debian-elasticsearch env: - name: FLUENT_ELASTICSEARCH_HOST value: &quot;elasticsearch-logging&quot; - name: FLUENT_ELASTICSEARCH_PORT value: &quot;9200&quot; - name: FLUENT_ELASTICSEARCH_SCHEME value: &quot;http&quot; # Option to configure elasticsearch plugin with self signed certs # ================================================================ - name: FLUENT_ELASTICSEARCH_SSL_VERIFY value: &quot;true&quot; # Option to configure elasticsearch plugin with tls # ================================================================ - name: FLUENT_ELASTICSEARCH_SSL_VERSION value: &quot;TLSv1_2&quot; # X-Pack Authentication # ===================== - name: FLUENT_ELASTICSEARCH_USER value: &quot;elastic&quot; - name: FLUENT_ELASTICSEARCH_PASSWORD value: &quot;changeme&quot; # Logz.io Authentication # ====================== - name: LOGZIO_TOKEN value: &quot;ThisIsASuperLongToken&quot; - name: LOGZIO_LOGTYPE value: &quot;kubernetes&quot; resources: limits: memory: 200Mi requests: cpu: 100m memory: 200Mi volumeMounts: - name: varlog mountPath: /var/log # When actual pod logs in /var/lib/docker/containers, the following lines should be used. # - name: dockercontainerlogdirectory # mountPath: /var/lib/docker/containers # readOnly: true # When actual pod logs in /var/log/pods, the following lines should be used. - name: dockercontainerlogdirectory mountPath: /var/log/pods readOnly: true terminationGracePeriodSeconds: 30 volumes: - name: varlog hostPath: path: /var/log # When actual pod logs in /var/lib/docker/containers, the following lines should be used. # - name: dockercontainerlogdirectory # hostPath: # path: /var/lib/docker/containers # When actual pod logs in /var/log/pods, the following lines should be used. - name: dockercontainerlogdirectory hostPath: path: /var/log/pods 特别之处在于，为了收集master节点上的pod日志，将会容忍fluentd调度到master节点。其中tolerations如下 Daemon Pods 通信与 DaemonSet 中的 Pod 进行通信的几种模式如下： 推送（Push）：配置 DaemonSet 中的 Pod，将更新发送到另一个服务，例如统计数据库。 NodeIP 和已知端口：DaemonSet 中的 Pod 可以使用 hostPort，从而可以通过节点 IP 访问到 Pod。客户端能通过某种方法获取节点 IP 列表，并且基于此也可以获取到相应的端口。比如prometheus的node-exporter。 DNS：创建具有相同 Pod 选择算符的 无头服务 通过使用 endpoints 资源或从 DNS 中检索到多个 A 记录来发现 DaemonSet。 DaemonSet 更新如果节点的标签被修改，DaemonSet 将立刻向新匹配上的节点添加 Pod， 同时删除不匹配的节点上的 Pod。 可以删除一个 DaemonSet。如果使用 kubectl 指定 --cascade=orphan 选项， 则 Pod 将被保留在节点上。接下来如果创建使用相同选择算符的新 DaemonSet， 新的 DaemonSet 会收养已有的 Pod。 如果有 Pod 需要被替换，DaemonSet 会根据其 updateStrategy 来替换。 比如prometheus中的node-exporter 以上是关于k8s中的DaemonSet相关内容。 参考： daemonset node-affinity node-exporter-daemonset 希望小作文对你有些许帮助，如果内容有误请指正。 您可以随意转载、修改、发布本文，无需经过本人同意。通过博客阅读：iqsing.github.io","categories":[{"name":"k8s","slug":"k8s","permalink":"https://iqsing.github.io/categories/k8s/"}],"tags":[{"name":"DaemonSet","slug":"DaemonSet","permalink":"https://iqsing.github.io/tags/DaemonSet/"}]},{"title":"k8s负载资源StatefulSet工作细节","slug":"k8s负载资源StatefulSet工作细节","date":"2021-09-26T16:47:21.000Z","updated":"2022-02-04T05:16:12.014Z","comments":true,"path":"2021/09/27/k8s负载资源StatefulSet工作细节/","link":"","permalink":"https://iqsing.github.io/2021/09/27/k8s%E8%B4%9F%E8%BD%BD%E8%B5%84%E6%BA%90StatefulSet%E5%B7%A5%E4%BD%9C%E7%BB%86%E8%8A%82/","excerpt":"","text":"在k8s中工作负载资源StatefulSet用于管理有状态应用。 什么是无状态？ 组成一个应用的pod是对等的，它们之前没有关联和依赖关系，不依赖外部存储。 即我们上篇小作文中deployment创建的nginx pod ，他们是完全一样的，任何一个pod 被移除后依然可以正常工作。由于不依赖外部存储，它们可以被轻易的调度到任何 node 上。 什么是有状态？ 显然无状态的反面就是有状态了，pod之间可能包含主从、主备的相互依赖关系，甚至对启动顺序也有要求。更关键的是这些pod 需要外部存储，一旦pod被清除或调度后，怎么把pod 和原来的外部数据联系起来？这就是StatefulSet厉害的地方。 StatefulSet将这些状态应用进行记录，在需要的时候恢复。 StatefulSet如何展开这些工作?一、维护应用拓扑状态通过dns记录为 pod 分配集群内唯一、稳定的网络标识。即只要保证pod 的名称不变，pod被调度到任何节点或者ip如何变更都能被找到。 在 k8s 中Service用来来将一组 Pod 暴露给外界访问的一种机制。当创建的service 中clusterIP为None 时（headless 无头服务）， 不会进行负载均衡，也不会为该服务分配集群 IP。仅自动配置 DNS。 这样我们集群中的 一个pod 将被绑定到一条DNS记录： 1&lt;pod-name&gt;.&lt;svc-name&gt;.&lt;namespace&gt;.svc.cluster.local 通过解析这个地址就能找到pod的IP 。 下面我们创建一个headless service，将clusterIP配置为 None： 12345678910111213#headless-service.ymlapiVersion: v1kind: Servicemetadata: name: nginx-headlessspec: ports: - name: nginx-service-port port: 80 targetPort: 9376 clusterIP: None selector: app: nginx 这个service将会绑定 app=nginx标签的pod，我们通过kubectl apply -f headless-service.yml应用service 并通过get 查看： 1234$ kubectl get serviceNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 18dnginx-headless ClusterIP None &lt;none&gt; 80/TCP 4h48m nginx-headless 这个headless service创建成功了。接着我们创建一个StatefulSet： 123456789101112131415161718192021#nginx-statefulset.ymlapiVersion: apps/v1kind: StatefulSetmetadata: name: nginx-statefulsetspec: serviceName: &quot;nginx-headless&quot; replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx-web image: nginx:1.17 ports: - containerPort: 82 nginx-statefulset 将会绑定我们前面的service nginx-headless并创建三个nginx pod。 我们查看创建的pod ,StatefulSet 中的每个 Pod 根据 StatefulSet 的名称和 Pod 的序号派生出它的主机名。同时statefulset创建出来的pod 名称以$(StatefulSet name)-$(order)开始编号。 123456789$ kubectl get pod NAME READY STATUS RESTARTS AGEnginx-statefulset-0 1/1 Running 0 18snginx-statefulset-1 1/1 Running 0 15snginx-statefulset-2 1/1 Running 0 12s$ kubectl exec nginx-statefulset-0 -- sh -c hostnamenginx-statefulset-0 其实他们的创建顺序也是从0-2，当我们删除这些pod时，statefulset 马上重建出相同名称的Pod 。 我们通过statefulset 的event可以观测到这个过程： 1234567$ kubectl describe nginx-statefulsetEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulCreate 7m43s statefulset-controller create Pod nginx-statefulset-0 in StatefulSet nginx-statefulset successful Normal SuccessfulCreate 7m40s statefulset-controller create Pod nginx-statefulset-1 in StatefulSet nginx-statefulset successful Normal SuccessfulCreate 7m37s statefulset-controller create Pod nginx-statefulset-2 in StatefulSet nginx-statefulset successful 现在我们来看一下 pod 是否存在于 DNS 记录中： 1kubectl run -it --image busybox busybox --rm /bin/sh 运行一个一次性 pod busybox ，接着使用 ping 命令查询之前提到的规则构建名称nginx-statefulset-0.nginx-headless.default.svc.cluster.local 解析的IP与如下nginx-statefulset-0相符。 这样我们使用pod名称通过DNS就可以找到这个pod 再加上StatefulSet可以按顺序创建出不变名称的 pod ，即一个应用通过StatefulSet准确维护其拓扑状态 二、维护应用存储状态k8s为应对应用的数据存储需求提供了卷的概念（volume）以及提供持久化存储的PVC（ PersistentVolumeClaim）PV（ PersistentVolume）当一个pod 和 PVC绑定后，即使pod 被移除，PVC和PV仍然保留在集群中，pod 再次被创建后会自动绑定到之前的PVC。他们看起来是这样的： 这里我们以讨论statefulset持久化存储为主，对于k8s存储本身不了解的同学可以参考k8s官方文档存储章节storage 首先我们创建存储目录 /data/volumes/ 以及一个本地的local类型（使用节点上的文件或目录来模拟网络附加存储）的PV： 123456789101112131415161718192021222324#pv-local.ymlapiVersion: v1kind: PersistentVolumemetadata: name: pv-localspec: capacity: storage: 5Gi volumeMode: Filesystem accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Delete storageClassName: local-storage local: path: /data/volumes/ nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - minikube PV是集群中的一块存储，它声明了后端使用的真实存储，通常会由K8S管理员创建。我们在pv-local中声明了后端存储类型为local挂载到目录 /data/volumes/ , 存储卷类名为local-storage，1Gb容量，访问模式ReadWriteMany – 卷可以被多个个节点以读写方式挂载。亲和的节点为minikube 我们通过get来查看这个PV： 123$ kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEpv-local 5Gi RWX Delete Available local-storage 25m 此时PV的状态为available，还未与任何PVC绑定。我们通过创建PV使集群得到了一块存储资源，但此时还不属于你的应用，我们需要通过PVC去构建一个使用它的”通道“。 12345678910111213#app1-pvc.ymlapiVersion: v1kind: PersistentVolumeClaimmetadata: name: app1-pvcspec: storageClassName: local-storage accessModes: - ReadWriteMany resources: requests: storage: 1Gi 现在我们开辟好一个5Gb容量的存储通道（PVC），此时PV和PVC已通过 storageClassName自动形成绑定。这样PV和PVC的status 皆为Bound 1234567$ kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEpv-local 5Gi RWX Delete Bound default/app-pvc local-storage 25m$ kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEapp-pvc Bound pv-local 5Gi RWX local-storage 27m 上面我们创建好通道，接下来要在我们statefuset中绑定这个通道，才能顺利使用存储。 1234567891011121314151617181920212223242526272829303132# nginx-statefulset.ymlapiVersion: apps/v1 kind: StatefulSet metadata: name: nginx-statefulsetspec: serviceName: &quot;nginx-headless&quot; replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: nodeName: minikube volumes: - name: app-storage persistentVolumeClaim: claimName: app-pvc containers: - name: nginx-web image: nginx:1.17 ports: - containerPort: 80 name: nginx-port volumeMounts: - mountPath: /usr/share/nginx/html name: app-storage 与之前的statefulset相比我们在pod 模板中添加了volume 已经 volumeMounts，这样使用这个statefulset 所创建的pod都将挂载 我们前面定义的PVC app-pvc，应用nginx-statefulset.yml后我们进入到pod 检验一下目录是否被正确挂载。 1234$ kubectl exec -it nginx-statefulset-0 -- /bin/bashroot@nginx-statefulset-0:/# cat /usr/share/nginx/html/index.htmlhello pv 查看本地目录文件： 12root@minikube:/# cat /data/volumes/index.html hello pv 接着我们在pod 中修改index.html内容为并将pod删除，检验重载后的 pod 存储数据是否能被找回。 1root@nginx-statefulset-0:/# echo &quot;pod data&quot; &gt; /usr/share/nginx/html/index.html 删除带有标签app=nginx的pod ,由于statefulset的控制器使pod按顺序被重建： 12345678910$ kubectl delete pod -l app=nginxpod &quot;nginx-statefulset-0&quot; deletedpod &quot;nginx-statefulset-1&quot; deletedpod &quot;nginx-statefulset-2&quot; deleted$ kubectl get pod NAME READY STATUS RESTARTS AGEnginx-statefulset-0 1/1 Running 0 9snginx-statefulset-1 1/1 Running 0 6snginx-statefulset-2 0/1 ContainerCreating 0 3s 毫无疑问，pod 数据完好无损： 123$ kubectl exec -it nginx-statefulset-0 -- /bin/bashroot@nginx-statefulset-0:/# cat /usr/share/nginx/html/index.htmlpod data 也就是说虽然我们的pod被删除了，但是PV已经PV依然保留在集群中，当pod 被重建后，它依然会去找定义的claimName: app-pvc这个PVC，接着挂载到容器中。 这里我们一个PVC 绑定了多个节点，其实可以为每一个 statefulset中的pod 创建PVC，可以自行了解。 k8s存储可操作性非常强，这里只在statefulset下做了简单的演示。后续我们会对k8s存储做更深入的了解。 三、总结这篇小作文我们一起学习了k8s中工作负载资源StatefulSet是如何管理有状态应用的，主要从维护应用拓扑状态和存储状态两个方面做了简单介绍。这样我们对statefulset这个工作资源有了大体了解：StatefulSet 与 Deployment 相比，它为每个管理的 Pod 都进行了编号，使Pod有一个稳定的启动顺序，并且是集群中唯一的网络标识。有了标识后使用PV、PVC对存储状态进行维护。 希望小作文对你有些许帮助，如果内容有误请指正。 您可以随意转载、修改、发布本文，无需经过本人同意。 通过博客阅读：iqsing.github.io","categories":[{"name":"k8s","slug":"k8s","permalink":"https://iqsing.github.io/categories/k8s/"}],"tags":[{"name":"statefulset","slug":"statefulset","permalink":"https://iqsing.github.io/tags/statefulset/"}]},{"title":"k8s工作负载资源之deployment","slug":"k8s工作负载资源之deployment","date":"2021-09-22T16:47:21.000Z","updated":"2022-02-04T05:16:11.969Z","comments":true,"path":"2021/09/23/k8s工作负载资源之deployment/","link":"","permalink":"https://iqsing.github.io/2021/09/23/k8s%E5%B7%A5%E4%BD%9C%E8%B4%9F%E8%BD%BD%E8%B5%84%E6%BA%90%E4%B9%8Bdeployment/","excerpt":"","text":"首先我们要理解：一个应用跑在k8s集群上了，那么这个应用就是一个工作负载（workloads）。 在k8s中会用pod的来承载这个应用，那么负责管理这个pod的东西就叫工作负载资源（workload resources）。 我们可以简单理解为是这样的： 工作负载资源又支持jj自定义或使用第三方资源，这里我们先认识内置的，k8s内置工作负载资源包含如下： deployment replicaset statefulset daemonset jobs cronjob TTL Controller for Finished Resources ReplicationController （逐步被ReplicaSet替代） 那让我们从最常用的deployment开始吧。 一个 Deployment 为 Pods和 ReplicaSets提供声明式的更新能力，我们从下面几个方面开始上手： 创建 Deployment 将 ReplicaSet 上线。 ReplicaSet 在后台创建 Pods。 检查 ReplicaSet 的上线状态，查看其是否成功。 **通过更新 Deployment 的 Pod模板（TemplateSpec），声明 Pod 的新状态 。 **新的 ReplicaSet 会被创建，Deployment 以受控速率将 Pod 从旧 ReplicaSet 迁移到新 ReplicaSet。 每个新的 ReplicaSet 都会更新到 Deployment 的修订版本。 如果 Deployment 与你的预期不符，可以回滚到较早的 Deployment 版本。 每次回滚都会更新到 Deployment 修订的新版本。 通过Deployment 扩大应用规模承担更多负载。 暂停 Deployment ，对 PodTemplateSpec 做修改然后恢复执行，让pod更新到新版本。 deployment创建说了这么多还不如手动写一个deployment的yml声明实在（如果你喜欢json也可以是json格式，本质上还是将yml转换为json格式请求的api）。 下面deployment创建了一个replicaset，这个replicaset将会启动三个nginx的pod： nginx-deployment.yml 12345678910111213141516171819apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deploymentspec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx-web image: nginx:latest ports: - containerPort: 80 通过kubectl apply 将声明文件转换为api提交给apiserver 12$ kubectl apply -f nginx-deployment.yml deployment.apps/nginx-deployment created 查看deployment资源创建的对象nginx-deployment（这里的对象与编程语言中对象同义） 123$ kubectl get deploymentNAME READY UP-TO-DATE AVAILABLE AGEnginx-deployment 3/3 3 3 67m 查看nginx-deplyment创建的replicat对象nginx-deployment-767cf44bff 123$kubectl get rsNAME DESIRED CURRENT READY AGEnginx-deployment-767cf44bff 3 3 3 68m 最后是nginx-deployment-767cf44bff创建的三个pod对象 123456$ kubectl get pod NAMESPACE NAME READY STATUS RESTARTS AGEdefault nginx-deployment-767cf44bff-9fj8q 1/1 Running 0 13mdefault nginx-deployment-767cf44bff-f746l 1/1 Running 0 13mdefault nginx-deployment-767cf44bff-ktbzl 1/1 Running 0 13m 也可以通过rollout status 查看 Deployment 上线状态。 12$kubectl rollout status deployment/nginx-deploymentdeployment &quot;nginx-deployment&quot; successfully rolled out 这就是deployment资源创建对象的关系图： 现在我们主要来看一下创建的这个nginx-deployment声明。 我们把yml文件分为两个大部分（红色）： 属性。 apiVersion - 创建该对象所使用的 Kubernetes API 的版本 kind - 想要创建的对象的类别 metadata - 帮助唯一性标识对象的一些数据，包括一个 name 字符串、UID 和可选的 namespace 规格 spec（specification） replicas - 期望的pod副本数量 selector - pod标签选择器 template - pod模板 我们在selector中匹配包含app=nginx标签的pod，pod模板中又为新创建的pod打上app=nginx的标签，这样就形成了控制闭环。 我们通过可以show-labels查看pod的标签 12345$ kubectl get pod --show-labelsNAME READY STATUS RESTARTS AGE LABELSnginx-deployment-767cf44bff-9fj8q 1/1 Running 0 81m app=nginx,pod-template-hash=767cf44bffnginx-deployment-767cf44bff-f746l 1/1 Running 0 81m app=nginx,pod-template-hash=767cf44bffnginx-deployment-767cf44bff-ktbzl 1/1 Running 0 81m app=nginx,pod-template-hash=767cf44bff 为什么pod中又有一个pod-template-hash标签？ eployment 控制器将 pod-template-hash 标签添加到 Deployment 所创建的每一个 ReplicaSet 中。我们来看一下rs的selector描述： 1234$ kubectl describe rsName: nginx-deployment-767cf44bffNamespace: defaultSelector: app=nginx,pod-template-hash=767cf44bff pod-template-hash 标签是通过对 ReplicaSet 的 PodTemplate 进行哈希处理，此标签可确保 Deployment 的子 ReplicaSets 不冲突，所生成的哈希值被添加到 ReplicaSet的selector、Pod 模板labels、以及 ReplicaSet 旗下的任何 Pod 中。这样deployment下的replicaset只能控制自己的pod。恩，妙哉。 不同工作负载资源所创建的对象，spec是不同的。比如在Deployment中spec可以包含如下字段，这个可以在Kubernetes API中找到。 大多数字段都包含了一个默认值，除非有特殊需求，大多数时候很难被用到。如果需要的时候你再谷歌一下也不迟。到这里deployment工作负载的第一个用例已经成了。 deployment更新仅当 Deployment Pod 模板（即 .spec.template字段）发生改变时，例如模板的标签或容器镜像被更新， 才会触发 Deployment 上线。 其他更新（如对 Deployment 执行扩缩容的操作）不会触发上线动作。 我们可以通过kubectl set 命令更新现有工作负责资源 12$ kubectl set image deployment/nginx-deployment nginx-web=nginx:1.17 --recorddeployment.apps/nginx-deployment image updated --record 用于记录kubectl对资源的操作。便于后期需要时回滚。下面会说到。 kubectl set -h 查询set支持更新的内容。 Available Commands: env Update environment variables on a pod template image Update image of a pod template resources Update resource requests/limits on objects with pod templates selector Set the selector on a resource serviceaccount Update ServiceAccount of a resource subject Update User, Group or ServiceAccount in a RoleBinding/ClusterRoleBinding 使用kubectl edit编辑deployment后自动更新 12$ kubectl edit deployment/nginx-deployment --recorddeployment.apps/nginx-deployment edited 直接更新deployment yml文件 个人觉得最好的方式是更新yml声明文件，通过kubectl apply 应用即可，这样你只要管理好你的的nginx-deployment.yml做到心中有数 当我们更新后查看rs状态，此时deployment 创建了一个新的nginx-deployment replicaset并投入使用。 1234$ kubectl get rsNAME DESIRED CURRENT READY AGEnginx-deployment-64f9765d86 4 4 4 4h11mnginx-deployment-6cf9cc9c9d 0 0 0 5h6m 这里顺便看一下deployment中pod滚动更新策略 我们可以通过kubectl describe deployment 查看RollingUpdateStrategy字段，即在滚动更新时最大不可用pod数为1/4，最大可用pod数为期望副本数1.25倍（多25%）。 RollingUpdateStrategy: 25% max unavailable, 25% max surge 假如我们将 deployment_A 中4个副本（pod）更新到deployment_B（也是4副本）,其中的某个数据pod状态如下 123456789$ kubectl get pod NAME READY STATUS RESTARTS AGEnginx-deployment-64f9765d86-c9rlj 0/1 ContainerCreating 0 2snginx-deployment-64f9765d86-wngmx 0/1 ContainerCreating 0 2snginx-deployment-7fcdcb4b75-lmsmm 1/1 Running 0 4h6mnginx-deployment-7fcdcb4b75-m99tx 1/1 Terminating 0 4h5mnginx-deployment-7fcdcb4b75-tghb2 1/1 Running 0 4h5mnginx-deployment-7fcdcb4b75-xfs2m 1/1 Running 0 4h6m 即只有1个在停止，正在创建2个新pod（即将有5个可用），详细滚动过程可通过kubectl descibe deployment中events查看。 尽量不要更新模板中labels，会造成pod孤立。在某些API版本已经被禁止了。 deployment回滚deployment回滚和更新一样，Pod 模板部分会被回滚。 我们通过 rollout history 来查看某个deployment的历史版本。即之前通过--record所记录的。 12345678910$ kubectl rollout history deployment/nginx-deployment deployment.apps/nginx-deployment REVISION CHANGE-CAUSE1 &lt;none&gt;2 &lt;none&gt;3 kubectl apply --filename=nginx-deployment.yml --record=true4 kubectl apply --filename=nginx-deployment.yml --record=true5 kubectl set image deployment/nginx-deployment nginx-web=nginx:1.17 --record=true9 kubectl edit deployment/nginx-deployment --record=true10 kubectl edit deployment/nginx-deployment --record=true 回滚到第5个版本。 12$ kubectl rollout undo deployment/nginx-deployment --to-revision=5deployment.apps/nginx-deployment rolled back 或者直接回滚到上一个版本。 1$ kubectl rollout undo deployment/nginx-deployment 还是如前面所说，回滚只一种更工程化的说法，其实回滚也是一种更新，yml声明依然是核心。所以我们更应该关注的对deployment的yml文件的版本控制。 deployment缩放缩放控制的是.spec.replicas,也可通过scale命令操作。 12$ kubectl scale deployment/nginx-deployment --replicas=6deployment.apps/nginx-deployment scaled 水平自动缩放本为暂不涉及，后面文章会详细讨论。可以参考：Horizontal Pod Autoscaler deployment暂停与恢复我们可以在触发更新之前暂停 Deployment，然后做多个修改之后再恢复，进行一次性上线。 还是我们之前的deployment 12345678$ kubectl get deployNAME READY UP-TO-DATE AVAILABLE AGEnginx-deployment 4/4 4 4 30m$ kubectl get rsNAME DESIRED CURRENT READY AGEnginx-deployment-64f9765d86 4 4 4 30m 暂停deployment 12$ kubectl rollout pause deployment/nginx-deploymenterror: deployments.apps &quot;nginx-deployment&quot; is already paused 对nginx-deployment做一些更新 直接修改yml文件，更新镜像为nginx:1.17，并通过kubectl apply应用更改。 通过set做资源限制。 12$ kubectl set resources deployment/nginx-deployment -c=nginx-web --limits=cpu=50m,memory=100Mideployment.apps/nginx-deployment resource requirements updated 此时我们查看rs副本状态，查看deployment版本信息 12345678$ kubectl get rs NAME DESIRED CURRENT READY AGEnginx-deployment-64f9765d86 4 4 4 40m$ kubectl rollout history deployment/nginx-deploymentdeployment.apps/nginx-deployment REVISION CHANGE-CAUSE1 &lt;none&gt; 可以看到还是之前的rs，deployment并没有将我们的修改应用到对象中。 现在我们将deployment通过Resume恢复 12$ kubectl rollout resume deployment/nginx-deploymentdeployment.apps/nginx-deployment resumed 查看rs状态 1234$ kubectl get rs NAME DESIRED CURRENT READY AGEnginx-deployment-64f9765d86 3 3 3 41mnginx-deployment-7f4447656b 2 2 0 4s 这会deployment已恢复，并应用了对资源对象的更新。 参考：k8s官方文档Deployment","categories":[{"name":"k8s","slug":"k8s","permalink":"https://iqsing.github.io/categories/k8s/"}],"tags":[{"name":"deployment","slug":"deployment","permalink":"https://iqsing.github.io/tags/deployment/"}]},{"title":"K8s工作流程详解","slug":"K8s工作流程详解","date":"2021-09-13T16:47:21.000Z","updated":"2022-02-04T05:16:11.713Z","comments":true,"path":"2021/09/14/K8s工作流程详解/","link":"","permalink":"https://iqsing.github.io/2021/09/14/K8s%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B%E8%AF%A6%E8%A7%A3/","excerpt":"","text":"在学习k8s工作流程之前，我们得再次认识一下上篇k8s架构与组件详解中提到的kube-controller-manager一个k8s中许多控制器的进程的集合。 比如Deployment 控制器（DeploymentController）和 Job 控制器（JobController）是 Kubernetes 内置控制器的典型例子。在 Kubernetes 中，一个控制器至少追踪一种类型的 Kubernetes 资源。这些 资源对象有一个代表期望状态的 spec 字段。 该资源的控制器负责所属对象当前状态接近期望状态。 一、控制器与apiserver的交互上面提到的这些资源的控制器是如何确保资源对象当前状态接近于期望状态？ 当然是持续同步apiserver中（查询etcd）资源对象的元数据，并不断更新对象属性。是这样么？ 当集群中有几十上百万个资源对象时，光控制器的http同步请求就够apiserver喝一壶的，显然不太棒。所以Kubernetes采用了一个叫Informer的机制。Informer 是 Client-go 中的一个核心工具包。 在这里informer主要实现的作用如下： 更快地返回 List/Get 请求，减少对 Kubenetes API 的直接调用 使用 Informer 实例的 Lister() 方法， List/Get Kubernetes 中的 Object 时，Informer 不会去请求 Kubernetes API，而是直接查找缓存在本地内存中的数据，依赖Etcd的List&amp;Watch机制，客户端及时获知这些对象的状态变化，然后更新本地缓存，这样就在客户端为这些API对象维护了一份和Etcd数据库中几乎一致的数据，然后控制器等客户端就可以直接访问缓存获取对象的信息，而不用去直接访问apiserver。通过这种方式，Informer 既可以更快地返回结果，又能减少对 Kubernetes API 的直接调用。 可监听事件并触发回调函数 Informer 通过 Kubernetes Watch API 监听某种 resource 下的所有事件。Watch API 本质上就是一种 APIServer 主动向客户端推送 Kubernetes 资源修改、创建的一种机制。这样我们就可以获取到资源的变更，及时更新对象状态。 关于k8s中 informer详细可参考：kubenetes informer 详解 通过上面我们知道了控制器是通过watch api监听apiserver中资源对象的更新，下面我们进入正题：k8s工作流程。 二、k8s工作流程我们来看通过deployment部署pod的常规流程： kubectl向apiserver发送部署请求（例如使用 kubectl create -f deployment.yml） apiserver将 Deployment 持久化到etcd；etcd与apiserver进行一次http通信。 controller manager通过watch api监听 apiserver ，deployment controller看到了一个新创建的deplayment对象更后，将其从队列中拉出，根据deployment的描述创建一个ReplicaSet并将 ReplicaSet 对象返回apiserver并持久化回etcd。 以此类推，当replicaset控制器看到新创建的replicaset对象，将其从队列中拉出，根据描述创建pod对象。 接着scheduler调度器看到未调度的pod对象，根据调度规则选择一个可调度的节点，加载到pod描述中nodeName字段，并将pod对象返回apiserver并写入etcd。 kubelet在看到有pod对象中nodeName字段属于本节点，将其从队列中拉出，通过容器运行时创建pod中描述的容器。 上面我们说到的deployment-replicaset-pod的关系如下： 希望小作文对你有些许帮助，如果内容有误请指正。 您可以随意转载、修改、发布本文，无需经过本人同意。 没什么用的blog：iqsing.github.io","categories":[{"name":"k8s","slug":"k8s","permalink":"https://iqsing.github.io/categories/k8s/"}],"tags":[{"name":"workflow","slug":"workflow","permalink":"https://iqsing.github.io/tags/workflow/"}]},{"title":"k8s架构与组件详解","slug":"k8s架构与组件详解","date":"2021-09-12T16:47:21.000Z","updated":"2022-02-04T05:16:11.982Z","comments":true,"path":"2021/09/13/k8s架构与组件详解/","link":"","permalink":"https://iqsing.github.io/2021/09/13/k8s%E6%9E%B6%E6%9E%84%E4%B8%8E%E7%BB%84%E4%BB%B6%E8%AF%A6%E8%A7%A3/","excerpt":"","text":"没有那么多花里胡哨，直接进行一个K8s架构与组件的学习。 一、K8s架构 k8s系统在设计是遵循c-s架构的，也就是我们图中apiserver与其余组件的交互。在生产中通常会有多个Master以实现K8s系统服务高可用。K8s集群至少有一个工作节点，节点上运行 K8s 所管理的容器化应用。 在Master通常上包括 kube-apiserver、etcd 存储、kube-controller-manager、cloud-controller-manager、kube-scheduler 和用于 K8s 服务的 DNS 服务器（插件）。这些对集群做出全局决策(比如调度)，以及检测和响应集群事件的组件集合也称为控制平面。 其实K8s官方并没有Master这一说，只是大多数安装工具（kubeadm）或者脚本为了架构更明了会把控制平面中的组件安装到一台机器上即Master机器，并且不会在此机器上运行用户容器。这不是强制性的，所以你也可以对将控制平面实行分布式部署，不过这样的话高可用会是一个不小的挑战。 在Node上组件包括 kubelet 、kube-porxy 以及服务于pod的容器运行时(runtime)。外部storage与registry用于为容器提供存储与镜像仓库服务。 从kubectl开始，我们来看一下K8s的基本工作流程： kubectl 客户端首先将CLI命令转化为RESTful的API调用，然后发送到kube-apiserver。 kube-apiserver 在验证这些 API 调用后，将任务元信息并存储到etcd，接着调用 kube-scheduler 开始决策一个用于作业的Node节点。 一旦 kube-scheduler 返回一个适合调度的目标节点后，kube-apiserver 就把任务的节点信息存入etcd，并创建任务。 此时目标节点中的 kubelet正监听apiserver，当监听到有新任务需要调度到本节点后，kubelet通过本地runtime创建任务容器，执行作业。 接着kubelet将任务状态等信息返回给apiserver存储到etcd。 这样我们的任务已经在运行了，此时control-manager发挥作用保证任务一直是我们期望的状态。 二、K8s组件介绍1、控制平面组件kube-apiserverAPI服务器为K8s集群资源操作提供唯一入口，并提供认证、授权、访问控制、API 注册和发现机制。 Kubernetes API 服务器的主要实现是 kube-apiserver。 kube-apiserver 设计上考虑了水平伸缩，也就是说，它可通过部署多个实例进行伸缩。 你可以运行 kube-apiserver 的多个实例，并在这些实例之间进行流量平衡。 etcdetcd 是兼具一致性和高可用性的键值数据库，可以作为保存 Kubernetes 所有集群数据的后台数据库(例如 Pod 的数量、状态、命名空间等）、API 对象和服务发现细节。在生产级k8s中etcd通常会以集群的方式存在，安全原因，它只能从 API 服务器访问。 etcd也是k8s生态的关键应用。关于 etcd 可参考 etcd 文档。 kube-schedulerkube-scheduler 负责监视新创建、未指定运行Node的 Pods，决策出一个让pod运行的节点。 例如，如果应用程序需要 1GB 内存和 2 个 CPU 内核，那么该应用程序的 pod 将被安排在至少具有这些资源的节点上。每次需要调度 pod 时，调度程序都会运行。调度程序必须知道可用的总资源以及分配给每个节点上现有工作负载的资源。 调度决策考虑的因素包括单个 Pod 和 Pod 集合的资源需求、硬件/软件/策略约束、亲和性和反亲和性规范、数据位置、工作负载间的干扰和最后时限。 kube-controller-managerk8s在后台运行许多不同的控制器进程，当服务配置发生更改时（例如，替换运行 pod 的镜像，或更改配置 yaml 文件中的参数），控制器会发现更改并开始朝着新的期望状态工作。 从逻辑上讲，每个控制器都是一个单独的进程， 但是为了降低复杂性，它们都被编译到同一个可执行文件，并在一个进程中运行。 控制器包括: 节点控制器（Node Controller）: 负责在节点出现故障时进行通知和响应 任务控制器（Job controller）: 监测代表一次性任务的 Job 对象，然后创建 Pods 来运行这些任务直至完成 端点控制器（Endpoints Controller）: 填充端点(Endpoints)对象(即加入 Service 与 Pod) 服务帐户和令牌控制器（Service Account &amp; Token Controllers）: 为新的命名空间创建默认帐户和 API 访问令牌 cloud-controller-manager 云控制器管理器使得你可以将你的集群连接到云提供商的 API 之上， 同时可以将云平台交互组件与本地集群中组件分离。 cloud-controller-manager 仅运行特定于云平台的控制回路。 如果我们在自己的环境中运行 Kubernetes，大多数时候非混合云环境是用不到这个组件的。 与 kube-controller-manager 类似，cloud-controller-manager 将若干逻辑上独立的 控制回路组合到同一个可执行文件中，供你以同一进程的方式运行。 你可以对其执行水平扩容（运行不止一个副本）以提升性能或者增强容错能力。 下面的控制器都包含对云平台驱动的依赖： 节点控制器（Node Controller）: 用于在节点终止响应后检查云提供商以确定节点是否已被删除 路由控制器（Route Controller）: 用于在底层云基础架构中设置路由 服务控制器（Service Controller）: 用于创建、更新和删除云提供商负载均衡器 2.Node中组件节点组件在每个节点上运行，维护运行的 Pod 并提供 Kubernetes 运行环境。 kubelet一个在集群中每个node上运行的代理。 它保证容器都 运行在 Pod 中。kubelet 定期接收新的或修改过的 pod 规范 PodSpecs（主要通过 kube-apiserver）并确保 pod 及容器健康并以所需状态运行。该组件还向 kube-apiserver 报告运行它的主机的健康状况。 kubelet 不会管理不是由 Kubernetes 创建的容器。 kube-proxykube-proxy 是集群中每个节点上运行的网络代理， 实现 Kubernetes 服务（Service） 概念的一部分。用于处理单个主机子网划分并向外部世界公开服务。它跨集群中的各种隔离网络将请求转发到正确的 pod/容器。 kube-proxy 维护节点上的网络规则。这些网络规则允许从集群内部或外部的网络会话与 Pod 进行网络通信。 如果操作系统提供了数据包过滤层并可用的话，kube-proxy 会通过它来实现网络规则。否则， kube-proxy 仅转发流量本身。 容器运行时（Container Runtime）容器运行时负责创建容器运行环境。 Kubernetes 支持多个容器运行时: Docker（即将被废弃）、containerd、CRI-O以及任何实现 Kubernetes CRI (容器运行环境接口)的runtime。 三、tips K8s拥有一个完整的云原生生态，是一个缤纷多彩同时又把复杂度拉满的世界。 k8s基础是容器，虽然docker运行时已被k8s弃用，但是学习docker依然是上手容器化最佳方式。 Kubernetes 官方文档https://kubernetes.io/docs/home/ NEXT k8s工作流程详解 希望小作文对你有些许帮助，如果内容有误请指正。 您可以随意转载、修改、发布本文，无需经过本人同意。","categories":[{"name":"k8s","slug":"k8s","permalink":"https://iqsing.github.io/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://iqsing.github.io/tags/k8s/"}]},{"title":"docker 容器如何精简镜像减小体积","slug":"docker容器 如何精简镜像减小体积","date":"2021-08-31T04:22:21.000Z","updated":"2022-02-04T05:16:11.752Z","comments":true,"path":"2021/08/31/docker容器 如何精简镜像减小体积/","link":"","permalink":"https://iqsing.github.io/2021/08/31/docker%E5%AE%B9%E5%99%A8%20%E5%A6%82%E4%BD%95%E7%B2%BE%E7%AE%80%E9%95%9C%E5%83%8F%E5%87%8F%E5%B0%8F%E4%BD%93%E7%A7%AF/","excerpt":"","text":"写在前面我们在上篇《Docker容器 关于镜像构建的安全问题》一起学习了如何构建一个基于安全的镜像，这篇小作文我们会学习镜像构建的另一个关键性问题，为何别人打造的镜像只有10MB而我的有几百MB？如何精简镜像减小镜像体积？ 精简镜像我们可以从两个方面切入： 减少镜像层数 缩减容量 一、减少镜像层数1.指令合并Dockerfile 中的每条指令都将创建一个层，不过查看官方文档中最佳实践有这样一句话： In older versions of Docker, it was important that you minimized the number of layers in your images to ensure they were performant. The following features were added to reduce this limitation: Only the instructions RUN, COPY, ADD create layers. Other instructions create temporary intermediate images, and do not increase the size of the build. … 参考地址：Minimize the number of layers 意味着只有 RUN, COPY, ADD 三个指令会创建层，其他指令会创建一个中间镜像，并且不会影响镜像大小。这样我们说的指令合并也就是以这三个指令为主。 我们以如下Dockerfile为例 123456789101112FROM debian:stableWORKDIR /var/wwwLABEL version=“v1”RUN apt-get updateRUN apt-get -y --no-install-recommends install curlRUN apt-get purge -y curlRUN apt-get autoremove -yRUN apt-get cleanRUN rm -rf /var/lib/apt/lists/* 构建镜像 1docker build -t curl:v1 . 通过history查看构建历史 12345678910111213# docker history curl:v1IMAGE CREATED CREATED BY SIZE COMMENT29b721c09b67 18 seconds ago /bin/sh -c rm -rf /var/lib/apt/lists/* 0B aa28ae151e59 20 seconds ago /bin/sh -c apt-get clean 0B 4f733781f557 22 seconds ago /bin/sh -c apt-get autoremove -y 989kB f66887372121 29 seconds ago /bin/sh -c apt-get purge -y curl 987kB d458ee0de463 34 seconds ago /bin/sh -c apt-get -y --no-install-recommend… 4.46MB 43fdcf68018c 44 seconds ago /bin/sh -c apt-get update 17.6MB 65631e8bb010 53 seconds ago /bin/sh -c #(nop) LABEL version=“v1” 0B 7ef7c53b019c 53 seconds ago /bin/sh -c #(nop) WORKDIR /var/www 0B 8bfa93572e55 13 days ago /bin/sh -c #(nop) CMD [&quot;bash&quot;] 0B &lt;missing&gt; 13 days ago /bin/sh -c #(nop) ADD file:d78d93eff67b18592… 124MB 镜像大小 123[root@localhost dockerfiles]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEcurl v1 29b721c09b67 10 minutes ago 148MB 我们将RUN指令通过类shell操作&amp;&amp;合并后 123456RUN apt-get update &amp;&amp; \\ apt-get -y --no-install-recommends install curl &amp;&amp; \\ apt-get purge -y curl &amp;&amp; \\ apt-get autoremove -y &amp;&amp; \\ apt-get clean &amp;&amp; \\ rm -rf /var/lib/apt/lists/* 查看构建历史与镜像大小 1234567891011# docker history curl:v2IMAGE CREATED CREATED BY SIZE COMMENT928e12c2f57e About a minute ago /bin/sh -c apt-get update &amp;&amp; apt-get -y … 989kB 5a32372025fb About a minute ago /bin/sh -c #(nop) LABEL version=“v2” 0B 7ef7c53b019c 30 minutes ago /bin/sh -c #(nop) WORKDIR /var/www 0B 8bfa93572e55 13 days ago /bin/sh -c #(nop) CMD [&quot;bash&quot;] 0B &lt;missing&gt; 13 days ago /bin/sh -c #(nop) ADD file:d78d93eff67b18592… 124MB# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEcurl v2 928e12c2f57e 3 minutes ago 125MB 可见只是一个简单的curl应用在通过指令合并的方式安装已经获得了约20MB的容量释放。同时使你的dockerfile文件更为易读和简约。 2.多阶段构建在Docker17.05 中引入了多阶段构建，通过多阶段构建可以大大降低构建复杂度，同时使缩小镜像尺寸更为简单。我们来看多阶段构建的Dockerfile 12345678910#阶段1FROM golang:1.16WORKDIR /go/srcCOPY app.go ./RUN go build app.go -o myapp#阶段2FROM scratchWORKDIR /serverCOPY --from=0 /go/src/myapp ./CMD [&quot;./myapp&quot;] 构建镜像 1# docker build --no-cache -t server_app:v2 . 查看构建好的镜像 123# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEserver_app v2 20225cb1ea6b 12 seconds ago 1.94MB 以上用例来自上篇文章《Dockerfile 多阶段构建实践》关于镜像多阶段构建具体内容可以前往查看，这里不做过多赘述。 3.启用squash特性通过启用squash特性（实验性功能）docker build --squash -t curl:v3 . 可以构建的镜像压缩为一层。但是为了充分发挥容器镜像层共享的优越设计，这种方法不被推荐。 二、缩减容量1. 选择小的基础镜像每个linux发行版镜像大小相差很多，甚至相同发行版镜像也存在差异。我们以debian为例： 稳定版和瘦身版相差约40MB 123# docker images debian stable-slim 2aa48a485e3a 13 days ago 80.4MBdebian stable 8bfa93572e55 13 days ago 124MB 我们将Dockerfile中基础镜像改为瘦身版debian:stable-slim 1FROM debian:stable-slim 构建后的镜像尺寸更小 123# docker images REPOSITORY TAG IMAGE ID CREATED SIZEcurl v4 1aab5c9bf8b3 17 seconds ago 81.4MB 当前映像基于 Debian，并包含许多二进制文件。Docker 容器应该包含一个进程，并包含运行它所需的最低限度。我们其实不需要整个操作系统。 我们可以使用基于 Alpine 的镜像 替换Debian 基础镜像。 123456789FROM alpineWORKDIR /var/wwwLABEL version=“v5”RUN echo -e &#x27;https://mirrors.aliyun.com/alpine/v3.6/main/\\nhttps://mirrors.aliyun.com/alpine/v3.6/community/&#x27; &gt; /etc/apk/repositories &amp;&amp; \\ apk update &amp;&amp; \\ apk upgrade &amp;&amp; \\ apk add --no-cache curl 查看镜像大小 123# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEcurl v5 7f735bb213be 11 seconds ago 10.1MB 此时我们的镜像来到了10MB。使用alpine镜像包管理工具是apk，一些软件包名可能不一样。最大的区别在于Alpine 采用的链接库是 musl libc 而不是 glibc 系列。 2.上下文管理我们经常会用到的COPY指令 1COPY . /server/dir COPY会把整个 构建上下文复制到镜像中，并生产新的缓存层。为了不必要的文件如日志、缓存文件、Git 历史记录被加载到构建上下文，我们最好添加**.dockerignore**用于忽略非必须文件。这也是精简镜像关键一步，同时能更好的保证我们构建的镜像安全性。 3.及时清理下载我们有如下Dockerfile 12345..WORKDIR /tmpRUN curl -LO https://docker.com/download.zip &amp;&amp; tar -xf download.zip -C /var/www RUN rm -f download.zip... 我们虽然使用了rm删除download.zip包，由于镜像分层的问题，download.zip是在新的一层被删除，上一层仍然存在。 我们要在一层中及时清理下载 1RUN curl -LO https://docker.com/download.zip &amp;&amp; tar -xf download.zip -C /var/www &amp;&amp; rm -f download.zip 另外在安装软件时应及时使用包管理工具清除你下载的软件依赖及缓存，比如在我们dockerfile中使用apt包管理工具做清理。 关于精简镜像的相关操作介绍到这里。 希望小作文对你有些许帮助，如果内容有误请指正。 您可以随意转载、修改、发布本文章，无需经过本人同意。","categories":[{"name":"docker","slug":"docker","permalink":"https://iqsing.github.io/categories/docker/"}],"tags":[{"name":"dockerfile","slug":"dockerfile","permalink":"https://iqsing.github.io/tags/dockerfile/"}]},{"title":"Docker容器 Dockerfile构建安全镜像","slug":"Docker容器 Dockerfile构建安全镜像","date":"2021-08-30T04:47:21.000Z","updated":"2022-02-04T05:16:11.693Z","comments":true,"path":"2021/08/30/Docker容器 Dockerfile构建安全镜像/","link":"","permalink":"https://iqsing.github.io/2021/08/30/Docker%E5%AE%B9%E5%99%A8%20Dockerfile%E6%9E%84%E5%BB%BA%E5%AE%89%E5%85%A8%E9%95%9C%E5%83%8F/","excerpt":"","text":"写在前面确保容器中服务与应用安全是容器化演进的关键点。容器安全涉及到应用开发与维护的整个生命周期，本文主要从镜像构建的视角来看docker容器的一些安全问题及应对措施。 一、权限管理1.避免以容器以root身份运行 在Openshift与k8s环境中默认容器需要以非root身份运行，使用root身份运行的情况很少，所以不要忘记在dockerfile中包含USER指令，以将启动容器时默认有效 的UID 更改为非 root 用户。 以非 root 身份运行需要在 Dockerfile 中做的两个步骤： 确保USER指令中指定的用户存在于容器内。 在进程将要读取或写入的位置提供适当的文件系统权限。 1234567FROM alpine#创建目录，添加myuser用户，目录所有作为myuserRUN mkdir /server &amp;&amp; adduser -D myuser &amp;&amp; chown -R myuser /serverUSER myuserWORKDIR /serverCOPY myapp ./CMD [&quot;./myapp&quot;] 2.可执行文件权限应为root用户拥有但不可写 容器中的每个可执行文件都应该由 root 用户拥有，即使它由非 root 用户执行，并且不应该是全局可写的。 通过阻止执行用户修改现有的二进制文件或脚本，可以有效降低攻击，保证容器不变性。不可变容器不会在运行时自动更新其代码，通过这种方式，我们可以防止正在运行的应用程序被意外或恶意修改。 我们在使用COPY时 123COPY --chown=myuser:myuser myapp ./#应改为COPY myapp ./ 二、减少攻击面避免加载不必要的包、第三方应用或暴露端口以减少攻击面。我们在镜像中包含的组件内容越多，容器暴露的就越多，维护起来就越困难。 1.采用多阶段构建 我们在《Dockerfile 多阶段构建实践》中说到采用多阶段构建，可以此降低构建复杂度，同时有效减小镜像尺寸。 在多阶段构建中，我们创建一个中间容器（阶段），其中包含编译工具及生成最终可执行文件。然后，我们只将生成的工件复制到最终镜像中，而无需额外的开发依赖项、临时构建文件等等。 精心设计的多阶段构建仅包含最终映像中所需的最少二进制文件和依赖项，而不包含构建工具或中间文件。它更为安全，并且还减小了镜像大小。可以有效减少了攻击面，减少了漏洞。 多阶段构建的实现请参考上篇文章《Dockerfile 多阶段构建实践》 2.使用可信赖的镜像 假如我们不是从头开始构建镜像，基镜像建立在不受信任或不受维护的镜像之上会将所有问题和漏洞从该镜像继承到您的容器中。 基础镜像选择的参考： 我们应该选择来自受信任仓库和经过验证的官方镜像。 使用自定义镜像时，我们应该检查镜像源和构建的 Dockerfile。更进一步，我们甚至应该以这个Dockerfile来构建自己的基础镜像。因为我们无法保证在dockerhub等公共仓库中发布的映像确实是从指定的 Dockerfile 构建的。也不能保证它是最新的。 有时候在安全性和极简主义方面考虑，官方镜像可能并不非合适的，最优解是我们自己从头构建属于自己的镜像。 2.从头开始构建镜像 假如如果你是从centos镜像开始构建，那么你创建的容器可能将会包含几十个或者上百个漏洞。所以构建一个安全的镜像我们最好需要知道我们的基镜像存在哪些威胁。在生产中通常会从Scratch空镜像或distroless开始。 distroless镜像仅包含应用程序及其运行时依赖项。它们不包括在标准 Linux 发行版中发布应用如包管理器、shell 或任何其他程序。Distroless 镜像非常小。最小的 distroless 图像gcr.io/distroless/static大约为 650 kB。只有alpine(约2.5 MB)大小的 四分之一 ，不到debian(50 MB)大小的 1.5% 。 12345678910111213FROM golang:1.13-buster as buildWORKDIR /go/src/appADD . /go/src/appRUN go get -d -v ./...RUN go build -o /go/bin/app# 引用Distroless镜像FROM gcr.io/distroless/base-debian10COPY --from=build /go/bin/app /CMD [&quot;/app&quot;] gcr.io/distroless/base-debian10只包含一组基本的包，如包括只需要的库，如glibc、libssl和openssl 当然对于像 Go 这样不需要libc 的静态编译应用程序我们就可以替换为如下基镜像 1FROM gcr.io/distroless/static-debian10 关于distroless基镜像的更多信息可以参考https://github.com/GoogleContainerTools/distroless 3.及时更新镜像 使用经常更新的基础镜像，在需要时重构你的镜像。随着新的安全漏洞不断被发现，坚持使用最新的安全补丁是一种通用的安全最佳实践。 版本控制策略： 坚持使用稳定或长期支持版本，这些版本会迅速提供安全修复程序。 提前计划。准备好在基本镜像版本达到生命周期结束或停止接收更新之前删除旧版本并迁移。 定期重建自己的镜像，从基础发行版、Node、Golang、Python 等获取最新的包。 大多数包或依赖项管理器，如npm或go mod，将提供指定版本最新的安全更新。 4.端口暴露 容器中每个打开的端口都是通往系统的大门。我们应该仅公开应用程序需要的端口，并且避免公开 SSH (22) 等端口。 我们知道 Dockerfile 提供了EXPOSE 命令有暴露端口，但是该命令仅用于提供信息和用于文档目的。运行容器时，容器不会自动允许所有 EXPOSE 端口的连接（除非在启动容器时使用docker run --publish-all）。 启动容器时，通过-P暴露的端口应与dockerfile中EXPOSE命令指定的端口一致，这样更便于维护。 三、敏感数据管理1.凭证和密钥 禁止在 Dockerfile 指令（环境变量、参数或其他任何命令中）中放入凭据和密钥。 在复制文件到镜像时，即使文件在 Dockerfile 的后续指令中被删除，它仍然可以在之前的层上访问。因为镜像分层原理，你的文件并没有真正被删除，只是“隐藏”在最终文件系统中。因此在构建镜像时，我们应该遵循以下做法： 如果应用程序支持通过环境变量进行配置，我们可以通过docker run 中的 -e 选项配置，或者使用Docker secrets、Kubernetes secrets提供值作为环境变量。 使用配置文件并在docker 中绑定挂载配置文件，或者使用Kubernetes secret 挂载。 关于secrets的使用会在后面文章中详细介绍。 2.ADD、COPY ADD 和 COPY 指令在 Dockerfile 中提供类似的功能。但是COPY 更为明确。 除非我们确实需要 使用ADD 功能，例如从 URL 或从 tar 文件添加文件。不然最好使用 COPY，COPY 的结果更具可预测性且不易出错。 在某些情况下，最好使用 RUN 指令而不是 ADD 来下载使用curl或wget的包，解压缩然后删除原始文件，减少层数。 3.构建上下文与dockerignore 在构建时我们通常使用.作为上下文 1#docker build -t images:v1 . 使用 .作为上下文时我们需要谨慎些，因为docker CLI会将上下文中机密或不必要的文件添加到守护进程，甚至到容器中，例如配置文件、凭据、备份、锁定文件、临时文件、源、子文件夹、点文件等等。 在比如： 1COPY . /server 此时会将目录下所有内容都添加到镜像中，包括Dockfile本身。 所以正确做法是创建一个包含需要在容器内复制文件的文件夹，将其用作构建上下文，并在可能的情况下明确 COPY 指令（避免使用通配符）。例如： 1#docker build -t images:v1 build_files/ 为了排除不必要的文件，我们也可以创建一个.dockerignore文件，在其中明确排除的文件和目录。 以上是容器构建时常见安全问题与相关处理措施，容器安全涉及面广，遍布整个devops流程中。有兴趣的同学可以另外一个位面介入深究。 NEXT Docker容器secrets详解 Docker容器减小镜像尺寸实践 希望小作文对你有些许帮助，如果内容有误请指正。 您可以随意转载、修改、发布本文章，无需经过本人同意。","categories":[{"name":"docker","slug":"docker","permalink":"https://iqsing.github.io/categories/docker/"}],"tags":[{"name":"dockerfile","slug":"dockerfile","permalink":"https://iqsing.github.io/tags/dockerfile/"}]},{"title":"Dockerfile 多阶段构建实践","slug":"Dockerfile 多阶段构建实践","date":"2021-08-25T04:31:21.000Z","updated":"2022-02-04T05:16:11.686Z","comments":true,"path":"2021/08/25/Dockerfile 多阶段构建实践/","link":"","permalink":"https://iqsing.github.io/2021/08/25/Dockerfile%20%E5%A4%9A%E9%98%B6%E6%AE%B5%E6%9E%84%E5%BB%BA%E5%AE%9E%E8%B7%B5/","excerpt":"","text":"写在前面在Docker Engine 17.05 中引入了多阶段构建，以此降低构建复杂度，同时使缩小镜像尺寸更为简单。这篇小作文我们来学习一下如何编写实现多阶段构建的Dockerfile 关于dockerfile基础编写可参考之前docker容器dockerfile详解 一 、不使用多阶段构建我们知道在Dockerfile中每新增一个指令都会在镜像中生产新的层，一个高效的Dockerfile应该在继续下一层之前清除之前所有不需要的资源。 不使用多阶段构建时，我们通常会创建两dockerfile文件，一个用于开发及编译应用，另一个用于构建精简的生产镜像。这样能比较大限度的减小生产镜像的大小。 我们以一个go应用来看看。我首先会创建一个dockerfile，构建这个镜像的主要目的就是编译我们的应用。 12345FROM golang:1.16WORKDIR /go/srcCOPY app.go ./#go编译RUN go build -o myapp app.go 构建镜像 12345678910111213141516[root@localhost dockerfiles]# docker build -t builder_app:v1 .Sending build context to Docker daemon 3.072kBStep 1/4 : FROM golang:1.16 ---&gt; 019c7b2e3cb8Step 2/4 : WORKDIR /go/src ---&gt; Using cache ---&gt; 15362720e897Step 3/4 : COPY app.go ./ ---&gt; Using cache ---&gt; 8f14ac97a68aStep 4/4 : RUN go build -o myapp app.go ---&gt; Running in 4368cc4617a7Removing intermediate container 4368cc4617a7 ---&gt; 631f67587803Successfully built 631f67587803Successfully tagged builder_app:v1 这样在这个镜像中就包含了我们编译后的应用myapp，现在我们可以创建容器将myapp拷贝到宿主机等待后续使用。 123# docker create --name builder builder_app:v1fafc1cf7ffa42e06d19430b807d24eafe0bf731fc45ff0ecf31ada5a6075f1d5# docker cp builder:/go/src/myapp ./ 我们有了应用，下一步就是构建生产镜像 1234FROM scratchWORKDIR /serverCOPY myapp ./CMD [&quot;./myapp&quot;] 由于此时我们不需要其他依赖环境，所以我们采用scratch这个空镜像，不仅可以减小容器尺寸，还可以提高安全性。 构建镜像 1#docker build --no-cache -t server_app:v1 . 我们看一次构建的两个镜像大小 1234# docker images REPOSITORY TAG IMAGE ID CREATED SIZEserver_app v1 6ebc0833cad0 6 minutes ago 1.94MBbuilder_app v1 801f0b615004 23 minutes ago 921MB 显然在不使用多阶段构建时，我们也可以构建出生产镜像，但是我们需要维护两个dockerfile，需要将app遗留到本地，并且带来了更多存储空间开销。在使用多阶段构建时能比较好的解决以上问题。 二、使用多阶段构建在一个Dockerfile中使用多个FROM指令，每个FROM都可以使用不同的基镜像，并且每条指令都将开始新阶段构建。在多阶段构建中，我们可以将资源从一个阶段复制到另一个阶段，在最终镜像中只保留我们所需要的内容。 我们将上面实例的两个Dockerfile合并为如下： 12345678910#阶段1FROM golang:1.16WORKDIR /go/srcCOPY app.go ./RUN go build app.go -o myapp#阶段2FROM scratchWORKDIR /serverCOPY --from=0 /go/src/myapp ./CMD [&quot;./myapp&quot;] 构建镜像 1# docker build --no-cache -t server_app:v2 . 查看构建好的镜像 123# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEserver_app v2 20225cb1ea6b 12 seconds ago 1.94MB 这样我们无需创建额外镜像，以更简单的方式构建出了同样微小的目标镜像。可以看到在多阶段构建dockerfile中最关键的是COPY --from=0 /go/src/myapp ./ 通过 --from=0指定我们资源来源，这里的0即是指第一阶段。 命令构建阶段默认情况下构建阶段没有名称，我们可以通过整数0~N来引用，即第一个from从0开始。其实我们还可以在FROM指令中添加AS &lt;NAME&gt; 来命名构建阶段，接着在COPY指令中通过&lt;NAME&gt;引用。我们对上面dockerfile修改如下： 1234567891011#阶段1命名为builderFROM golang:1.16 as builderWORKDIR /go/srcCOPY app.go ./RUN go build app.go -o myapp#阶段2FROM scratchWORKDIR /server#通过名称引用COPY --from=builder /go/src/myapp ./CMD [&quot;./myapp&quot;] 只构建某个阶段构建镜像时，您不一定需要构建整个 Dockerfile，我们可以通过--target参数指定某个目标阶段构建，比如我们开发阶段我们只构建builder阶段进行测试。 1#docker build --target builder -t builder_app:v2 . 使用外部镜像使用多阶段构建时，我们局限于从之前在 Dockerfile 中创建的阶段进行复制。还可以使用COPY --from指令从单独的镜像复制，如本地镜像名称、本地或 Dockerhub上可用的标签或标签 ID。Docker 客户端在必要时会拉取需要的镜像到本地。 1COPY --from httpd:latest /usr/local/apache2/conf/httpd.conf ./httpd.conf 从上一阶段创建新的阶段我们可以通过FROM指令来引用上一阶段作为新阶段的开始 12345678910#阶段1命名为builderFROM golang:1.16 as builderWORKDIR /go/srcCOPY app.go ./RUN go build app.go -o myapp#阶段2FROM builder as builder_exADD dest.tar ./... 通过上面我们对dockerfile多阶段构建有了一个整体的了解。 NEXT Dockerfile 与Docker容器安全实践 希望小作文对你有些许帮助，如果内容有误请指正。 您可以随意转载、修改、发布本文章，无需经过本人同意。 个人blog：iqsing.github.io","categories":[{"name":"docker","slug":"docker","permalink":"https://iqsing.github.io/categories/docker/"}],"tags":[{"name":"dockerfile","slug":"dockerfile","permalink":"https://iqsing.github.io/tags/dockerfile/"}]},{"title":"dockerfile中ENTRYPOINT与CMD的结合","slug":"dockerfile中ENTRYPOINT与CMD的结合","date":"2021-08-22T17:30:21.000Z","updated":"2022-02-04T05:16:11.745Z","comments":true,"path":"2021/08/23/dockerfile中ENTRYPOINT与CMD的结合/","link":"","permalink":"https://iqsing.github.io/2021/08/23/dockerfile%E4%B8%ADENTRYPOINT%E4%B8%8ECMD%E7%9A%84%E7%BB%93%E5%90%88/","excerpt":"","text":"一、写在前面我们在上篇小作文docker容器dockerfile详解对中dockerfile有了比较全面的认识，我们也提到ENTRYPOINT和CMD都可以指定容器启动命令。因为这两个命令是掌握dockerfile编写的核心，所以这边还是单独拿出来再讲一讲。 二、CMD 与 ENTRYPOINT主要区别我们直接进入主题，CMD 与 ENTRYPOINT都是用于指定启动容器执行的命令，区别在于： 当docker run 命令中有参数时，守护进程会忽略CMD命令。 使用ENTRYPOINT指令不会忽略，并且会接收docker run 参数附加到命令行中。 为了使构建的容器可以正常启动，我们编写的dockerfile文件必须包含一个CMD或ENTRYPOINT指令。 三、CMD 与 ENTRYPOINT的结合使用1.CMDCMD指令有三种形式： CMD [&quot;executable&quot;,&quot;param1&quot;,&quot;param2&quot;]（exec形式，这是首选形式） CMD [&quot;param1&quot;,&quot;param2&quot;]（作为ENTRYPOINT 的默认参数） CMD command param1 param2（shell形式） dockerfile文件中包含多个CMD时，只有最后一个被加载使用。 我们在dockerhub中搜索centos官方镜像，看一下的官方dockerfile文件。 基本上每一个官方镜像都会为我们提供各自版本的dockerfile链接，如下： 我们查看latest标签的dockerfile 1234FROM scratchADD centos-8-x86_64.tar.xz /LABEL org.label-schema.schema-version=&quot;1.0&quot; org.label-schema.name=&quot;CentOS Base Image&quot; org.label-schema.vendor=&quot;CentOS&quot; org.label-schema.license=&quot;GPLv2&quot; org.label-schema.build-date=&quot;20201204&quot;CMD [&quot;/bin/bash&quot;] 只有四行，这就是构建一个latest版本centos8.3.2011镜像的dockerfile全部内容。指定基镜像（这里从scratch这个空镜像开始构建），添加rootfs内容，打标签，通过CMD指定启动命令。 不止centos，其他debian、ubuntu、busybox等镜像都只需通过CMD指定启动命令。比如busybox更为简约： 123FROM scratchADD busybox.tar.xz /CMD [&quot;sh&quot;] 这种基础类、工具类镜像的构建我们只需要指定一个必要CMD来启动容器即可。但是我们编写一个dockerfile并不是为了启动容器而编写，大多数时候我们要在容器运行我们的app，运行我们的服务。 当然通过CMD也可以启动，可是如此一来有一个缺陷，我们上面说到的CMD的启动命令会被docker run 参数代替。 我们有下面Dockerfile 123[root@localhost dockerfiles]# cat Dockerfile FROM centosCMD [&quot;/bin/top&quot;,&quot;-b&quot;] 构建后，使用参数ps启动容器。 123[root@localhost dockerfiles]# docker run -it centos_top:v1 ps PID TTY TIME CMD 1 pts/0 00:00:00 ps 可看看到启动容器后top -b 已经被替换为ps，并非实现参数的替换。显然这不是我们想要的。有没有什么办法既可以默认启动应用，又可以加载到docker run 参数？这就是接下来ENTRYPOINT与CMD的妙用。 2.ENTRYPOINT结合CMDENTRYPOINT的exec和shell形式： ENTRYPOINT [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;] ENTRYPOINT command param1 param2 上面我们提到CMD [&quot;param1&quot;,&quot;param2&quot;]形式可以作为ENTRYPOINT参数，同时ENTRYPOINT 指定的命令无法被docker run 参数取代。假如我们把CMD和ENTRYPOINT两个指令相结合，这样我们就可以通过CMD来接收docker run 参数，然后把参数传递给ENTRYPOINT执行。 我们以nginx官方dockerfile latest版本1.21为例 首先我们查看Dockerfile，这里我们只关注启动命令，如下： 123456789101112...COPY docker-entrypoint.sh /COPY 10-listen-on-ipv6-by-default.sh /docker-entrypoint.dCOPY 20-envsubst-on-templates.sh /docker-entrypoint.dCOPY 30-tune-worker-processes.sh /docker-entrypoint.dENTRYPOINT [&quot;/docker-entrypoint.sh&quot;]EXPOSE 80STOPSIGNAL SIGQUITCMD [&quot;nginx&quot;, &quot;-g&quot;, &quot;daemon off;&quot;] 从上面我们可以看到，在启动nginx容器时首先运行docker-entrypoint.sh脚本并把CMD命令中的参数nginx -g &quot;daemon off;&quot;传递进来。即docker run不添加参数时启动容器相当于执行如下脚本与默认参数。 1#docker-entrypoint.sh nginx -g &quot;daemon off;&quot; 当我们使用docker run 传入参数会怎样？ 我启动nginx-debug 1#docker run -dt nginx nginx-debug -g &quot;daemon off;&quot; 此时启动容器相当于执行如下脚本与参数 1#docker-entrypoint.sh nginx-debug -g &quot;daemon off;&quot; 我们通过ps来看一下我们启动的容器 12345678[root@localhost dockerfiles]# ps -ef|grep nginxroot 6327 6306 0 Aug12 pts/0 00:00:00 nginx: master process nginx -g daemon off;101 6384 6327 0 Aug12 pts/0 00:00:00 nginx: worker process101 6385 6327 0 Aug12 pts/0 00:00:00 nginx: worker processroot 16800 16780 3 12:51 pts/0 00:00:00 nginx: master process nginx-debug -g daemon off;101 16857 16800 0 12:51 pts/0 00:00:00 nginx: worker process101 16858 16800 0 12:51 pts/0 00:00:00 nginx: worker process 显然我们两种参数nginx、nginx-debug的容器都启动成功！ 也就是说我们通过ENTRYPOINT [&quot;/docker-entrypoint.sh&quot;]指定的命令在启动时无论如何都会执行，并且可以接收到了docker run 的参数。 docker-entrypoint.sh是什么？docker-entrypoint.sh这是一个预处理脚本通常用来过滤命令行参数或者执行exec 来启动容器为1的进程。 通过ENTRYPOINT+CMD实现命令默认参数或接收docker run 参数是一种非常流行并且有用的dockerfile编写方式。 希望小作文对你有些许帮助，如果内容有错误请指正。 您可以随意转载、修改、发布本文章，无需经过本人同意。 个人blog：iqsing.github.io","categories":[{"name":"docker","slug":"docker","permalink":"https://iqsing.github.io/categories/docker/"}],"tags":[{"name":"dockerfile","slug":"dockerfile","permalink":"https://iqsing.github.io/tags/dockerfile/"}]},{"title":"docker容器dockerfile详解","slug":"docker容器dockerfile详解","date":"2021-08-19T02:57:21.000Z","updated":"2022-02-04T05:16:11.762Z","comments":true,"path":"2021/08/19/docker容器dockerfile详解/","link":"","permalink":"https://iqsing.github.io/2021/08/19/docker%E5%AE%B9%E5%99%A8dockerfile%E8%AF%A6%E8%A7%A3/","excerpt":"","text":"docker公司在容器技术发展中提出了镜像分层的理念，可以说也是这个革命性的理念让原本只不过是整合linux内核特性的容器，开始野蛮生长。 docker通过UnionFS联合文件系统将镜像的分层实现合并,关于镜像相关知识有兴趣的同学可参考我们之前文章《docker容器技术基础之联合文件系统OverlayFS》 本文是对docker官方文档Dockerfile reference学习与实践，在学习docker容器相关技术的同学别光收藏，你要动起来！实践起来！ 提示：没有人比docker公司更懂docker，本小作文含部分自己的理解，有英文阅读习惯的同学，建议直接阅读官方文档哈。 docker buildDockerfile是一个镜像构建命令集合的文本文件，下面是我们最常见的Dockerfile构建,假如我们目录下有一个文件Dockerfile 123[root@localhost nginx_project]# lsDockerfile[root@localhost nginx_project]# docker build -t nginx:v1 . 通过build指定了目标镜像的标签为nginx:v1，以及Dockerfile的上下文context . 什么是docker上下文？ 一个面向服务端的目录夹结构，除了Dockerfile，你的一切构建资源都应该在这个目录（指定的上下文）中。 上下文是递归处理的。因此， 如果是PATH则包含任何子目录，如果是一个URL则包含存储库及其子模块。 关键点，构建是由 Docker 守护程序运行，而不是由 CLI 运行，所以docker会把上下文资源打包传输给守护进程进行构建，为了减少不必要的臃肿，最好从一个空目录作为上下文开始，并将 Dockerfile 保存在该目录中。仅添加构建 Dockerfile 所需的文件。 我们可以使用-f选项指定dockerfile 1[root@localhost folder]# docker build -f ../Dockerfile -t nginx:v1 . 使用多个-t选项保持多个tag 12345678910[root@localhost folder]# docker build -t nginx:v1 -t dockerhub.com/nginx:v2 .Sending build context to Docker daemon 1.583kBStep 1/2 : FROM nginx ---&gt; 08b152afcfaeStep 2/2 : run echo 123 ---&gt; Using cache ---&gt; 3b636c79fbfaSuccessfully built 3b636c79fbfaSuccessfully tagged nginx:v1Successfully tagged dockerhub.com/nginx:v2 这样就构建两个不同tag的同一ID镜像 1234[root@localhost folder]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEdockerhub.com/nginx v2 3b636c79fbfa 23 minutes ago 133MBnginx v1 3b636c79fbfa 23 minutes ago 133MB BuildKitbuildkit将 Dockerfile 变成了 Docker 镜像。它不只是构建 Docker 镜像；它可以构建 OCI 图像和其他几种输出格式。 从版本18.09开始，Docker支持由moby / buildkit项目提供的用于执行构建的新后端。与旧的实现相比，BuildKit后端提供了许多好处。例如，BuildKit可以： 检测并跳过执行未使用的构建阶段。 平行构建独立的构建阶段。 在不同的构建过程中,只增加传输构建上下文中的更改文件。 在构建上下文中检测并跳过传输未使用的文件。 使用外部Dockerfile实现许多新功能。 避免与API的其他部分(中间镜像和容器)产生副作用。 优先处理您的构建缓存,以便自动修剪。 要使用BuildKit后端，只需要在调用 DOCKER_BUILDKIT=1 docker build 之前在CLI上设置环境变量DOCKER_BUILDKIT = 1。或者配置/etc/docker/daemon.json启用。 1234567891011121314[root@localhost folder]# DOCKER_BUILDKIT=1 docker build -f ../Dockerfile -t nginx:v1 -t dockerhub.com/nginx:v2 .[+] Building 5.2s (6/6) FINISHED =&gt; [internal] load build definition from Dockerfile 0.7s =&gt; =&gt; transferring dockerfile: 118B 0.0s =&gt; [internal] load .dockerignore 0.6s =&gt; =&gt; transferring context: 2B 0.0s =&gt; [internal] load metadata for docker.io/library/nginx:latest 0.0s =&gt; [1/2] FROM docker.io/library/nginx 2.2s =&gt; [2/2] RUN echo 123 1.3s =&gt; exporting to image 0.5s =&gt; =&gt; exporting layers 0.2s =&gt; =&gt; writing image sha256:813b09c58322dce98ee28e717baeb9f3593ce3e46a032488949250f761004495 0.0s =&gt; =&gt; naming to docker.io/library/nginx:v1 0.0s =&gt; =&gt; naming to dockerhub.com/nginx:v2 dockerfile格式1、注释一个标准的dockerfile，注释是必须的。 1234#这是dockerfile注释,dockerfile中指令以&quot;CMD args&quot;格式出现CMD argsCMD args... 一个Dockerfile 第一个指令必须是FROM指令，用于指定基础镜像，那么基础镜像的父镜像从哪里来？答案是scratch带有该FROM scratch指令的 Dockerfile会创建一个基本映像。 2.解析器指令解析器指令是可选的，会影响 aDockerfile中后续行的处理方式。解析器指令不会向构建添加层，也不会显示为构建步骤，单个指令只能使用一次。 dockerfile目前支持以下两个解析器指令： syntax escape 2.1syntax此功能仅在使用BuildKit后端时可用，在使用经典构建器后端时会被忽略。 我们可以在dockerfile文件开头指定此dockerfile语法解析器，如下： 123# syntax=docker/dockerfile:1# syntax=docker.io/docker/dockerfile:1# syntax=example.com/user/repo:tag@sha256:abcdef... 通过syntax自定义 Dockerfile 语法解析器可以实现如下： 在不更新 Docker 守护进程的情况下自动修复错误 确保所有用户都使用相同的解析器来构建您的 Dockerfile 无需更新 Docker 守护程序即可使用最新功能 在将新功能或第三方功能集成到 Docker 守护进程之前试用它们 使用替代的构建定义，或创建自己的定义 官方dockerfile解析器： docker/dockerfile:1 不断更新最新的1.x.x次要和补丁版本 docker/dockerfile:1.2 保持更新最新的1.2.x补丁版本，一旦版本1.3.0发布就停止接收更新。 docker/dockerfile:1.2.1 不可变：从不更新1.2版本 比如我们使用1.2最新补丁版本，我们的Dockerfile如下： 123#syntax=docker/dockerfile:1.2FROM busyboxrun echo 123 我们启用buildkit构建 123456789101112131415# DOCKER_BUILDKIT=1 docker build -t busybox:v1 .[+] Building 5.8s (8/8) FINISHED =&gt; [internal] load build definition from Dockerfile 0.3s =&gt; =&gt; transferring dockerfile: 150B 0.0s =&gt; [internal] load .dockerignore 0.4s =&gt; =&gt; transferring context: 2B 0.0s =&gt; resolve image config for docker.io/docker/dockerfile:1.2 2.6s =&gt; CACHED docker-image://docker.io/docker/dockerfile:1.2@sha256:e2a8561e419ab1ba6b2fe6cbdf49fd92b95 0.0s =&gt; [internal] load metadata for docker.io/library/busybox:latest 0.0s =&gt; [1/2] FROM docker.io/library/busybox 0.3s =&gt; [2/2] RUN echo 123 1.1s =&gt; exporting to image 0.3s =&gt; =&gt; exporting layers 0.3s =&gt; =&gt; writing image sha256:bd66a3db9598d942b68450a7ac08117830b4d66b68180b6e9d63599d01bc8a04 0.0s =&gt; =&gt; naming to docker.io/library/busybox:v1 2.2 escape通过escape定义dockerfile的换行拼接转义符 1# escape=\\ 如果要构建一个window镜像就有大用处了，我们看下面dockerfile 123FROM microsoft/nanoserverCOPY testfile.txt c:\\\\RUN dir c:\\ 由于默认转义符为\\，则在构建的第二步step2会是这样COPY testfile.txt c:\\RUN dir c:显然与我们的预期不符。 我们把转义符换成`号即可 12345# escape=`FROM microsoft/nanoserverCOPY testfile.txt c:\\ `RUN dir c:\\ 3.类bash的环境变量12345FROM busyboxENV FOO=/barWORKDIR $&#123;FOO&#125; # WORKDIR /barADD . $FOO # ADD . /barCOPY \\$FOO /quux # COPY $FOO /quux $&#123;variable_name&#125;语法还支持bash 指定的一些标准修饰符： $&#123;variable:-word&#125;表示如果variable变量被设置（存在），则结果将是该值。如果variable未设置，word则将是结果。 $&#123;variable:+word&#125;表示如果variable被设置则为word结果，否则为空字符串。 4. .dockerignore.dockerignore用于忽略CLI发送到docker守护进程的文件或目录。以下是一个.dockerignore文件 123456#.dockeringre可以有注释*.md!README.mdtemp?*/temp**/*/temp* 规则 行为 */temp* 排除名称以temp根目录的任何直接子目录开头的文件和目录。例如，纯文件/somedir/temporary.txt被排除在外，目录/somedir/temp. */*/temp* 排除temp从根目录下两级的任何子目录开始的文件和目录。例如，/somedir/subdir/temporary.txt被排除在外。 temp? 排除根目录中名称为一个字符扩展名的文件和目录temp。例如，/tempa和/tempb被排除在外。 ！ 不排除到文件 dockerfile命令1.FROM指定基础镜像。一般格式如下，[]括号内容可省略： 1FROM [--platform=&lt;platform&gt;] &lt;image&gt;[:&lt;tag&gt;] [AS &lt;name&gt;] 特别需要注意的是FROM在一个dockerfile中可以多次出现，以实现多阶段构建。并且可以和ARG 参数交互。如下： 123456ARG CODE_VERSION=latestFROM base:$&#123;CODE_VERSION&#125;CMD /code/run-appFROM extras:$&#123;CODE_VERSION&#125;CMD /code/run-extras 我们加载了两个通过arg参数指定的不同版本基础镜像。 2.RUNRUN的两种形式 RUN 首选， (命令在shell中运行,即默认为/bin/sh -c ) RUN [“exec”,param1,param2] RUN命令主要是在镜像构建时执行，形成新层。比如我们经常会看到在构建镜像时安装相关软件。 1RUN yum install -y gcc 当我们不想使用默认shell是可以采用exec形式实现 1RUN [&quot;/bin/bash&quot;,&quot;-c&quot;,&quot;yum install -y gcc&quot;] 当然，exec形式可以不使用shell 1RUN [&quot;yum&quot;,&quot;install&quot;,&quot;-y&quot;,&quot;gcc&quot;] EXEC形式被解析为一个JSON阵列，所以必须使用双引号 3.CMDCMD指令有三种形式： CMD [&quot;executable&quot;,&quot;param1&quot;,&quot;param2&quot;]（exec形式，这是首选形式） CMD [&quot;param1&quot;,&quot;param2&quot;]（作为ENTRYPOINT 的默认参数） CMD command param1 param2（shell形式） 一个dockerfile中，应该只写一个CMD，如果有多个只有最后一个生效。在实际编写dockerfie时CMD命令常常用于为ENTRYPOINT提供默认值，后面我们会讲到。 与RUN相比，CMD在构建时不会执行任何操作，主要用于指定镜像的启动命令。CMD的启动命令可以被docker run 参数代替。 我们在dockerfile中添加如下CMD命令 1CMD echo hello 构建镜像后，docker run 不添加参数，启动容器 12[root@localhost dockerfiles]# docker run centos:v1hello 当我们在docker run 添加参数后 12[root@localhost dockerfiles]# docker run centos_env:v1 echo containercontainer 显然我们CMD命令echo hello已被docker run中的参数echo container取代。 4. LABELlabel用于添加镜像的元数据，采用key-value的形式。 1LABEL &lt;key&gt;=&lt;value&gt; 比如我们添加如下LABEL 123LABEL &quot;miantainer&quot;=&quot;iqsing.github.io&quot;LABEL &quot;version&quot;=&quot;v1.2&quot;LABEL &quot;author&quot;=&quot;waterman&amp;&amp;iqsing&quot; 为了防止创建三层，我们最好通过一个标签来写。 123LABEL &quot;miantainer&quot;=&quot;iqsing.github.io&quot; \\ &quot;version&quot;=&quot;v1.2&quot; \\ &quot;author&quot;=&quot;waterman&amp;&amp;iqsing&quot; 我们通过docker inspect 来查看镜像label信息 12345678910111213#docker inspect centos_labels:v1&quot;Labels&quot;: &#123; &quot;author&quot;: &quot;waterman&amp;&amp;iqsing&quot;, &quot;miantainer&quot;: &quot;iqsing.github.io&quot;, &quot;org.label-schema.build-date&quot;: &quot;20201204&quot;, &quot;org.label-schema.license&quot;: &quot;GPLv2&quot;, &quot;org.label-schema.name&quot;: &quot;CentOS Base Image&quot;, &quot;org.label-schema.schema-version&quot;: &quot;1.0&quot;, &quot;org.label-schema.vendor&quot;: &quot;CentOS&quot;, &quot;version&quot;: &quot;v1.2&quot;&#125; 5.EXPOSE12EXPOSE 80/tcpEXPOSE 161/udp 注意，EXPOSE只是告诉dockerfile的阅读者，我们构建的镜像需要暴露哪些端口，只是一个信息。在容器中还是需要通过-p选项来暴露端口。 6.ENV123ENV &lt;key&gt;=&lt;value&gt; ... 首先方式或ENV &lt;key&gt; &lt;value&gt; 通过ENV指定环境变量，将作用于在构建阶段的所有后续指令的环境中。 1ENV username=&quot;iqsing&quot; 这样当我们启动这个容器后可以查看到容器信息已经附带了ENV环境变量 1234&quot;Env&quot;: [&quot;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&quot;,&quot;username=iqsing&quot;], 当然我们也可以在启动容器时添加环境变量 1docker run --env &lt;key&gt;=&lt;value&gt; 另外如果只需要在镜像构建期间使用环境变量，更好的选择是使用ARG参数来处理 7.ADD &amp;&amp; COPYADD和COPY格式相似，有两种形式,包含空格的路径需要后一种形式： 12345ADD [--chown=&lt;user&gt;:&lt;group&gt;] &lt;src&gt;... &lt;dest&gt;ADD [--chown=&lt;user&gt;:&lt;group&gt;] [&quot;&lt;src&gt;&quot;,... &quot;&lt;dest&gt;&quot;]COPY [--chown=&lt;user&gt;:&lt;group&gt;] &lt;src&gt;... &lt;dest&gt;COPY [--chown=&lt;user&gt;:&lt;group&gt;] [&quot;&lt;src&gt;&quot;,... &quot;&lt;dest&gt;&quot;] 在linux平台中可以对添加到远程目录或文件设置所属用户和组。 &lt;SRC&gt; 指复制新文件、目录或远程文件 URL，每&lt;src&gt;可以包含通配符，如下： 12ADD hom* /mydir/ADD hom?.txt /mydir/ 一般使用中，ADD、COPY都遵守以下规则： &lt;src&gt;路径必须是内部语境的构建; 你不能COPY ../something /something，因为 docker build是将上下文目录（和子目录）发送到 docker 守护进程。 如果&lt;src&gt;是目录，则复制目录的全部内容，包括文件系统元数据。 如果&lt;src&gt;是任何其他类型的文件，则将其与其元数据一起单独复制。在这种情况下，如果&lt;dest&gt;以斜杠结尾/，它将被视为一个目录，其内容&lt;src&gt;将被写入&lt;dest&gt;/base(&lt;src&gt;)。 如果&lt;src&gt;直接指定了多个资源，或者由于使用了通配符，则&lt;dest&gt;必须是目录，并且必须以斜杠结尾/。 如果&lt;dest&gt;不以斜杠结尾，则将其视为常规文件，并将其内容&lt;src&gt;写入&lt;dest&gt;. 如果&lt;dest&gt;不存在，则在其路径中创建所有丢失的目录。 特别的，当是可识别的压缩包如gzip、bzip2等tar包时，首先会将包添加到镜像中，然后自动解压。这可以说是与COPY命令在使用中的最大的区别。 8.ENTRYPOINTexec首选和shell形式: 12ENTRYPOINT [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;]ENTRYPOINT command param1 param2 ENTRYPOINT 和CMD很相似，都是指定启动命令，不同之处在于ENTRYPOINT 指定的命令无法被docker run 参数取代。 我们在dockerfile中添加ENTRYPOINT 1ENTRYPOINT echo hello container 构建镜像并启动容器，可以看到docker run 中的参数并未取代ENTRYPOINT 12[root@localhost dockerfiles]# docker run centos_entrtpoint:v1 echo hello dockerhello container 这指令优秀的另一个地方在于可以和CMD指令做交互。让容器以应用或者服务运行。 经典操作：ENTRYPOINT + CMD = 默认容器命令参数 ENTRYPOINT是dockerfile中非常重要的指令，有必要另写一篇小作文深入学习一下这东西。 9.VOLUME1VOLUME [&quot;/data&quot;] volume指令可以用于创建存储卷，我来看一下实例： 1234FROM centosRUN mkdir /volumeRUN echo &quot;hello world&quot; &gt; /volume/greetingVOLUME /volume 构建镜像后，创建一个容器 12345678910111213141516[root@localhost dockerfiles]# docker create --name centos_volume centos_volue:v1[root@localhost dockerfiles]# docker inspect centos_volume &quot;Mounts&quot;: [ &#123; &quot;Type&quot;: &quot;volume&quot;, &quot;Name&quot;: &quot;494cdb193984680045c36a16bbc2b759cf568b55c7e9b0852ccf6dff8bf79c46&quot;, &quot;Source&quot;: &quot;/var/lib/docker/volumes/494cdb193984680045c36a16bbc2b759cf568b55c7e9b0852ccf6dff8bf79c46/_data&quot;, &quot;Destination&quot;: &quot;/volume&quot;, &quot;Driver&quot;: &quot;local&quot;, &quot;Mode&quot;: &quot;&quot;, &quot;RW&quot;: true, &quot;Propagation&quot;: &quot;&quot; &#125; ], 这样我们就通过VOLUME指令创建一个存储卷，你可以通过--volumes-from共享这个容器，可参考我之前的小作文《docker容器存储》 10.USER指定指令集所属用户和组。组默认为root。可以作用于RUN，CMD和 ENTRYPOINT它们后面的指令。 123USER &lt;user&gt;[:&lt;group&gt;]或USER &lt;UID&gt;[:&lt;GID&gt;] 11.WORKDIR指定指令集所在的工作目录，若目录不存在将会自动创建。可作用于RUN，CMD， ENTRYPOINT，COPY和ADD 1WORKDIR /path/to/workdir 12.ARG1ARG &lt;name&gt;[=&lt;default value&gt;] ARG指令定义了一个变量，我们可以在docker build通过使用--build-arg &lt;varname&gt;=&lt;value&gt; 标志的命令将其传递给构建器。 如果ARG指令具有默认值并且在构建时没有传递任何值，则构建器使用默认值。 在多阶段构建应该添加多个ARG ENV变量会覆盖ARG变量 与ENV变量相比，ARG变量多用于构建，无法驻留在镜像中。 13.STOPSIGNAL配置容器退出时的系统调用 1STOPSIGNAL signal 14.HEALTHCHECKHEALTHCHECK指令有两种形式： HEALTHCHECK [OPTIONS] CMD command （通过在容器内运行命令来检查容器健康状况） HEALTHCHECK NONE （禁用从基础镜像继承的任何健康检查） OPTIONS支持如下参数： --interval=DURATION（默认值：30s） --timeout=DURATION（默认值：30s） --start-period=DURATION（默认值：0s） --retries=N（默认值：3） 比如我们可以添加如下参数用于检查web服务： 12HEALTHCHECK --interval=5m --timeout=3s \\ CMD curl -f http://localhost/ || exit 1 每五分钟左右检查一次web服务器能否在3s内响应。如果失败则返回状态码1 命令的退出状态指示容器的健康状态。可能的值为： 0：成功 - 容器运行良好，可以使用 1：不健康 - 容器无法正常工作 2：reserved - 不要使用这个退出代码 编写一个优质的Dockerfile并不容易，你需要考虑所构建镜像的迭代、服务稳定运行、启动与停止、安全等等问题，希望这篇小作文可以帮助你对Dockerfile有多一点了解。 您可以随意转载、修改、发布本文章，无需经过本人同意。 个人blog：iqsing.github.io NEXT Dockerfile 理解ENTRYPOINT与CMD结合 Dockerfile 多阶段构建实践 Dockerfile 与docker容器安全实践","categories":[{"name":"docker","slug":"docker","permalink":"https://iqsing.github.io/categories/docker/"}],"tags":[{"name":"dockerfile","slug":"dockerfile","permalink":"https://iqsing.github.io/tags/dockerfile/"}]},{"title":"docker容器存储","slug":"docker容器存储","date":"2021-08-12T04:47:21.000Z","updated":"2022-02-04T05:16:11.773Z","comments":true,"path":"2021/08/12/docker容器存储/","link":"","permalink":"https://iqsing.github.io/2021/08/12/docker%E5%AE%B9%E5%99%A8%E5%AD%98%E5%82%A8/","excerpt":"","text":"写在前面我们在上篇学习了容器网络，对容器网络驱动bridge工作原理做了较为详细的介绍，今天小作文一起看看容器中另一个关键域-存储。 容器的存储可以分为两大类： 一种是与镜像相关的即我们在《docker容器技术基础之联合文件系统OverlayFS》一文提到的容器层Copy-On-Write特性。默认情况下，在容器内创建的所有文件都存储在可写容器层上，这种直接将文件存储在容器层的方式数据难以持久化和共享，由于依赖存储驱动与使用直接写入主机文件系统的数据卷相比，这种额外的抽象会降低性能 。 另一中是宿主机存储即通过将宿主机目录绑定或挂在到容器中使用，容器停止后数据也能持久化。小作文主要介绍后者。 几种存储挂载方式这里我们根据数据存储在 Docker 主机上的不同位置绘制如下图： 1.bind mounts绑定挂载与卷相比，功能有限。使用绑定挂载时，主机上的文件或目录会挂载到容器中。文件或目录由其在主机上的完整路径引用。目录不需要已经存在于 Docker 主机上，如果不存在，docker会帮我们创建。注意一下，只能自动创建目录哦。 我们通过 -v 选项绑定挂载一个目录 /nginx/html 到容器中看看 1docker run -dt -v /nginx/html:/usr/share/nginx/html --name nginx nginx 通过docker inspect nginx 查看容器 Mounts字段 12345678910&quot;Mounts&quot;: [ &#123; &quot;Type&quot;: &quot;bind&quot;, &quot;Source&quot;: &quot;/nginx/html&quot;, &quot;Destination&quot;: &quot;/usr/share/nginx/html&quot;, &quot;Mode&quot;: &quot;&quot;, &quot;RW&quot;: true, &quot;Propagation&quot;: &quot;rprivate&quot; &#125;], 接着我们在docker主机上创建一个index.html并写入hello nginx，然后访问容器IP，显然我们的挂载已经生效了。 123[root@localhost ~]# echo &quot;hello nginx&quot; &gt; /nginx/html/index.html[root@localhost ~]# curl 172.17.0.4hello nginx 这里有一个问题，我们可以通过docker主机修改文件使容器内文件生效，反过来也一样，容器可以修改、创建和删除主机文件系统上的内容。处理这个问题我们可以在创建容器的时候配置挂载目录的权限，比如下面的只读权限： 1docker run -dt -v /nginx/html:/usr/share/nginx/html:ro --name nginx nginx 所以在我们使用绑定挂载的时候，你操作的是主机文件系统，你必须清楚如下： 你挂载的目录包含哪些内容，以免对其他应用造成影响。 你的容器是否应该有权操作这些目录。 2.volumesvolume存储卷由 Docker 创建和管理，我们可以使用该docker volume create命令显式的创建卷，或者在容器创建时创建卷。 123456789101112131415[root@localhost ~]# docker volume create nginx_volumenginx_volume[root@localhost volumes]# docker inspect nginx_volume[ &#123; &quot;CreatedAt&quot;: &quot;2021-08-12T01:58:04-04:00&quot;, &quot;Driver&quot;: &quot;local&quot;, &quot;Labels&quot;: &#123;&#125;, &quot;Mountpoint&quot;: &quot;/var/lib/docker/volumes/nginx_volume/_data&quot;, &quot;Name&quot;: &quot;nginx_volume&quot;, &quot;Options&quot;: &#123;&#125;, &quot;Scope&quot;: &quot;local&quot; &#125;] 可以看到挂载点处于docker的根目录/var/lib/docker/volumes下 通过docker volume rm/prune 清除单个或所有未再使用的卷，可以通过docker 命令来管理卷是对比绑定挂载的一个优势。 1234567891011121314[root@localhost ~]# docker volume lsDRIVER VOLUME NAMElocal owncloud-docker-server_fileslocal owncloud-docker-server_mysqllocal owncloud-docker-server_redis[root@localhost ~]# docker volume pruneWARNING! This will remove all local volumes not used by at least one container.Are you sure you want to continue? [y/N] yDeleted Volumes:owncloud-docker-server_filesowncloud-docker-server_mysqlowncloud-docker-server_redisTotal reclaimed space: 199.4MB 在创建容器时如果未指定容器挂载的源则docker会自动为我们创建一个匿名卷，同样位于docker根目录下。 123456789[root@localhost volumes]# docker run -dt -v /usr/share/nginx/html --name nginx_with_volume nginxd25bdfce9c7ac7bde5ae35067f6d9cf9f0cd2c9cbea6d1bbd7127b3949ef5ac6[root@localhost volumes]# docker volume ls DRIVER VOLUME NAMElocal d8e943f57d17a255f8a4ac3ecbd6471a735aa64cc7a606c52f61319a6c754980local nginx_volume[root@localhost volumes]# ls /var/lib/docker/volumes/backingFsBlockDev d8e943f57d17a255f8a4ac3ecbd6471a735aa64cc7a606c52f61319a6c754980 metadata.db nginx_volume 当我们创建挂载卷之后，此时的存储与bind mounts是一致，不过当 docker 主机不能保证具有给定的目录或文件结构时，卷可帮助我们将 docker 主机的配置与容器运行时分离。这样一来当我们需要将数据从一台 Docker 主机备份、还原或迁移到另一台时，卷就很方便了，可以脱离host path的限制。 在使用绑定挂载和卷时我们要注意下面传播覆盖原则： 挂载一个空卷时：容器内目录的内容会传播（复制）到卷中。 绑定挂载或非空卷时：容器内目录的内容会被卷或绑定的主机目录覆盖。 3.tmpfs mounttmpfs挂载仅适用于linux主机，当我们使用tmpfs挂载创建容器时，容器可以在容器的可写层之外创建文件。将数据保留在内存中，当容器停止时，写入的数据也将被移除。主要用于临时存储不想保留在主机或容器可写层中的敏感文件。 通过--tmpfs选项挂载一个内存块。 1docker run -dt --name busybox_tmpfs --tmpfs /etc/running busybox 通过--mount的方式带上参数,指定临时存储大小。 1docker run -dt --name busybox_tmpfs2 --mount type=tmpfs,tmpfs-size=2048,destination=/etc/running busybox 存储数据共享在容器之间共享数据主要有两种方法，第一种比较简单，只需要将目录或者volume挂载到多个容器中即可。这里不做赘述，我们来看一下通过中间容器实现共享的方式。 我们创建一个中间容器,包含绑定挂载目录和一个卷。 1docker create -v /share:/volume1 -v /volume2 --name volume_share busybox 在我们需要共享的容器中通过选项--volumes-from拿过来用即可 1docker run -d -t --volumes-from volume_share --name container1 busybox 我们inspect检查一下Mounts字段，此时container1已经挂载到了一个bind目录和一个volume 123456789101112131415161718192021&quot;Mounts&quot;: [ &#123; &quot;Type&quot;: &quot;bind&quot;, &quot;Source&quot;: &quot;/share&quot;, &quot;Destination&quot;: &quot;/volume1&quot;, &quot;Mode&quot;: &quot;&quot;, &quot;RW&quot;: true, &quot;Propagation&quot;: &quot;rprivate&quot; &#125;, &#123; &quot;Type&quot;: &quot;volume&quot;, &quot;Name&quot;: &quot;21605e49a0ba90a1b952a32c1b3f0d42735da8bfe718f0dc76c37e91f1e51c0e&quot;, &quot;Source&quot;: &quot;/var/lib/docker/volumes/21605e49a0ba90a1b952a32c1b3f0d42735da8bfe718f0dc76c37e91f1e51c0e/_data&quot;, &quot;Destination&quot;: &quot;/volume2&quot;, &quot;Driver&quot;: &quot;local&quot;, &quot;Mode&quot;: &quot;&quot;, &quot;RW&quot;: true, &quot;Propagation&quot;: &quot;&quot; &#125;], 关于docker容器存储我们先学习到这，希望这篇小作文在你需要时对你有点用。 blog：iqsing.github.io 您可以随意转载、修改、发布本文章，无需经过本人同意。","categories":[{"name":"docker","slug":"docker","permalink":"https://iqsing.github.io/categories/docker/"}],"tags":[{"name":"storage","slug":"storage","permalink":"https://iqsing.github.io/tags/storage/"}]},{"title":"docker容器网络bridge","slug":"docker容器网络bridge","date":"2021-08-10T04:47:21.000Z","updated":"2022-02-04T05:16:11.806Z","comments":true,"path":"2021/08/10/docker容器网络bridge/","link":"","permalink":"https://iqsing.github.io/2021/08/10/docker%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9Cbridge/","excerpt":"","text":"我们知道docker利用linux内核特性namespace实现了网络的隔离，让每个容器都处于自己的小世界里面，当这个小世界需要与外界（宿主机或其他容器）通信的时候docker的网络就发挥作用了，这篇小作文我们一起来学习一下docker容器网络基础，这里我们会着重学习bridge模式的工作原理。 docker提供了三个开箱即用的网络模式（驱动）bridge、host 和 none 我们通过ls看一下相关信息： 12345[root@bogon /]# docker network ls NETWORK ID NAME DRIVER SCOPEf33b32dd8351 bridge bridge local2c102587be2f host host local04a61e996030 none null local 一、host与none模式在host模式下，容器共享主机的网络命名空间，直接将其暴露给外界。同时主机名与宿主机一致。 123456789101112131415161718[root@bogon proc]# docker run -it --name busybox_host --net host busybox/ # ip addr1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: ens192: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq qlen 1000 link/ether 00:0c:29:45:89:e6 brd ff:ff:ff:ff:ff:ff inet 192.168.1.123/24 brd 192.168.1.255 scope global noprefixroute ens192 valid_lft forever preferred_lft forever inet6 fe80::806e:979a:d38f:a85b/64 scope link noprefixroute valid_lft forever preferred_lft forever/ # hostnamelocalhost host模式主要用于对网络性能要求高、追求传输效率的服务，但是需要注意的是会占用主机的端口，多个容器工作可能需要处理冲突。 none模式下，不配置网络，并且无法访问外部网络以及其他容器，对于不需要网络访问的容器很有用，比如批处理应用，密码管理等等。 123456[root@bogon proc]# docker run -it --name busybox_none --net none busybox/ # ip addr1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 二、bridge模式及其工作原理1.默认和自定义bridge网络bridge是docker默认网络模式，docker安装后会选择一个私有网段作为bridge的子网，从下面config中可以看到我们的16位掩码的子网是172.17.0.0/16，这样在我们创建容器时默认会将容器网络加入到这个子网中。 1234567891011121314151617181920212223242526272829303132333435363738[root@bogon proc]# docker inspect bridge[ &#123; &quot;Name&quot;: &quot;bridge&quot;, &quot;Id&quot;: &quot;f33b32dd835100abc5c94855d3676b082ef6f0ebea10847425fbe6187d263823&quot;, &quot;Created&quot;: &quot;2021-08-09T21:41:49.031849089-04:00&quot;, &quot;Scope&quot;: &quot;local&quot;, &quot;Driver&quot;: &quot;bridge&quot;, &quot;EnableIPv6&quot;: false, &quot;IPAM&quot;: &#123; &quot;Driver&quot;: &quot;default&quot;, &quot;Options&quot;: null, &quot;Config&quot;: [ &#123; &quot;Subnet&quot;: &quot;172.17.0.0/16&quot;, &quot;Gateway&quot;: &quot;172.17.0.1&quot; &#125; ] &#125;, &quot;Internal&quot;: false, &quot;Attachable&quot;: false, &quot;Ingress&quot;: false, &quot;ConfigFrom&quot;: &#123; &quot;Network&quot;: &quot;&quot; &#125;, &quot;ConfigOnly&quot;: false, &quot;Containers&quot;: &#123;&#125;, &quot;Options&quot;: &#123; &quot;com.docker.network.bridge.default_bridge&quot;: &quot;true&quot;, &quot;com.docker.network.bridge.enable_icc&quot;: &quot;true&quot;, &quot;com.docker.network.bridge.enable_ip_masquerade&quot;: &quot;true&quot;, &quot;com.docker.network.bridge.host_binding_ipv4&quot;: &quot;0.0.0.0&quot;, &quot;com.docker.network.bridge.name&quot;: &quot;docker0&quot;, &quot;com.docker.network.driver.mtu&quot;: &quot;1500&quot; &#125;, &quot;Labels&quot;: &#123;&#125; &#125;] 上面Containers字段中并为包含容器，现在我们创建一个busybox工具箱容器,通过inspect分别查看bridge和容器信息，显然172.17.0.1在子网172.17.0.0/16下。 12345678910111213141516171819202122232425262728293031323334[root@bogon proc]# docker run -d -t --name busybox busybox[root@bogon proc]# docker inspect bridge... &quot;Containers&quot;: &#123; &quot;b6f1e0103c1b44f76cd31fb75a3d9537f3292e5390e4441db2376e7e13d31ed2&quot;: &#123; &quot;Name&quot;: &quot;busybox&quot;, &quot;EndpointID&quot;: &quot;caac0bd4feedd0ac483fa44155c46e6c07d72e1d6281ca43599d5e72018f8066&quot;, &quot;MacAddress&quot;: &quot;02:42:ac:11:00:02&quot;, &quot;IPv4Address&quot;: &quot;172.17.0.2/16&quot;, &quot;IPv6Address&quot;: &quot;&quot; &#125;...[root@bogon proc]# docker inspect b6f... &quot;Networks&quot;: &#123; &quot;bridge&quot;: &#123; &quot;IPAMConfig&quot;: null, &quot;Links&quot;: null, &quot;Aliases&quot;: null, &quot;NetworkID&quot;: &quot;f33b32dd835100abc5c94855d3676b082ef6f0ebea10847425fbe6187d263823&quot;, &quot;EndpointID&quot;: &quot;caac0bd4feedd0ac483fa44155c46e6c07d72e1d6281ca43599d5e72018f8066&quot;, &quot;Gateway&quot;: &quot;172.17.0.1&quot;, &quot;IPAddress&quot;: &quot;172.17.0.2&quot;, &quot;IPPrefixLen&quot;: 16, &quot;IPv6Gateway&quot;: &quot;&quot;, &quot;GlobalIPv6Address&quot;: &quot;&quot;, &quot;GlobalIPv6PrefixLen&quot;: 0, &quot;MacAddress&quot;: &quot;02:42:ac:11:00:02&quot;, &quot;DriverOpts&quot;: null &#125; &#125;... 从上面我们可以看到使用默认bridge模式非常简单，当然在生产中我们一般会手动创建一个bridge，然后将容器加入到这个网络，以防不必要的容器被加入到生产子网中。 123456789[root@bogon proc]# docker network create --driver bridge --subnet 172.0.0.0/24 --gateway 172.0.0.1 newbridge1b1bc5ffe581472c098418ddaafc38fbdc101ef55d840d4be55b882242bca552[root@bogon proc]# docker network ls NETWORK ID NAME DRIVER SCOPEf33b32dd8351 bridge bridge local2c102587be2f host host local1b1bc5ffe581 newbridge bridge local04a61e996030 none null local –dirver、–subnet、–gateway分别指定我们的驱动、子网和网关 我们已经创建好了一个新的bridge网络，这样我们在创建容器的时候使用–net参数就可以加入到这个网络。 1[root@bogon proc]# docker run -d -t --net newbridge --name busybox_newbridge busybox 以上我们大概了解了创建一个bridge网络和使用，但是我们不了解其工作原理，这不太棒。 2.bridge网络工作原理我们从下面这张图展开bridge网络工作原理 我们安装Docker后守护进程将创建一个linux虚拟以太网桥docker0，它会在连接到其上的所有接口之间转发数据包，默认情况下主机上的所有容器都连接到这个内部桥接器，它会将一个接口（虚拟设备对veth pair）作为容器的eth0接口和主机命名空间中的另一个接口。这样容器就获得了私有 IP 地址分配。同时为了防止本地网络上的 ARP 冲突，Docker 守护进程从分配的 IP 地址上生成一个随机 MAC 地址。这样一来容器就和网桥连接起来了，然后通过iptables NAT规则和主机上eth0网卡交换数据。 veth pair我们可以理解为一条虚拟网络电缆，其两端都有一个虚拟网络接口设备。 接下来我们从实际中来看一看。使用brctl show来查看linux网桥和接口。 12345[root@bogon proc]# brctl showbridge name bridge id STP enabled interfacesbr-1b1bc5ffe581 8000.024297a8cc80 no vethe19101ddocker0 8000.02426da67767 no veth53e494e 可以看到我们默认的docker0和手动创建的newbridge对应的br-1b这两个网桥上分别接了vethe19101d和veth53e494e，为了方便下面咋们只关注docker0网桥。 同样我们也可以通过ip addr来查看接口信息，veth53e494e所属的网桥正是docker0 我们进入busybox容器看看，同样的存在接口eth0@if19 12345678910/ # ip addr1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever18: eth0@if19: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0 valid_lft forever preferred_lft forever 如此一来我们的虚拟设备对veth pair就成了，网桥与容器的通信水到渠成。最后容器怎么通过网桥与外界通信呢？ 我们来查看iptables nat规则： 1234567[root@bogon proc]# iptables -t nat -vnL...Chain POSTROUTING (policy ACCEPT 184 packets, 14201 bytes) pkts bytes target prot opt in out source destination 0 0 MASQUERADE all -- * !docker0 172.17.0.0/16 0.0.0.0/0 0 0 MASQUERADE all -- * !br-1b1bc5ffe581 172.0.0.0/24 0.0.0.0/0... POSTROUTING链会将所有来自172.17.0.0/16、172.0.0.0/24的流量伪装为宿主机网卡发出。即容器的流量通过NAT后服务端并没有感知，只知道是源自宿主机网卡的流量，相当于SNAT。 我们在看看DNAT，这里我们启动一个端口映射容器 1[root@bogon proc]# docker run -d -p 8888:80 httpd 此时通过tcp/8888端口的流量已经被转发到了172.17.0.3:80，这也是为什么在使用docker是需要开启net.ipv4.ip_forward转发。 12345Chain DOCKER (2 references) pkts bytes target prot opt in out source destination 0 0 RETURN all -- docker0 * 0.0.0.0/0 0.0.0.0/0 0 0 RETURN all -- br-1b1bc5ffe581 * 0.0.0.0/0 0.0.0.0/0 0 0 DNAT tcp -- !docker0 * 0.0.0.0/0 0.0.0.0/0 tcp dpt:8888 to:172.17.0.3:80 bridge网络模式大致到这里。容器的实现有太多linux细节，博大精深，关于网络部分水更深，本文只是冰山一角，有兴趣的同志可以通过搜索引擎遨游知识海洋。欢迎关注查看后面关于容器网络驱动Overlay、macvlan的小作文。 博客iqsing.github.io 您可以随意转载、修改、发布本文章，无需经过本人同意。 NEXT docker容器网络Overlay docker容器网络macvlan","categories":[{"name":"docker","slug":"docker","permalink":"https://iqsing.github.io/categories/docker/"}],"tags":[{"name":"bridge","slug":"bridge","permalink":"https://iqsing.github.io/tags/bridge/"}]},{"title":"docker容器技术基础之联合文件系统OverlayFS","slug":"docker容器技术基础之联合文件系统OverlayFS","date":"2021-07-22T16:47:21.000Z","updated":"2022-02-04T05:16:11.797Z","comments":true,"path":"2021/07/23/docker容器技术基础之联合文件系统OverlayFS/","link":"","permalink":"https://iqsing.github.io/2021/07/23/docker%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF%E5%9F%BA%E7%A1%80%E4%B9%8B%E8%81%94%E5%90%88%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9FOverlayFS/","excerpt":"","text":"我们在上篇介绍了容器技术中资源隔离与限制docker容器技术基础之linux cgroup、namespace 这篇小作文我们要尝试学习容器的另外一个重要技术之联合文件系统之OverlayFS，在介绍OverlayFS之前我们会学习一下镜像、容器、层的相关知识，然后是OverlayFS及相关实例，最后介绍docker中overlay2驱动即overlayfs在容器中的实现。 一、镜像、容器和层docker中镜像是层级结构的，即图中的image layers，每一层只是与它之前的层的一组差异。这些层堆叠在彼此的顶部。当我们创建一个新容器时，会在镜像层加一个新的可写层。这一层通常被称为“容器层”。对正在运行的容器所做的所有更改，例如写入新文件、修改现有文件和删除文件，都将写入这个薄的可写容器层。 那么一个镜像创建多个容器时是怎样的景象呢？如下图所示，添加新数据或修改现有数据的所有写入容器都存储在此可写层中。当容器被删除时，可写层也被删除。底层镜像保持不变。因为每个容器都有自己的可写容器层，所有的变化都存储在这个容器层中，所以多个容器可以共享对同一个底层镜像的访问，同时又拥有自己的数据状态。 到这里你可能要问了：镜像为什么要分层啊？乱七八糟的！ 其实不然，通过镜像的层级结构主要的一个优点是你可以把你的基础镜像进行共享，什么意思呢？比如你现在需要一个Nginx镜像、一个Tomcat镜像它们都可以通过一个base镜像如centos或者ubuntu制作而成，它看起来是这样的 如此一来通过镜像分层可以大大减少磁盘空间占用，同时降低镜像复构建杂度，何乐而不为。 二、联合文件系统OverlayFS通过上面我们大概了解了镜像、容器和层的关系，那么又有一个问题了：镜像层和可写容器层的文件或者内容是如何来管理的？明明是分层的又是怎么合并的？ 接下来我们将介绍UnionFS（联合文件系统），它的厉害之处在于可以将多个目录挂载到一个根目录。OverlayFS 是linux现代联合文件系统的一个代表，合并于Linux内核的3.18版本。从 docker 18.06后docker为OverlayFS提供了两个存储驱动，原始的overlay及overlay2（改善 inode 利用率），overlay2是目前docker推荐和首选存储驱动，通过它来管理镜像层和可写容器层内容。 我们可以在docker info中查看docker存储驱动版本 123456[root@i-k9pwet2d ~]# docker info... Server Version: 20.10.6 Storage Driver: overlay2 Backing Filesystem: extfs... OverlayFS这种堆叠的文件系统，依赖于其他文件系统之上，比如我们在info 中看到的extfs或者xfs等，它的结构如下图： 我们的基础层称为“lowerdir”即原始文件所在的位置。 客户端所做的任何修改都将反映在“upperdir”层上： 如果更改文件，新版本将写入其中（file1）。 如果删除文件，将在该层上创建一个删除标记（file2）。 创建一个新文件（file4）。 最后，“merged”是所有层合并后的最终视图。 假如你有一些数据，需要多个进程来访问和修改它。每个进程都要创建一个独立的数据视图，你要存储多份原始数据，数据量大的话显然这会非常低效的。使用OverlayFS将会是very good！ 接下来我们来我们搞个实验看看 我建立如下目录结构,workdir在OverlayFS中需要为空，用作内部临时存储。lowerdir包含3个文件file1、file2、file3 1234567891011121314151617[root@i-k9pwet2d overlayfs_test]# tree ..├── client_1│ ├── upperdir│ └── workdir├── client_2│ ├── upperdir│ └── workdir├── lowerdir│ ├── file1.txt│ └── file2.txt│ └── file3.txt└── merged ├── client_1 └── client_210 directories, 3 files 挂载overlay 1234567891011mount -t overlay overlay \\-o lowerdir=/overlaytest/lowerdir \\-o upperdir=/overlaytest/client_1/upperdir \\-o workdir=/overlaytest/client_1/workdir \\/overlaytest/merged/client_1mount -t overlay overlay \\-o lowerdir=/overlaytest/lowerdir \\-o upperdir=/overlaytest/client_2/upperdir \\-o workdir=/overlaytest/client_2/workdir \\/overlaytest/merged/client_2 挂载后查看我们的视图，可以看到三个文件已经被合并到merged区了 1234567891011121314151617181920212223[root@i-k9pwet2d overlaytest]# tree ..├── client_1│ ├── upperdir│ └── workdir│ └── work├── client_2│ ├── upperdir│ └── workdir│ └── work├── lowerdir│ ├── file1.txt│ ├── file2.txt│ └── file3.txt└── merged ├── client_1 │ ├── file1.txt │ ├── file2.txt │ └── file3.txt └── client_2 ├── file1.txt ├── file2.txt └── file3.txt 下一步我们修改merged/client_1下修改我们的都数据 12345[root@i-k9pwet2d client_1]# echo &quot;data no.1&quot;&gt;&gt;file1.txt [root@i-k9pwet2d client_1]# rm file2.txt [root@i-k9pwet2d client_1]# echo &quot;data4&quot; &gt; file4.txt[root@i-k9pwet2d client_1]# lsfile1.txt file3.txt file4.txt 再看我们的视图，可以看到修改只作用于client_1/upperdir，对我们lowerdir下原始数据以及client_2数据并不影响。 12345678910111213141516171819202122232425262728[root@i-k9pwet2d overlaytest]# tree ..├── client_1│ ├── upperdir│ │ ├── file1.txt│ │ ├── file2.txt│ │ └── file4.txt│ └── workdir│ └── work├── client_2│ ├── upperdir│ └── workdir│ └── work├── lowerdir│ ├── file1.txt│ ├── file2.txt│ └── file3.txt└── merged ├── client_1 │ ├── file1.txt │ ├── file3.txt │ └── file4.txt └── client_2 ├── file1.txt ├── file2.txt └── file3.txt12 directories, 12 files OverlayFS在容器中的实现下图显示了 在Docker 中镜像和 容器是如何通过OverlayFS分层与互相构造的映射。图像层是lowerdir，容器层是upperdir。统一视图合并到merged目录，该目录实际上是容器安装点。 我们查看一个真正运行容器centos的inspect 1docker inspect ca9a9e0a35c7 可以看到OverlayFS对应的目录地址，lowerDir即我们的原始数据包含image的rootfs（根文件）以及init相关文件，关于docker init层可以自行检索一下哈，这里不做介绍了。 12345678910&quot;GraphDriver&quot;: &#123; &quot;Data&quot;: &#123; &quot;LowerDir&quot;: &quot;/var/lib/docker/overlay2/444808f5d6566eebf4ea73ea593b2c2076d4347ff57bd98cbc179dbac9265968-init/diff:/var/lib/docker/overlay2/0d6b94986ba1af1cc75e7c237f78d7e02d40f5ae5ec3f67ddb699ae6d07a2ca8/diff&quot;, &quot;MergedDir&quot;: &quot;/var/lib/docker/overlay2/444808f5d6566eebf4ea73ea593b2c2076d4347ff57bd98cbc179dbac9265968/merged&quot;, &quot;UpperDir&quot;: &quot;/var/lib/docker/overlay2/444808f5d6566eebf4ea73ea593b2c2076d4347ff57bd98cbc179dbac9265968/diff&quot;, &quot;WorkDir&quot;: &quot;/var/lib/docker/overlay2/444808f5d6566eebf4ea73ea593b2c2076d4347ff57bd98cbc179dbac9265968/work&quot; &#125;, &quot;Name&quot;: &quot;overlay2&quot; &#125;, 到LowerDir下查看原始rootfs 123[root@i-k9pwet2d client_1]# ls /var/lib/docker/overlay2/0d6b94986ba1af1cc75e7c237f78d7e02d40f5ae5ec3f67ddb699ae6d07a2ca8/diffbin dev etc home lib lib64 lost+found media mnt opt proc root run sbin srv sys tmp usr var 我们进入容器添加和删除文件看看 12345[root@i-k9pwet2d ~]# docker exec -it ca9a /bin/bash[root@ca9a9e0a35c7 /]# echo &quot;newfile&quot; &gt;file [root@ca9a9e0a35c7 /]# rm /tmp/ks-script-esd4my7v 显然到到LowerDir下查看原始rootfs并不受影响而是把变更写入到了upperdir即容器层 12345678910cd /var/lib/docker/overlay2/444808f5d6566eebf4ea73ea593b2c2076d4347ff57bd98cbc179dbac9265968/diff[root@i-k9pwet2d diff]# tree ..├── file└── tmp └── ks-script-esd4my7v1 directory, 2 files 以上的操作也就是docker中所谓的CoW（写时复制）策略。在docker中overlay2驱动对联合文件系统操作的更多场景可以参阅官方文档 这样我们实现容器的三大基础技术Namespace、Cgroup、UnionFS联合文件系统已经介绍完啦，希望这三篇小作文对想了解容器实现原理的读者有些许帮助。 参考： Use the OverlayFS storage driver Understanding Container Images, Part 3: Working with Overlays 小作文有不足的地方欢迎指出。 感谢收藏、点赞。关注顶级饮水机管理员，除了烧热水，有时还做点别的。 您的支持是我烧热水最大的动力…","categories":[{"name":"docker","slug":"docker","permalink":"https://iqsing.github.io/categories/docker/"},{"name":"linux","slug":"docker/linux","permalink":"https://iqsing.github.io/categories/docker/linux/"}],"tags":[{"name":"OverlayFS","slug":"OverlayFS","permalink":"https://iqsing.github.io/tags/OverlayFS/"}]},{"title":"docker容器技术基础之linux cgroup、namespace","slug":"docker容器技术基础之linux cgroup、namespace","date":"2021-07-19T16:47:21.000Z","updated":"2022-02-04T05:16:11.785Z","comments":true,"path":"2021/07/20/docker容器技术基础之linux cgroup、namespace/","link":"","permalink":"https://iqsing.github.io/2021/07/20/docker%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF%E5%9F%BA%E7%A1%80%E4%B9%8Blinux%20cgroup%E3%80%81namespace/","excerpt":"","text":"一、开头接触过docker的同学多多少少听过这样一句话“docker容器通过linux namespace、cgroup特性实现资源的隔离与限制”。今天我们来尝试学习一下这两个东西。 二、关于namesapce命名空间将全局系统资源包装在一个抽象中,使命名空间内的进程看起来它们拥有自己独立的全局资源实例。命名空间内对全局资源的改变对其他进程可见，命名空间的成员对其他进程不可见。 目前linux 内核已实现的7种命名空间如下：123456789Namespace Flag（API操作类型别名） Isolates（隔离内容）Cgroup CLONE_NEWCGROUP Cgroup root directory (since Linux 4.6)IPC CLONE_NEWIPC System V IPC, POSIX message queues (since Linux 2.6.19)Network CLONE_NEWNET Network devices, stacks, ports, etc. (since Linux 2.6.24)Mount CLONE_NEWNS Mount points (since Linux 2.4.19)PID CLONE_NEWPID Process IDs (since Linux 2.6.24)User CLONE_NEWUSER User and group IDs (started in Linux 2.6.23 and completed in Linux 3.8)UTS CLONE_NEWUTS Hostname and NIS domain name (since Linux 2.6.19) 查看进程的namespace123456789101112[root@i-k9pwet2d ~]# pidof bash14208 11123 2053[root@i-k9pwet2d ~]# ls -l /proc/14208/nstotal 0lrwxrwxrwx 1 root root 0 Jul 20 09:36 ipc -&gt; ipc:[4026531839]lrwxrwxrwx 1 root root 0 Jul 20 09:36 mnt -&gt; mnt:[4026531840]lrwxrwxrwx 1 root root 0 Jul 20 09:36 net -&gt; net:[4026531956]lrwxrwxrwx 1 root root 0 Jul 20 09:36 pid -&gt; pid:[4026531836]lrwxrwxrwx 1 root root 0 Jul 20 09:36 user -&gt; user:[4026531837]lrwxrwxrwx 1 root root 0 Jul 20 09:36 uts -&gt; uts:[4026531838] 每一个进程在/proc/[pid]/ns 都可以看到其所属的namespace信息，这些链接文件指向所属的namespace及inode ID,我们可以通过readlink 来查看两个进程的是否属于同一个命名空间，inode相同则他们所属相同命名空间 1234[root@i-k9pwet2d ~]# readlink /proc/11123/ns/utsuts:[4026531838][root@i-k9pwet2d ~]# readlink /proc/14208/ns/utsuts:[4026531838] 如何将你的进程注册到命名空间（API操作）？**clone()*：创建一个新的命名空间，子进程同属新的命名空间,flags即我们创建的namespace类型，形如CLONE_NEW 12int clone(int (*fn)(void *), void *stack, int flags, void *arg, ... /* pid_t *parent_tid, void *tls, pid_t *child_tid */ ); setns(): 加入一个命名空间，fd为/proc/[pid]/ns 下的链接文件，nstype即我们的Flag 1int setns(int fd, int nstype); unshare() ：退出某个namespace并加入创建的新空间。 1int unshare(int flags); ioctl() : ioctl系统调用可用于查询命名空间的信息 1int ioctl(int fd , unsigned long request , ...); 下面我们通过shell 命令 unshare 来看看命名空间7大隔离实现 1.PID Namespace PID Namespace 的作用是用来隔离进程，利用 PID Namespace 可以实现每个容器的主进程为 1 号进程，而容器内的进程在主机上却拥有不同的PID。 123456789101112131415[root@i-k9pwet2d ~]# unshare --fork --pid --mount-proc /bin/bash[root@i-k9pwet2d ~]# ps -auxUSER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDroot 1 0.0 0.1 115680 2036 pts/0 S 10:46 0:00 /bin/bashroot 12 0.0 0.1 115684 2048 pts/0 S 10:47 0:00 -bashroot 30 0.0 0.0 155468 1804 pts/0 R+ 10:57 0:00 ps -auxls -l /proc/1/nstotal 0lrwxrwxrwx 1 root root 0 Jul 20 11:05 ipc -&gt; ipc:[4026531839]lrwxrwxrwx 1 root root 0 Jul 20 11:05 mnt -&gt; mnt:[4026532545]lrwxrwxrwx 1 root root 0 Jul 20 11:05 net -&gt; net:[4026531956]lrwxrwxrwx 1 root root 0 Jul 20 11:05 pid -&gt; pid:[4026532546]lrwxrwxrwx 1 root root 0 Jul 20 11:05 user -&gt; user:[4026531837]lrwxrwxrwx 1 root root 0 Jul 20 11:05 uts -&gt; uts:[4026531838] 在新的PID Namespace中我们只能看到自身命名空间的进程。并且当前的bash处于起来的mnt、pid命名空间。 2.Mount Namespace 它可以用来隔离不同的进程或进程组看到的挂载点。在容器内的挂载操作不会影响主机的挂载目录。 我们创建一个命名空间 1unshare --mount --fork /bin/bash 挂载一个目录 12345[root@i-k9pwet2d ~]# mkdir /tmp/mnt[root@i-k9pwet2d ~]# mount -t tmpfs -o size=1m tmpfs /tmp/mnt[root@i-k9pwet2d ~]# df -h |grep mnttmpfs 1M 0 1M 0% /tmp/mnt 在命名空间内的挂载并不影响我们的主机目录，我们在主机上查看不到挂载信息 1df -h |grep mnt 3.User Namespace User Namespace用来隔离用户和用户组。我们来创建一个用户命名空间并修改提示符 1234[root@i-k9pwet2d ~]# PS1=&#x27;\\u@container#&#x27; unshare --user -r /bin/bashroot@container# 再查看ns，用户链接是不同的，已处于不同空间。 1234[root@i-k9pwet2d ~]# readlink /proc/1835/ns/useruser:[4026532192][root@i-k9pwet2d ~]# readlink /proc/$$/ns/useruser:[4026531837] 用户命名空间的最大优势是无需 root 权限即可运行容器，避免应用使用root对主机的影响。 4.UTS Namespace UTS Namespace 用于隔离主机名的，它允许每个 UTS Namespace 拥有一个独立的主机名。 1[root@i-k9pwet2d ~]# unshare --fork --uts /bin/bash 在命名空间中修改主机名,在主机中不受影响 123[root@i-k9pwet2d ~]# hostname -b container[root@i-k9pwet2d ~]# hostnamecontainer 主机中 12[root@i-k9pwet2d ~]# hostnamei-k9pwet2d 5.IPC Namespace IPC 命名空间隔离某些 IPC 资源，即 System V IPC 对象（参见sysvipc(7)）和（自 Linux 2.6.30 起）POSIX 消息队列（请参阅mq_overview(7)）。容器通过IPC Namespace、PID Namespace实现同一 IPC Namespace 内的进程彼此可以通信，不同 IPC Namespace 的进程却不能通信。 我们使用linux中ipc相关命令来测试 ipcs -q 命令：用来查看系统间通信队列列表。 ipcmk -Q 命令：用来创建系统间通信队列。 我们先创建一个IPC Namespace 1[root@i-k9pwet2d ~]# unshare --fork --ipc /bin/bash 创建一个通信队列后查询一下 1234567[root@i-k9pwet2d ~]# ipcmk -QMessage queue id: 0[root@i-k9pwet2d ~]# ipcs -q------ Message Queues --------key msqid owner perms used-bytes messages 0x1de4aef6 0 root 644 0 0 在主机上查询，可以看到通信已经被隔离了 1234[root@i-k9pwet2d ~]# ipcs -q------ Message Queues --------key msqid owner perms used-bytes messages 6.Net Namespace Net Namespace 可用于隔离网络设备、IP 地址和端口等信息。Net Namespace 可以让每个进程拥有自己独立的 IP 地址，端口和网卡信息。 我们继续创建一个Net Namespace 1[root@i-k9pwet2d ~]# unshare --net --fork /bin/bash 查看网络和端口信息 1234567[root@i-k9pwet2d ~]# ip addr1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 [root@i-k9pwet2d ~]# netstat -ntlpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name 上面看到了一个回环接口lo，状态处于DOWN，我们将它启动，这样我们的Namespace有了自己的网络地址。 1234561: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 主机中 1234567891011121314151617181920[root@i-k9pwet2d ~]# ip addr1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 52:54:96:e1:36:04 brd ff:ff:ff:ff:ff:ff inet 10.150.25.9/24 brd 10.150.25.255 scope global noprefixroute dynamic eth0 valid_lft 80720sec preferred_lft 80720sec inet6 fe80::5054:96ff:fee1:3604/64 scope link valid_lft forever preferred_lft forever [root@i-k9pwet2d ~]# netstat -ntlpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 757/sshd tcp 0 0 127.0.0.1:25 0.0.0.0:* LISTEN 1112/master ... 7.Cgroup Namespace Cgroup是对进程的cgroup视图虚拟化。 每个 cgroup 命名空间都有自己的一组 cgroup 根目录。Linux 4.6开始支持。 cgroup 命名空间提供的虚拟化有多种用途： 防止信息泄漏。否则容器外的cgroup 目录路径对容器中的进程可见。 简化了容器迁移等任务。 允许更好地限制容器化进程。可以挂载容器的 cgroup 文件系统，这样容器无需访问主机 cgroup 目录。 8.Time Namespace 虚拟化两个系统时钟，用于隔离时间。 linux 5.7内核开始支持 参考地址：TIME_NAMESPACES(7) 三、关于Cgroup从上面我们了解到当我们要运行一个容器时，docker等应用会为该容器创建一组 namespace，对操作系统而言可以理解为一组进程。这下我们完成了“权利”的集中，但是“权利越大，责任也大”，我们不能放任这组“大权“不管，所以又有了Cgroup（Linux Control Group）这个东西。 Cgroup最主要的作用，就是限制一个进程组能够使用的资源上限，包括 CPU、内存、磁盘、网络带宽等等。 cgroups 框架提供了以下内容： 资源限制： 可以为我们的进程组配置内存限制或cpu个数限制又或者仅限于某个特定外围设备。 优先级： 一个或多个组可以配置为优先占用 CPU 或磁盘 I/O 吞吐量。 资源记录： 监视和测量组的资源使用情况。 控制： 可以冻结或停止和重新启动进程组。 一个 cgroup 可以由一个或多个进程组成，这些进程都绑定到同一组限制。这些组也可以是分层的，即子组可以继承父组管理的限制。 Linux 内核为 cgroup 技术提供了对一系列控制器或子系统的访问。控制器负责将特定类型的系统资源分配给一组一个或多个进程。例如，memory控制器限制内存使用，而cpuacct控制器监控 CPU 使用。 我们通过Mount查看系统中cgroup的子系统 123456789101112[root@i-k9pwet2d ~]# mount -t cgroup cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd)cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpuacct,cpu)cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_prio,net_cls)cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids)cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset)cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer)cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event) 可以看到cgroup已通过文件系统方式挂载到/sys/fs/cgroup/ 123456789101112131415161718[root@i-k9pwet2d ~]# ls -l /sys/fs/cgroup/total 0drwxr-xr-x 2 root root 0 Jul 20 12:23 blkiolrwxrwxrwx 1 root root 11 Jul 20 12:23 cpu -&gt; cpu,cpuacctlrwxrwxrwx 1 root root 11 Jul 20 12:23 cpuacct -&gt; cpu,cpuacctdrwxr-xr-x 2 root root 0 Jul 20 12:23 cpu,cpuacctdrwxr-xr-x 2 root root 0 Jul 20 12:23 cpusetdrwxr-xr-x 4 root root 0 Jul 20 12:23 devicesdrwxr-xr-x 2 root root 0 Jul 20 12:23 freezerdrwxr-xr-x 2 root root 0 Jul 20 12:23 hugetlbdrwxr-xr-x 2 root root 0 Jul 20 12:23 memorylrwxrwxrwx 1 root root 16 Jul 20 12:23 net_cls -&gt; net_cls,net_priodrwxr-xr-x 2 root root 0 Jul 20 12:23 net_cls,net_priolrwxrwxrwx 1 root root 16 Jul 20 12:23 net_prio -&gt; net_cls,net_priodrwxr-xr-x 2 root root 0 Jul 20 12:23 perf_eventdrwxr-xr-x 2 root root 0 Jul 20 12:23 pidsdrwxr-xr-x 4 root root 0 Jul 20 12:23 systemd 接下来我们通过一个实例看看cgroup是如何限制CPU使用的我们启动一个循环脚本，这个循环脚本将占用近100%的CPU，我们通过cgroup限制到50% 123456$ cat loop.sh#!/bash/shwhile [ 1 ]; do:done 将我们的脚本放到后台，获取它的PID为21497 1nohup bash loop.sh &amp; 我们需要创建一个cgroup控制组loop 1[root@i-k9pwet2d ~]# mkdir /sys/fs/cgroup/cpu/loop loop组是CPU的子组，上面提到子组可以继承父组管理的限制所以loop将继承对系统整个cpu的访问权限 1234567891011121314151617[root@i-k9pwet2d shell]# ls -l /sys/fs/cgroup/cpu/looptotal 0-rw-r--r-- 1 root root 0 Jul 20 17:15 cgroup.clone_children--w--w--w- 1 root root 0 Jul 20 17:15 cgroup.event_control-rw-r--r-- 1 root root 0 Jul 20 17:15 cgroup.procs-r--r--r-- 1 root root 0 Jul 20 17:15 cpuacct.stat-rw-r--r-- 1 root root 0 Jul 20 17:15 cpuacct.usage-r--r--r-- 1 root root 0 Jul 20 17:15 cpuacct.usage_percpu-rw-r--r-- 1 root root 0 Jul 20 17:15 cpu.cfs_period_us-rw-r--r-- 1 root root 0 Jul 20 17:15 cpu.cfs_quota_us-rw-r--r-- 1 root root 0 Jul 20 17:15 cpu.rt_period_us-rw-r--r-- 1 root root 0 Jul 20 17:15 cpu.rt_runtime_us-rw-r--r-- 1 root root 0 Jul 20 17:15 cpu.shares-r--r--r-- 1 root root 0 Jul 20 17:15 cpu.stat-rw-r--r-- 1 root root 0 Jul 20 17:15 notify_on_release-rw-r--r-- 1 root root 0 Jul 20 17:15 tasks 查看继承后的loop组cpu限制，计算周期为100000us，采样时间无限制（-1） 1234[root@i-k9pwet2d shell]# cat /sys/fs/cgroup/cpu/loop/cpu.cfs_period_us100000[root@i-k9pwet2d shell]# cat /sys/fs/cgroup/cpu/loop/cpu.cfs_quota_us-1 为了限制进程的的cpu使用率为50%，我们需要更新cpu.cfs_quota_us的值为50000 1echo 50000 &gt;/sys/fs/cgroup/cpu/loop/cpu.cfs_quota_us 将脚本PID更新到loop控制组下的tasks 1[root@i-k9pwet2d shell]# echo 21497 &gt;/sys/fs/cgroup/cpu/loop/tasks 此时我们的脚本CPU使用率已被限制到50% 12 PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 21497 root 20 0 113284 1176 996 R 50.0 0.1 12:17.48 bash 在docker启动容器时做的cpu限制参数--cpu-period、--cpu-quota实际上就是调整对应容器控制组的cpu配额。 参考： 《深入剖析Kubernetes》张磊 Everything You Need to Know about Linux Containers, Part I: Linux Control Groups and Process Isolation namespaces(7) — Linux manual page 小作文有不足的地方欢迎指出。 欢迎收藏、点赞、提问。关注顶级饮水机管理员，除了管烧热水，有时还做点别的。","categories":[{"name":"docker","slug":"docker","permalink":"https://iqsing.github.io/categories/docker/"},{"name":"linux","slug":"docker/linux","permalink":"https://iqsing.github.io/categories/docker/linux/"}],"tags":[{"name":"Cgroup","slug":"Cgroup","permalink":"https://iqsing.github.io/tags/Cgroup/"},{"name":"Namespace","slug":"Namespace","permalink":"https://iqsing.github.io/tags/Namespace/"}]},{"title":"Github Copilot 结合python的使用","slug":"Github Copilot 结合python的使用","date":"2021-07-18T16:47:21.000Z","updated":"2022-02-04T05:16:11.704Z","comments":true,"path":"2021/07/19/Github Copilot 结合python的使用/","link":"","permalink":"https://iqsing.github.io/2021/07/19/Github%20Copilot%20%E7%BB%93%E5%90%88python%E7%9A%84%E4%BD%BF%E7%94%A8/","excerpt":"","text":"之前提交的github copilot技术预览版申请，今天收到准入邮件，于是安上试一试这个准备把我送去电子厂上班的copy a lot ？ 官网及申请地址：https://copilot.github.com/ 小作文包含如下内容： copilot简单介绍 使用python对copilot做些简单使用测试 使用copilot对LeetCode 题目解答 一、copilot简单介绍 github copilot（副驾驶）目前只适用于vscode的扩展插件，它依赖于github数十亿公开代码库的训练而成的AI编码辅助器（包括整行代码提供或函数建议），目前支持数十种编程语言，技术预览版对 Python、JavaScript、TypeScript、Ruby 和 Go 的表现尤其出色。 他的工作原理：通过大量公共代码库对AI模型训练后构建成copilot服务，服务接收来自copilot插件返回的提要编码，并提供代码建议，插件又将来自程序员对建议的采纳性的回传到copilot服务，如此反复强化AI模型。 下图以蔽之： 二、使用python对copilot做些简单使用测试在vscode插件中我们安上Github Copilot，建一个测试文件Copilot_test.py 1.获取列表的最大和最小值的函数我们要写的可能看起来是这样的 1234567891011121314151617&#x27;&#x27;&#x27;Function to get the max and min values of a list&#x27;&#x27;&#x27;def get_max_min(my_list): max_value = max(my_list) min_value = min(my_list) return max_value, min_valuedef main(): my_list = [1,2,3,4,5] max_value, min_value = get_max_min(my_list) print(&quot;Max value:&quot;, max_value) print(&quot;Min value:&quot;, min_value) if __name__ == &quot;__main__&quot;: main() 注释是copilot的关键部分，所有AI的是基于大数据的应用，甚至可以把copilot简单的认为是对github代码库的检索… 2.一个计算器 copilot给出的建议允许我们进行选择，通过Alt+[,Alt+]对建议上下查看。 我们可以使用Ctrl+Eeter打开建议结果面板，可以看到对应这些建议，copilot给了我们是10个解决方案 三、使用copilot对LeetCode 题目解答我们在LeetCode找一题【回文数】，题目如下： 然后我们把题目写到代码注释中 代码区的类也加过来 看起来我们的代码就是这样的，灰色code的copilot给出的建议 我们选择其中一种建议放到LeetCode的执行看看 这个建议似乎不太理想…勇敢牛牛不怕困难，有兴趣的同学可以看看困难模式的题目哦，PS：对于中文的注释不确定copilot能百分之百给出回应。 copilot就像它的名字一样【副驾驶】，正经事还是得你来干，不过未来可能会一个不错的协助工具。 文章有不足的地方欢迎在评论区指出。 欢迎收藏、点赞、提问。关注顶级饮水机管理员，除了管烧热水，有时还做点别的。","categories":[{"name":"python","slug":"python","permalink":"https://iqsing.github.io/categories/python/"}],"tags":[{"name":"Copilot","slug":"Copilot","permalink":"https://iqsing.github.io/tags/Copilot/"}]},{"title":"harbor搭建docker、k8s镜像仓库","slug":"harbor搭建docker、k8s镜像仓库","date":"2021-07-17T16:47:21.000Z","updated":"2022-02-04T05:16:11.909Z","comments":true,"path":"2021/07/18/harbor搭建docker、k8s镜像仓库/","link":"","permalink":"https://iqsing.github.io/2021/07/18/harbor%E6%90%AD%E5%BB%BAdocker%E3%80%81k8s%E9%95%9C%E5%83%8F%E4%BB%93%E5%BA%93/","excerpt":"","text":"一、harbor简介harbor是一个开源企业级容器镜像仓库，如果你的企业正在使用容器技术，使用harbor搭建容器镜像仓库是一个不错的选择。 harbor包含如下特性： 权限管理(RBAC)、LDAP、审计 安全漏洞扫描 镜像验真 提供管理界面 提供注册中心 高可用HA 同时针对中国用户的特点，设计镜像复制和中文支持等功能。 ”我们的使命是成为kubernetes生态中可信赖的云原生镜像仓库” 官方地址：https://goharbor.io/ 官方demo地址：https://demo.goharbor.io/ 二、harbor安装官方提供了在线和离线两种安装方式，本文以在线安装方式进行（谁叫咋有这个条件呢？） github地址:https://github.com/goharbor/harbor/ 1.安装前准备harbor v2.x包含如下组件 硬件要求如下： 软件版本要求如下： 防火墙开放端口如下： 更新依赖软件版本： 12345678910111213141516171819202122232425262728293031323334yum remove docker-1.13.1-205.git7d71120.el7.centos.x86_64 yum remove docker-common-2:1.13.1-204.git0be3e21.el7.x86_64#移除centos7自带旧版本docker1.13wget https://repo.huaweicloud.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo #获取华为云docker-ce镜像源yum install docker-ce docker-compose openssl#安装stable版本docker、openssl#检测版本[root@VM-0-5-centos yum.repos.d]# docker versionClient: Docker Engine - Community Version: 20.10.6 API version: 1.41 Go version: go1.13.15 Git commit: 370c289 Built: Fri Apr 9 22:45:33 2021 OS/Arch: linux/amd64 Context: default Experimental: true[root@VM-0-5-centos yum.repos.d]# docker-compose versiondocker-compose version 1.18.0, build 8dd22a9docker-py version: 2.6.1CPython version: 3.6.8OpenSSL version: OpenSSL 1.0.2k-fips 26 Jan 2017 2. 安装harbor获取v2.1.5安装脚本并解压 123456789 wget https://pd.zwc365.com/seturl/https://github.com/goharbor/harbor/releases/download/v2.1.5/harbor-online-installer-v2.1.5.tgz #解压得到harbor文件夹 [root@VM-0-5-centos harbor]# lscommon.sh harbor.yml harbor.yml.tmpl input install.sh LICENSE preparecp harbor.yml.tmpl /root/harbor/harbor.yml#将yml配置文件拷贝到/root/harbor/harbor.yml 编辑配置文件如下，本文关闭https认证。 vim /root/harbor/harbor.yml 配置docker 镜像源，vim /etc/docker/daemon.json 123456789&#123;&quot;registry-mirrors&quot;: [ &quot;https://uyqa6c1l.mirror.aliyuncs.com&quot;, &quot;https://hub-mirror.c.163.com&quot;, &quot;https://dockerhub.azk8s.cn&quot;, &quot;https://reg-mirror.qiniu.com&quot;, &quot;https://registry.docker-cn.com&quot; ]&#125; 启动docker，执行安装脚本。 等待拉去镜像结束 验证安装 http://ip ，如下安装已完成。 下篇文章将简单介绍docker如何从harbor获取镜像。 文章有不足的地方欢迎在评论区指出。 欢迎收藏、点赞、提问。关注顶级饮水机管理员，除了管烧热水，有时还做点别的。","categories":[{"name":"k8s","slug":"k8s","permalink":"https://iqsing.github.io/categories/k8s/"},{"name":"docker","slug":"k8s/docker","permalink":"https://iqsing.github.io/categories/k8s/docker/"}],"tags":[{"name":"harbor","slug":"harbor","permalink":"https://iqsing.github.io/tags/harbor/"}]},{"title":"python django与celery的集成","slug":"python django与celery的集成","date":"2021-07-14T16:47:21.000Z","updated":"2022-02-04T05:16:12.022Z","comments":true,"path":"2021/07/15/python django与celery的集成/","link":"","permalink":"https://iqsing.github.io/2021/07/15/python%20django%E4%B8%8Ecelery%E7%9A%84%E9%9B%86%E6%88%90/","excerpt":"","text":"一、celery与django关于celery介绍和使用可以查看上篇Python中任务队列-芹菜celery的使用 关于django的介绍和使用可查看python django框架+vue.js前后端分离 我来看一下celery集成到django后的整个工作链：django将任务转发给消息队列，celery读取到任务后执行并将结果通过django ORM 存储。 在本文中两个插件： 使用django-celery-results将celery处理结果进行ORM存储 使用django-celery-beat 对任务管理和周期调度 二、基本使用1.创建一个django项目,不再赘述建好后目录看起来是这样的 12345678910111213141516django_celery├── celery_app│ ├── __init__.py│ ├── apps.py│ ├── migrations/│ ├── models.py│ ├── admin.py│ └── views.py├── manage.py├── django_celery│ ├── __init__.py│ ├── settings.py│ ├── urls.py│ └── wsgi.py└── env/│── db.sqlite3 安装插件 12pip install django-celery-resultspip install django-celery-beat 在项目settings.py我们会安装appcelery_app、django_celery_results、django_celery_beat并开启我们的admin后台用于查看和管理我们的任务调度。 2.添加任务tasks.py我们在celery_app中添加任务文件tasks.py其中包含上篇文章中单args_add1任务，并通过shared_task进行装饰。 12345678910from __future__ import absolute_importfrom celery import shared_taskimport time@shared_taskdef args_add1(x,y): print(&quot;start task no.1 now!&quot;) time.sleep(10) print(&quot;task no.1 end!&quot;) return x+y 3.配置celery应用在django_celery目录下添加celery.py用于创建我们的celery应用 123456789import osfrom celery import Celery#加载配置os.environ.setdefault(&#x27;DJANGO_SETTINGS_MODULE&#x27;, &#x27;django_celery.settings&#x27;)#创建celery appapp = Celery(&#x27;django_celery&#x27;)app.config_from_object(&#x27;django.conf:settings&#x27;, namespace=&#x27;CELERY&#x27;)#自动发现项目中的tasksapp.autodiscover_tasks() 在setting中添加celery相关配置，指定Broker和Backend。这里我们延用上篇的rabbitmq作为Broker，后端配置为django-db 123CELERY_BROKER_URL = &#x27;amqp://rabbit_user:rabbit_pass@i-k9pwet2d/rabbit_vhost&#x27;CELERY_RESULT_BACKEND = &#x27;django-db&#x27; 我们的celery应用配置好了，然后我们要启动它，在__init__.py中加入我们的app 123from .celery import app as celery_app__all__ = [&#x27;celery_app&#x27;] 4.配置django通过上面的配置我们有了celery应用，也有了任务，接下来我们配置一个URL请求来发送我们的任务给celery。 对我们app下views.py编写如下 12345678910from django.http import JsonResponsefrom celery_app import tasksfrom celery.result import AsyncResult# Create your views here.def celery(request,*args,**kwargs): res=tasks.args_add1.delay(123,456) #发送任务给celery result = AsyncResult(res.task_id) return JsonResponse(&#123;&#x27;status&#x27;:result.status,&#x27;task_id&#x27;:result.task_id&#125;) 配置路由 12345678from django.contrib import adminfrom django.urls import pathimport celery_app.viewsurlpatterns = [ path(&#x27;admin/&#x27;, admin.site.urls), path(&#x27;celery/&#x27;,celery_app.views.celery)] 5.查看任务执行通过上面配置我们基本款已经成了。我们启动django和celery worker看看 12python manage.py migratepython manage.py runserver 启动celery worker 1celery -A django_celery worker --loglevel=info --concurrency=10 访问地址：http://127.0.0.1:8000/celery/ 发送我们的任务，此时任务已处于等待状态。 在worker终端上可以查看到任务正在被处理。 我们在django后台查看task执行记录和细节 三、周期性任务调度在前面我们已经安装了django-celery-beat 我们在后台Periodic tasks中配置我们的周期性任务，也可以通过Crontab Schedule来配置计划任务。 我们配置一个每5小时执行一次的任务。 启动调度器,这样我们的任务就会在后台默默无闻的周期性工作啦。 1celery -A django_celery beat -l info --scheduler django_celery_beat.schedulers:DatabaseScheduler 以上是celery集成到django中的基础使用。 文章有不足的地方欢迎指出。 欢迎收藏、点赞、提问。关注顶级饮水机管理员，除了管烧热水，有时还做点别的。","categories":[{"name":"python","slug":"python","permalink":"https://iqsing.github.io/categories/python/"}],"tags":[{"name":"celery","slug":"celery","permalink":"https://iqsing.github.io/tags/celery/"},{"name":"django","slug":"django","permalink":"https://iqsing.github.io/tags/django/"}]},{"title":"Python中任务队列celery的使用","slug":"Python中任务队列celery的使用","date":"2021-07-06T16:47:21.000Z","updated":"2022-02-04T05:16:11.721Z","comments":true,"path":"2021/07/07/Python中任务队列celery的使用/","link":"","permalink":"https://iqsing.github.io/2021/07/07/Python%E4%B8%AD%E4%BB%BB%E5%8A%A1%E9%98%9F%E5%88%97celery%E7%9A%84%E4%BD%BF%E7%94%A8/","excerpt":"","text":"一、关于celery芹菜celery是一个python实现的异步任务队列，可以用于爬虫、web后台查询、计算等等。通过任务队列，当一个任务来临时不再傻傻等待。 它的架构如下： Broker 我们的生产者创建任务后会进入celery的任务调度队列中间件Broker，Broker通过调度规则将消息（任务）调度消息队列，Broker依赖第三方队列消息代理如rabbitmq、redis等。 Worker 广大劳动者，盯着消息队列，当队列中有消息时把它拿过来给处理了。 Backend 用于结果存储经worker处理的结果，比如常用的数据库等。 使用celery在本文中咱们使用rabbitmq（celery推荐）作为消息代理中间件。 我们创建的celery目录如下 1234567learn_celery/...celery_env/...celery.py...my_task1.py...my_task2.py...task1_run.py...task2_run.py 1. 创建虚拟环境并安装celery、flower（web监控），这里不做赘述。2.安装咱们的消息队列中间件rabbitmq这里以docker的方式运行并配置，指定主机名为rabbit（rabbitmq是以主机名来访问的，所以这是必须的），容器名称为celery_rabbitmq 1docker run -d -p 5672:5672 -h rabbit --name celery_rabbitmq rabbitmq 添加用于celery访问的用户，以及配置configure、write和read权限，在下面我们配置rabbit_user拥有所有配置、写入和读取权限。 1234docker exec -it celery_rabbitmq rabbitmqctl add_user rabbit_user rabbit_passdocker exec -it celery_rabbitmq rabbitmqctl add_vhost rabbit_vhostdocker exec -it celery_rabbitmq rabbitmqctl set_user_tags rabbit_user celerydocker exec -it celery_rabbitmq rabbitmqctl set_permissions -p rabbit_vhost rabbit_user &quot;.*&quot; &quot;.*&quot; &quot;.*&quot; 3.创建celery应用12345#celery.pyfrom celery import Celerybroker_rabbitmq=&quot;amqp://rabbit_user:rabbit_pass@i-k9pwet2d/rabbit_vhost&quot;app=Celery(&quot;learn_celery&quot;,broker=broker_rabbitmq,backend=&quot;rpc://&quot;,include=[&quot;learn_celery.my_task2&quot;,&quot;learn_celery.my_task2&quot;]) 我们通过创建app来实例化Celery，项目包的名称为learn_celery，通过broker_rabbitmq来连接rabbitmq，rabbitmq的amqp协议格式为 1amqp://userid:password@hostname:port/virtual_host 由于我们是在docker中启动的rabbitmq，所以我们的hostname应该为宿主机的hostname。 指定后端通过rpc回传数据，include加载带worker处理的任务learn_celery.my_task1、learn_celery.my_task2 4.创建两个任务(消息)123456789101112131415161718192021#my_task1.pyfrom .celery import appimport time@app.taskdef args_add1(x,y): print(&quot;start task no.1 now!&quot;) time.sleep(10) print(&quot;task no.1 end!&quot;) return x+y#my_task12.pyfrom .celery import appimport time@app.taskdef args_add2(x,y): print(&quot;start task no.2 now!&quot;) time.sleep(20) print(&quot;task no.2 end!&quot;) return x+y 在这里我们导入了celery中的app，并用它来装饰我们的方法args_add，在args_add中模拟任务处理时间分别为10s、20s然后返回结果。 5.发送任务给celery12345678910111213141516171819202122#tasks1_run.pyfrom .my_task1 import args_add1import timereslut=args_add1.delay(11,22)print(&quot;task over?&#123;&#125;&quot;.format(reslut.ready()))print(&quot;task reslut:&#123;&#125;&quot;.format(reslut.result))time.sleep(15)print(&quot;task over?&#123;&#125;&quot;.format(reslut.ready()))print(&quot;task reslut:&#123;&#125;&quot;.format(reslut.result))#tasks2_run.pyfrom .my_task2 import args_add2import timereslut=args_add2.delay(33,44)print(&quot;task over?&#123;&#125;&quot;.format(reslut.ready()))print(&quot;task reslut:&#123;&#125;&quot;.format(reslut.result))time.sleep(25)print(&quot;task over?&#123;&#125;&quot;.format(reslut.ready()))print(&quot;task reslut:&#123;&#125;&quot;.format(reslut.result)) 关于任务的delay，官方文档（参考）是这样描述的，我把它理解为发送任务给celery或者celery调用待进来的任务。 reslut.ready() 返回任务执行是否执行完成True or False reslut.result 返回任务执行结果 我们在任务进入celery和结束分别检查一次。 二、看看结果1.启动worker进入learn_celery的父目录。启动learn_celery的这个应用worker，并指定并发数为10个 1celery -A learn_celery worker --loglevel=info --concurrency=10 若celery连接rabbitmq正常，我们可以看到如下的info 2.执行任务为了便于观察，我们另外开启一个窗口2，到learn_celery父目录运行task1_run模块 1python -m learn_celery.tasks1_run 开启窗口3，到learn_celery父目录运行task2_run模块 1python -m learn_celery.tasks2_run 可以看到经过各自任务的等待时间后，两个任务都顺利执行结束，并得到结果，接下来我们到worker上看一下info 由于celery的并发性，收到任务马上被调入执行，任务1耗时10s结果为33，任务2耗时20s结果为77 三、使用Flower监控celery1.启动flower1celery -A learn_celery flower 2. 查看web监控 http://ip:5555在Tasks中可以查看到当前任务队列的状态、参数、接收和启动、执行时间。在Dashborad中查看当前worker节点的相关信息 文章有不足的地方欢迎指出。 欢迎收藏、点赞、提问。关注顶级饮水机管理员，除了管烧热水，有时还做点别的。 NEXT celery的深入了解 celery在django中的使用","categories":[{"name":"python","slug":"python","permalink":"https://iqsing.github.io/categories/python/"}],"tags":[{"name":"celery","slug":"celery","permalink":"https://iqsing.github.io/tags/celery/"}]},{"title":"通过浏览器运行cmd命令、启动steam","slug":"通过浏览器运行cmd命令、启动steam","date":"2021-06-29T16:47:21.000Z","updated":"2022-02-04T05:16:12.082Z","comments":true,"path":"2021/06/30/通过浏览器运行cmd命令、启动steam/","link":"","permalink":"https://iqsing.github.io/2021/06/30/%E9%80%9A%E8%BF%87%E6%B5%8F%E8%A7%88%E5%99%A8%E8%BF%90%E8%A1%8Ccmd%E5%91%BD%E4%BB%A4%E3%80%81%E5%90%AF%E5%8A%A8steam/","excerpt":"","text":"我们先来看看实现起来的效果，我们在浏览器中输入ping so.com 试试打开计算器、启动steam 要实现这个效果其实用到了浏览器自定义协议，我们可以通过自定义协议打开wechat、扣扣、emali应用等待。比如在web客服系统中通常会使用tencent://自定义协议提供一个扣扣聊天按钮，当我们点击时浏览器会启动外部应用。 我们将自定义协议写入注册表后，浏览器会根据协议启动我们的先导应用，如下在tencent://协议中启动了一个Timwp.exe，Timwp.exe将URL参数解析后启动对应服务。 据此我们就可以自己注册一个cmd协议来启动我们的应用。我们将如下urlCmd.reg注册表文件导入到注册表HKEY_CLASSES_ROOT项中，当然也可以手动在注册表中添加项和字串来实现，我们告诉注册表我要注册一个URL Protocol,他的目录结构为HKEY_CLASSES_ROOT\\cmd\\shell\\open\\command。 123456789101112Windows Registry Editor Version 5.00[HKEY_CLASSES_ROOT\\cmd]@=&quot;URL: cmd protocol&quot;&quot;URL Protocol&quot;=&quot;&quot;[HKEY_CLASSES_ROOT\\cmd\\shell][HKEY_CLASSES_ROOT\\cmd\\shell\\open][HKEY_CLASSES_ROOT\\cmd\\shell\\open\\command]@=&quot;C:\\\\Windows\\\\system32\\\\urlCmd.exe \\&quot;%1\\&quot;&quot; 导入后看起来是这样的，我们要启动的先导应用为C:\\Windows\\system32\\urlCmd.exe 接下来我们要编写一个urlCmd.exe来实现我们需要的功能，这里我们通过go来编写，我们将传过来的URL解析为&lt;command&gt; &lt;arg&gt;形式，然后调用cmd来执行。注意传参是经过URL编码的，在下面urlCmd.go我们只做了空格的解码。 12345678910111213141516171819202122232425262728293031323334353637//urlCmd.gopackage mainimport ( &quot;fmt&quot; &quot;log&quot; &quot;os&quot; &quot;os/exec&quot; &quot;strings&quot; &quot;time&quot;)func main() &#123; var cmd_args, cmd_com string // 带参数 不带参 cmd_all := os.Args[1:] // [cmd://&lt;command&gt;%20&lt;arg&gt;] [cmd://&lt;command&gt;/] cmd_cmd := strings.Split(string(cmd_all[0]), &quot;//&quot;) // [cmd: &lt;command&gt;%20&lt;arg&gt;] [cmd: &lt;command&gt;/] cmd_pre := strings.Split(string(cmd_cmd[1]), &quot;%20&quot;) // [&lt;command&gt; &lt;arg&gt;] [&lt;command&gt;/] if len(cmd_pre) == 1 &#123; cmd_args = &quot;&quot; // 参数为空 &#125; else &#123; cmd_args = strings.Replace(cmd_pre[1], &quot;/&quot;, &quot;&quot;, 1) &#125; cmd_com = strings.Replace(cmd_pre[0], &quot;/&quot;, &quot;&quot;, 1) fmt.Printf(&quot;\\ncommand: %v\\nargs: %v\\n&quot;, cmd_com, cmd_args) cmd := exec.Command(&quot;cmd&quot;, &quot;/c&quot;, cmd_com, cmd_args) cmd.Stdout = os.Stdout cmd.Stderr = os.Stderr err := cmd.Run() if err != nil &#123; log.Fatalf(&quot;err:%v&quot;, err) &#125; time.Sleep(2 * time.Second)&#125; 我们build一下编译成exe文件 1go bulid urlCmd.go 将urlCmd.exe放到C:\\Windows\\system32\\目录下即可，这样我们在浏览器以cmd://协议打开的URL都会路由到这个应用啦。 我学废了，你呢？urlCmd.go还有许多改进空间，有兴趣的同学可以做一个自己的解析器试试哦。 文章有不足的地方欢迎在评论区指出。 欢迎收藏、点赞、提问。关注顶级饮水机管理员，除了管烧热水，有时还做点别的。","categories":[{"name":"golang","slug":"golang","permalink":"https://iqsing.github.io/categories/golang/"}],"tags":[{"name":"commandline","slug":"commandline","permalink":"https://iqsing.github.io/tags/commandline/"}]},{"title":"git命令使用（必备系列）","slug":"git命令使用（必备系列）","date":"2021-06-23T16:47:21.000Z","updated":"2022-02-04T05:16:11.823Z","comments":true,"path":"2021/06/24/git命令使用（必备系列）/","link":"","permalink":"https://iqsing.github.io/2021/06/24/git%E5%91%BD%E4%BB%A4%E4%BD%BF%E7%94%A8%EF%BC%88%E5%BF%85%E5%A4%87%E7%B3%BB%E5%88%97%EF%BC%89/","excerpt":"","text":"git是一个分布式版本控制系统，得益于高效、协作和快速的项目代码管理特性几乎每一个软件开发团队都在深度使用。本篇是对git命令的介绍，涵盖了不低于95%的日常操作命令，对你有用话可以收藏一下哦。 一、初始化相关初始化一个仓库 1git init myrepo 克隆一个仓库到本地myrepo目录 12git clone git://github.com/linux/linux.git myrepogit remote add origin git://github.com/linux/linux.git #指定（关联）远程仓库地址，适用于从本地创建的仓库 二、配置相关查看配置信息 12git config --list git config --list --global #全局的 配置.git文件 12git config -e git config -e --global 全局的 配置全局认证信息，去除--global 为当前仓库配置 123git config --global user.name &quot;user_git&quot;git config --global user.email git@github.comgit config --global user.password &quot;mypasswd&quot; 三、常规操作相关git中数据在工作区、暂存区、本地仓库、远程仓库 区域流转，下面图片可以比较清晰的呈现 任何时候查看文件git文件流转当前状态 1git status 添加文件到暂存区 12git add &lt;file&gt;git add . #添加所有文件（改动文件） 从暂存区删除文件（添加了无用文件） 12git restore --staged &lt;file&gt;git reset &lt;file&gt; #效果一样 删除或移动、重命名文件，并自动提交到暂存区。用于处理已提交到仓库的文件 1git rm|mv &lt;file&gt; 提交修改到本地仓库 12git commit -m &quot;change to...&quot;git commit -am &quot;change to ...&quot; #用于直接提交修改文件，无法提交新增文件 撤销某次提交（会在分支上长生一次commit记录） 1git revert &lt;commit&gt; 查看提交记录 123git loggit log --graph #图形化查看git blame &lt;file&gt; #列表的方式查看指定文件修改历史 查看文件修改或区别 123git diff #暂存和工作区所有差别 git diff &lt;file&gt; #查看指定文件git diff HEAD -- &lt;file&gt; #查看工作区和版本库里面最新版本的区别 反复横跳（版本回退） 123git reset &lt;commitid&gt; #我们使用git log查询到需要回退版本的commitidgit reset HEAD~1 #回退到上一个版本git rest HEAD~n #回退到前n个版本 提交修改到远程仓库 123git push #提交有冲突时需要拉取远程最新版本git push orgin master #将本地master分支提交到远程master分支git push -u orgin master #提交并关联（适用于从本地创建的仓库） 拉取远程仓库最新版本 1234567git pull #拉取合并git fetch #拉取不合并git pull --rebase #不会产生merge记录，保持分支干净卫生git add &lt;conflict-file&gt; #添加解决的冲突的文件git rebase --continue #解决冲突后继续rebase 四、分支管理查看本地分支 1git branch 创建分支 1git branch &lt;branchName&gt; 删除分支 1git branch -d &lt;branchName&gt; #-D 强制删除 切换分支 12345git checkout &lt;branchName&gt;git checkout -b &lt;branchName&gt; #创建分支并切换git switch &lt;branchName&gt;git switch -c &lt;branchName&gt; #创建分支并切换 拉取远程分支 12git checkout -b dev(本地分支名称) origin/develop(远程分支名称) #创建并切换git fetch origin develop（develop为远程仓库的分支名 #创建 checkout回滚到某个commit id 12git checkout &lt;commitid&gt; #不会影响当前的工作区或分支（只读状态），修改不被保存git checkout master #回到当前工作区（分支） checkout回滚某个文件到commitid 1git checkout &lt;commit&gt; &lt;file&gt; #会影响当前工作区文件 五、分支操作–合并分支快速模式，head指针的移动（fast-forward）,合并dev到当前分支 1git merge dev #合并dev分支 普通模式（no ff），相当于head指针移动到新节点，保留合并历史分支。 1git merge --no-ff -m &quot;merge it&quot; dev 六、标签的管理查看标签列表 1git tag 查看标签信息 12git show v0.01git show commit_id #查看某次提交的信息 打标 1git tag v0.01 对某次commit打标 123git tag v0.01 f45212545git tag -a v0.01 -m &quot;v0.01 release&quot; f45212545 #包含文字说明 删除标签 1git tag -d v0.01 将标签推送到远程，标签默认只存储到本地 12git push origin &lt;tagname&gt; git push origin --tags #推送所有标签 删除远程标签 12git tag -d v0.01git push origin :refs/tags/v0.01 七、其他工作区暂存 1git stash 查看暂存的工作区 1git stash list 恢复暂存工作区，stash内容不删除 1234git stash apply git stash apply &lt;stash@&#123;n&#125;&gt; #恢复到执行工作区git stash drop #删除stash内容 恢复暂存工作区，stash内容自动删除 12git stash pop git stash pop &lt;stash@&#123;n&#125;&gt; 复制提交到当前分支，减少重复修改 1git cherry-pick &lt;commit&gt; 同属必备系列文章： vim命令：https://juejin.cn/post/6966772543919226887 文章有不足的地方欢迎在评论区指出。 欢迎收藏、点赞、提问。关注顶级饮水机管理员，除了管烧热水，有时还做点别的。","categories":[{"name":"linux","slug":"linux","permalink":"https://iqsing.github.io/categories/linux/"}],"tags":[{"name":"git","slug":"git","permalink":"https://iqsing.github.io/tags/git/"}]},{"title":"python django中restful框架的使用","slug":"python django中restful框架的使用","date":"2021-06-21T16:47:21.000Z","updated":"2022-02-04T05:16:12.032Z","comments":true,"path":"2021/06/22/python django中restful框架的使用/","link":"","permalink":"https://iqsing.github.io/2021/06/22/python%20django%E4%B8%ADrestful%E6%A1%86%E6%9E%B6%E7%9A%84%E4%BD%BF%E7%94%A8/","excerpt":"","text":"在使用django进行前后台分离开发时通常会搭配django-rest-framework框架创建RESTful风格的接口API。框架介绍及版本要求可参考官方地址：https://www.django-rest-framework.org 本文以创建man包含name、sex字段的API为实例学习django-rest-framework框架的使用。 主要包含下面5个步骤： 创建Django项目 创建ORM模型 加载Django REST Framework 序列化模型 创建加载数据的view和url 1.创建Django项目创建django_rest 1django-admin startproject django_rest 进入django_rest，创建虚拟环境env 1virtualenv env 激活虚拟环境,并安装django 1source ./env/bin/activate 安装 django 1pip install django 创建rest_app 1python manage.py startapp rest_app 注册app，将app添加到INSTALLED_APPS 123456789101112#setting.pyINSTALLED_APPS = [ &#x27;django.contrib.admin&#x27;, &#x27;django.contrib.auth&#x27;, &#x27;django.contrib.contenttypes&#x27;, &#x27;django.contrib.sessions&#x27;, &#x27;django.contrib.messages&#x27;, &#x27;django.contrib.staticfiles&#x27;, &#x27;rest_app&#x27;] 创建后台admin账户用于管理 1234567$ python manage.py createsuperuserUsername (leave blank to use &#x27;root&#x27;):admin Email address: Password: Password (again): Superuser created successfully. 2.创建ORM模型数据库我们使用默认sqlite3 即可，如需要变更可在setting.py中databases配置。 修改我们/django_rest/models.py添加我们man的模型 1234567891011121314#models.pyfrom django.db import models# Create your models here.class Man(models.Model): name = models.CharField(max_length=64) sex = models.CharField(max_length=64) def __str__(self): return self.name 做数据库迁移 12python manage.py makemigrations python manage.py migrate 将Man这个model注册到我们的后台，以便可以通过django的后台做增删查改，编写admin.py如下 123456#admin.pyfrom django.contrib import adminfrom .models import Man# Register your models here.admin.site.register(Man) # 注册Man到后台 启动django服务 1python manage.py runserver 访问 http://127.0.0.1:8000/admin/可以看到登录界面，输入密码登录 可以看到我们的rest_app下的模型man对象mans 我们添加一个那个男人lgd.ame 3. 加载Django REST Framework安装工具包 1pip install djangorestframework 注册rest_framework 1234567891011#setting.pyINSTALLED_APPS = [ &#x27;django.contrib.admin&#x27;, &#x27;django.contrib.auth&#x27;, &#x27;django.contrib.contenttypes&#x27;, &#x27;django.contrib.sessions&#x27;, &#x27;django.contrib.messages&#x27;, &#x27;django.contrib.staticfiles&#x27;, &#x27;rest_app&#x27; &#x27;rest_framework&#x27; #注册] 4.序列化模型序列化器会把我们的模型数据转化（序列化）为json格式，这样就能够被请求了。同样当有josn数据提交过来的时候，序列化器会将json数据转换为模型供咋们使用。 我们在rest_app下创建文件serializer.py 我们要做三件事： 导入Man模型 导入序REST Framework序列化器 创建新的类将模型和序列化器链接起来 12345678910from rest_framework import serializersfrom .models import Manclass Manserializer(serializers.HyperlinkedModelSerializer): class Meta: model = Man fields = (&#x27;name&#x27;,&#x27;sex&#x27;) 5.创建加载数据的view和url我们需要把序列化后的数据返回给浏览器，所以要做一下步骤： 通过不通的Man查询数据库 将查询后的数据传递给序列化器，通过序列化器转化为json 我们在rest_app/views.py编写我们的视图,ModelViewSet由rest_framework提供，包含了get、post方法 12345678910# views.pyfrom rest_framework import viewsetsfrom .serializers import ManSerializerfrom .models import Manclass ManViewSet(viewsets.ModelViewSet): queryset = Man.objects.all().order_by(&#x27;name&#x27;) #查询结果给queryset serializer_class = ManSerializer #对结果进序列化 在django_rest目录下urls.py添加api路由 1234567from django.contrib import adminfrom django.urls import path, includeurlpatterns = [ path(&#x27;admin/&#x27;, admin.site.urls), path(&#x27;&#x27;, include(&#x27;rest_app.urls&#x27;)), ] 在rest_app目录下创建urls.py添加视图路由，通过rest_framework中router确保我们的请求到正确的动态资源。 12345678910111213from django.urls import include, pathfrom rest_framework import routersfrom . import viewsrouter = routers.DefaultRouter()router.register(r&#x27;man&#x27;, views.ManViewSet) #路由到ManViewSet视图# Wire up our API using automatic URL routing.# Additionally, we include login URLs for the browsable API.urlpatterns = [ path(&#x27;&#x27;, include(router.urls)), #使用router路由 path(&#x27;api-auth/&#x27;, include(&#x27;rest_framework.urls&#x27;, namespace=&#x27;rest_framework&#x27;))] 最后我们来启动服务,访问http://127.0.0.1:8000/ 可以在浏览器查看到我们api信息 1python manage.py runserver 访问http://127.0.0.1:8000/man/ 来查看man资源 通过id来访问api资源http://127.0.0.1:8000/man/1/ 这样我们一个基础restful风格的API创建完成了。感觉关键点还是理解ModelViewSet和内置router不读源码很难知其所以然。 文章有不足的地方欢迎指出。 欢迎收藏、点赞、提问。关注顶级饮水机管理员，除了管烧热水，有时还做点别的。","categories":[{"name":"python","slug":"python","permalink":"https://iqsing.github.io/categories/python/"}],"tags":[{"name":"django","slug":"django","permalink":"https://iqsing.github.io/tags/django/"},{"name":"restful","slug":"restful","permalink":"https://iqsing.github.io/tags/restful/"}]},{"title":"python django框架+vue.js前后端分离","slug":"python django框架+vue.js前后端分离","date":"2021-06-16T16:47:21.000Z","updated":"2022-02-04T05:16:12.043Z","comments":true,"path":"2021/06/17/python django框架+vue.js前后端分离/","link":"","permalink":"https://iqsing.github.io/2021/06/17/python%20django%E6%A1%86%E6%9E%B6+vue.js%E5%89%8D%E5%90%8E%E7%AB%AF%E5%88%86%E7%A6%BB/","excerpt":"","text":"本文用于学习django+vue.js实现web前后端分离协作开发。以一个添加和删除数据库书籍应用为实例。 django框架官方地址：https://www.djangoproject.com/ vue.js 框架官方地址：https://cn.vuejs.org/ 一、构建django项目1. 创建工程文件和APP创建django_vue 1django-admin startproject django_vue 进入django_vue，创建虚拟环境django_vue_env 12pip install virtualenv #安装virtualenv django_vue_env 激活虚拟环境,并安装django 1source ./django_vue_env/bin/activate 安装 django、后面用到的django-cors-headers（跨域）、requests 创建django app 1python manage.py startapp app 我们的目录应该是这样的，appfront为vue项目会在后面创建。 数据库我们使用默认sqlite3 即可，如需要变更可在setting.py中databases配置。 添加app到INSTALLED_APPS 12345678910INSTALLED_APPS = [ &#x27;django.contrib.admin&#x27;, &#x27;django.contrib.auth&#x27;, &#x27;django.contrib.contenttypes&#x27;, &#x27;django.contrib.sessions&#x27;, &#x27;django.contrib.messages&#x27;, &#x27;django.contrib.staticfiles&#x27;, &#x27;app&#x27;] 添加数据库模型,包含book_name和add_time用于记录书籍名称和添加时间。 123456789101112from django.db import models# Create your models here.class Book(models.Model): book_name = models.CharField(max_length=64) add_time = models.DateTimeField(auto_now_add=True) def __str__(self): return self.book_name 做数据库迁移 12python manage.py makemigrations apppython manage.py migrate 编写views.py添加 show_books 和add_book两个api接口，通过JsonResponse将请求数据返回。 12345678910111213141516171819202122232425262728293031323334353637383940from django.shortcuts import render# Create your views here.# 需要导入相关的模块from django.http import JsonResponsefrom django.views.decorators.http import require_http_methodsfrom django.core import serializersimport requestsimport jsonfrom .models import Book@require_http_methods([&quot;GET&quot;])def add_book(request): response = &#123;&#125; try: book = Book(book_name=request.GET.get(&#x27;book_name&#x27;)) book.save() response[&#x27;msg&#x27;] = &#x27;success&#x27; response[&#x27;error_num&#x27;] = 0 except Exception as e: response[&#x27;msg&#x27;] = str(e) response[&#x27;error_num&#x27;] = 1 return JsonResponse(response)@require_http_methods([&quot;GET&quot;])def show_books(request): response = &#123;&#125; try: books = Book.objects.filter() response[&#x27;list&#x27;] = json.loads(serializers.serialize(&quot;json&quot;, books)) response[&#x27;msg&#x27;] = &#x27;success&#x27; response[&#x27;error_num&#x27;] = 0 except Exception as e: response[&#x27;msg&#x27;] = str(e) response[&#x27;error_num&#x27;] = 1 return JsonResponse(response) 在django_vue目录下urls.py添加api路由 12345678from django.contrib import adminfrom django.urls import path,includeimport app.urlsurlpatterns = [ path(&#x27;admin/&#x27;, admin.site.urls), path(&#x27;api/&#x27;,include(app.urls)),] 在app目录下的urls.py添加视图路由 12345678910from django.urls import path,re_path# 导入 myapp 应用的 views 文件from . import viewsurlpatterns = [ re_path(r&#x27;add_book$&#x27;, views.add_book), re_path(r&#x27;show_books$&#x27;, views.show_books)] 重启服务，通过curl命令测试api可用性，如下接口正常。 123456789python manage.py runservercurl http://127.0.0.1:8000/api/add_book?book_name=mylife&#123;&quot;msg&quot;: &quot;success&quot;, &quot;error_num&quot;: 0&#125;curl http://127.0.0.1:8000/api/show_books&#123;&quot;list&quot;: [ &#123;&quot;model&quot;: &quot;app.book&quot;, &quot;pk&quot;: 9, &quot;fields&quot;: &#123;&quot;book_name&quot;: &quot;mylife&quot;, &quot;add_time&quot;: &quot;2021-06-16T14:44:49.230Z&quot;&#125;&#125;], &quot;msg&quot;: &quot;success&quot;, &quot;error_num&quot;: 0&#125; django后端大致构建完成，接下来做vue前端。 一、构建vue项目安装vue初始化命令行工具vue-cli 1npm install -g vue-cli 在django_vue目录下构建前端工程appfront，其中包含webpack工具。 1vue-init webpack appfront appfront目录如下 安装渲染element-ui 、vue-resource 12npm install element-uinpm install vue-resource 调整src/main.js如下 123456789101112131415161718import Vue from &#x27;vue&#x27;import App from &#x27;./App&#x27;import router from &#x27;./router&#x27;import ElementUI from &#x27;element-ui&#x27;import VueResource from &#x27;vue-resource&#x27;import &#x27;element-ui/lib/theme-chalk/index.css&#x27;Vue.config.productionTip = falseVue.use(ElementUI)Vue.use(VueResource)/* eslint-disable no-new */new Vue(&#123; el: &#x27;#app&#x27;, router, components: &#123; App &#125;, template: &#x27;&lt;App/&gt;&#x27;&#125;) 在src/component目录下新建Home.vue,包含showBooks和addBook两个方法用于api查询。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081&lt;template&gt; &lt;div class=&quot;home&quot;&gt; &lt;el-row display=&quot;margin-top:10px&quot;&gt; &lt;el-input v-model=&quot;input&quot; placeholder=&quot;请输入书名&quot; style=&quot;display:inline-table; width: 30%; float:left&quot;&gt;&lt;/el-input&gt; &lt;el-button type=&quot;primary&quot; @click=&quot;addBook()&quot; style=&quot;float:left; margin: 2px;&quot;&gt;新增&lt;/el-button&gt; &lt;/el-row&gt; &lt;el-row&gt; &lt;el-table :data=&quot;bookList&quot; style=&quot;width: 100%&quot; border&gt; &lt;el-table-column prop=&quot;id&quot; label=&quot;编号&quot; min-width=&quot;100&quot;&gt; &lt;template scope=&quot;scope&quot;&gt; &#123;&#123; scope.row.pk &#125;&#125; &lt;/template&gt; &lt;/el-table-column&gt; &lt;el-table-column prop=&quot;book_name&quot; label=&quot;书名&quot; min-width=&quot;100&quot;&gt; &lt;template scope=&quot;scope&quot;&gt; &#123;&#123; scope.row.fields.book_name &#125;&#125; &lt;/template&gt; &lt;/el-table-column&gt; &lt;el-table-column prop=&quot;add_time&quot; label=&quot;添加时间&quot; min-width=&quot;100&quot;&gt; &lt;template scope=&quot;scope&quot;&gt; &#123;&#123; scope.row.fields.add_time &#125;&#125; &lt;/template&gt; &lt;/el-table-column&gt; &lt;/el-table&gt; &lt;/el-row&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123; name: &#x27;home&#x27;, data () &#123; return &#123; input: &#x27;&#x27;, bookList: [], &#125; &#125;, mounted: function() &#123; this.showBooks() &#125;, methods: &#123; addBook()&#123; this.$http.get(&#x27;http://139.198.114.148:8000/api/add_book?book_name=&#x27; + this.input) .then((response) =&gt; &#123; var res = JSON.parse(response.bodyText) if (res.error_num == 0) &#123; this.showBooks() &#125; else &#123; this.$message.error(&#x27;新增书籍失败，请重试&#x27;) console.log(res[&#x27;msg&#x27;]) &#125; &#125;) &#125;, showBooks()&#123; this.$http.get(&#x27;http://139.198.114.148:8000/api/show_books&#x27;) .then((response) =&gt; &#123; var res = JSON.parse(response.bodyText) console.log(res) if (res.error_num == 0) &#123; this.bookList = res[&#x27;list&#x27;] &#125; else &#123; this.$message.error(&#x27;查询书籍失败&#x27;) console.log(res[&#x27;msg&#x27;]) &#125; &#125;) &#125; &#125;&#125;&lt;/script&gt;&lt;!-- Add &quot;scoped&quot; attribute to limit CSS to this component only --&gt;&lt;style scoped&gt;h1, h2 &#123; font-weight: normal;&#125;ul &#123; list-style-type: none; padding: 0;&#125;li &#123; display: inline-block; margin: 0 10px;&#125;a &#123; color: #42b983;&#125;&lt;/style&gt; 我们通过django-cors-headers处理跨域问题 12345678910INSTALLED_APPS = [ &#x27;django.contrib.admin&#x27;, &#x27;django.contrib.auth&#x27;, &#x27;django.contrib.contenttypes&#x27;, &#x27;django.contrib.sessions&#x27;, &#x27;django.contrib.messages&#x27;, &#x27;django.contrib.staticfiles&#x27;, &#x27;app&#x27;, &#x27;corsheaders&#x27;, //添加app] 添加中间件corsheaders.middleware.CorsMiddleware 12345678910MIDDLEWARE = [ &#x27;django.middleware.security.SecurityMiddleware&#x27;, &#x27;django.contrib.sessions.middleware.SessionMiddleware&#x27;, &#x27;django.middleware.common.CommonMiddleware&#x27;, &#x27;django.middleware.csrf.CsrfViewMiddleware&#x27;, &#x27;corsheaders.middleware.CorsMiddleware&#x27;, &#x27;django.contrib.auth.middleware.AuthenticationMiddleware&#x27;, &#x27;django.contrib.messages.middleware.MessageMiddleware&#x27;, &#x27;django.middleware.clickjacking.XFrameOptionsMiddleware&#x27;,] 在setting.py中配置跨域规则 123456789101112131415161718192021CORS_ALLOW_METHODS = ( &#x27;DELETE&#x27;, &#x27;GET&#x27;, &#x27;OPTIONS&#x27;, &#x27;PATCH&#x27;, &#x27;POST&#x27;, &#x27;PUT&#x27;, &#x27;VIEW&#x27;,) CORS_ALLOW_HEADERS = ( &#x27;accept&#x27;, &#x27;accept-encoding&#x27;, &#x27;authorization&#x27;, &#x27;content-type&#x27;, &#x27;dnt&#x27;, &#x27;origin&#x27;, &#x27;user-agent&#x27;, &#x27;x-csrftoken&#x27;, &#x27;x-requested-with&#x27;,) npm run dev启动node服务器 通过npm run build打包前端到dist目录，用于后续django链接。 三、django链接到前端调整django_vue目录下路由urls.py如下 1234567891011from django.contrib import adminfrom django.urls import path,includefrom django.views.generic import TemplateView //导入通用视图import app.urlsurlpatterns = [ path(&#x27;admin/&#x27;, admin.site.urls), path(&#x27;api/&#x27;,include(app.urls)), path(&#x27;&#x27;,TemplateView.as_view(template_name=&quot;index.html&quot;)), //路由到index.html] 在setting.py下添加静态文件地址 1STATICFILES_DIRS = [(os.path.join(BASE_DIR,&#x27;appfront/dist/static&#x27;))] 启动django 服务 1python manage.py runserver 访问我们的django地址，此时django已链接到前端 参考：https://github.com/rogerlh/django_with_vue NEXT django-rest-framework 创建restful api django wsgi的应用 文章有不足的地方欢迎在评论区指出。 欢迎收藏、点赞、提问。关注顶级饮水机管理员，除了管烧热水，有时还做点别的。","categories":[{"name":"python","slug":"python","permalink":"https://iqsing.github.io/categories/python/"}],"tags":[{"name":"django","slug":"django","permalink":"https://iqsing.github.io/tags/django/"},{"name":"vue.js","slug":"vue-js","permalink":"https://iqsing.github.io/tags/vue-js/"}]},{"title":"golang快速入门（九）JSON处理","slug":"golang快速入门（九）JSON处理","date":"2021-06-06T16:47:21.000Z","updated":"2022-02-04T05:16:11.852Z","comments":true,"path":"2021/06/07/golang快速入门（九）JSON处理/","link":"","permalink":"https://iqsing.github.io/2021/06/07/golang%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%EF%BC%88%E4%B9%9D%EF%BC%89JSON%E5%A4%84%E7%90%86/","excerpt":"","text":"一、 关于JSONJSON是一种简洁、结构清晰的轻量级数据交换格式。基本的JSON类型有数字（十进制或科学记数法）、布尔值（true或false）、字符串，其中字符串是以双引号包含的Unicode字符序列。 在python中可通过json.dumps()、json.loads()对数据进行编码（导出为字符串对象）和解码（导入为python对象）。 在golang可通过标准库中的encoding/json中json.Marshal()（编码）、json.Unmarshal()（解码）对json数据操作。 二、golang中json编码与解码 一个JSON数组是一个有序的值序列，写在一个方括号中并以逗号分隔；一个JSON数组可以用于编码Go语言的数组和slice。 一个JSON对象是一个字符串到值的映射，写成一系列的name:value对形式，用花括号包含并以逗号分隔；JSON的对象类型可以用于编码Go语言的map类型（key类型是字符串）和结构体。 1. 将切片、map、编码为json字符串12345678910111213141516171819202122package mainimport ( &quot;encoding/json&quot; &quot;fmt&quot; //&quot;os&quot;)func main() &#123; slcD := []string&#123;&quot;apple&quot;, &quot;peach&quot;, &quot;pear&quot;&#125; slcB, _ := json.Marshal(slcD) fmt.Println(string(slcB)) mapD := map[string]int&#123;&quot;apple&quot;: 5, &quot;lettuce&quot;: 7&#125; mapB, _ := json.Marshal(mapD) fmt.Println(string(mapB))&#125;//output[root@VM-0-5-centos course9]# go run json.go[&quot;apple&quot;,&quot;peach&quot;,&quot;pear&quot;]&#123;&quot;apple&quot;:5,&quot;lettuce&quot;:7&#125; 2.结构体编码为json 在结构体中可以使用key:&quot;value&quot;标签如下json:&quot;price&quot;标签作用于成员调整编码后的key，使其key为price。因为有些JSON成员名字和Go结构体成员名字并不相同，因此需要Go语言结构体成员Tag来指定对应的JSON名字。 在编码时，默认使用Go语言结构体的成员名字作为JSON的对象，只有可导出的成员能进行编码，即以大写开头。 123456789101112131415161718192021222324package mainimport ( &quot;encoding/json&quot; &quot;fmt&quot; //&quot;os&quot;)type response1 struct &#123; Page int `json:&quot;price&quot;` Fruits []string&#125;func main() &#123; res1 := &amp;response1&#123; Page: 20, Fruits: []string&#123;&quot;apple&quot;, &quot;peach&quot;, &quot;pear&quot;&#125;&#125; result, _ := json.Marshal(res1) fmt.Println(string(result))&#125;//output[root@VM-0-5-centos course9]# go run json2.go&#123;&quot;price&quot;:20,&quot;Fruits&quot;:[&quot;apple&quot;,&quot;peach&quot;,&quot;pear&quot;]&#125; 3. json 解码为golang对象json.Unmarshal([]byte(str), &amp;res) 传入了一个[]byte()类型字符串和一个接口类型参数。关于接口需要在后面学习，这里可以理解为不确定数据类型。为甚么不解析为map类型呢？得再思考。 1func Unmarshal(data []byte, v interface&#123;&#125;) error 我们创建了一个response2结构体用于接收解析后的数据。在我们创建的结构体中需要做tag处理，否则数据无法对正确应到成员变量。 12345678910111213141516171819202122232425package mainimport ( &quot;encoding/json&quot; &quot;fmt&quot; //&quot;os&quot;)type response2 struct &#123; Page int `json:&quot;price&quot;` Fruits []string&#125;func main() &#123; str := `&#123;&quot;page&quot;: 1, &quot;fruits&quot;: [&quot;apple&quot;, &quot;peach&quot;]&#125;` //golang中 ` 常用于非转义字符串 res := response2&#123;&#125; json.Unmarshal([]byte(str), &amp;res) fmt.Println(res) fmt.Println(res.Fruits[1])&#125;//output[root@VM-0-5-centos course9]# go run json3.go&#123;0 [apple peach]&#125;peach 从输出来看，序列化后的数据为一个结构体对象res，我们可以通过.开访问其中数据。 文章有不足的地方欢迎在评论区指出。 欢迎收藏、点赞、提问。关注顶级饮水机管理员，除了管烧热水，有时还做点别的。","categories":[{"name":"golang","slug":"golang","permalink":"https://iqsing.github.io/categories/golang/"}],"tags":[{"name":"DateType","slug":"DateType","permalink":"https://iqsing.github.io/tags/DateType/"}]},{"title":"golang快速入门（八）数据类型特别之处(下)","slug":"golang快速入门（八）数据类型特别之处(下)","date":"2021-06-06T16:47:21.000Z","updated":"2022-02-04T05:16:11.875Z","comments":true,"path":"2021/06/07/golang快速入门（八）数据类型特别之处(下)/","link":"","permalink":"https://iqsing.github.io/2021/06/07/golang%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%EF%BC%88%E5%85%AB%EF%BC%89%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E7%89%B9%E5%88%AB%E4%B9%8B%E5%A4%84(%E4%B8%8B)/","excerpt":"","text":"二、 golang 复合数据类型特别之处复合数据类型也是由基础数量类型构造出来的，golang中主要有数组、slice（切片）、map和结构体四种。数组和结构体都是有固定内存大小的数据结构。相比之下，slice和map则是动态的数据结构，它们将根据需要动态增长。 1. 数组数组定义定义及初始化，数组长度是固定的，且为常量，在编译时已确定。 123var q [3]int = [3]int&#123;1, 2, 3&#125; q := [...]int&#123;1, 2, 3&#125; // 或通过...占位，自动计算个数 可通过range在遍历数组的索引和值。 123456789var a [3]int for index, value := range a &#123; fmt.Printf(&quot;%d %d\\n&quot;, index, value)&#125;// 仅获取valuefor _, value := range a &#123; fmt.Printf(&quot;&quot;%d\\n&quot;, value)&#125; 向函数传递数组时不允许在函数内部修改底层数组的元素。 12345678910111213main()&#123; num := [3]int&#123;1, 2, 3&#125; myInt := 100 changeNum(myInt, num) fmt.Println(num[0], myInt)&#125;func changeNum(myInt int, num [3]int) &#123; num[0] = 0 myInt++&#125;//output1 100 我们传入changeNum函数一个变量和数组，底层数组数据均未被修改。在golang中函数参数变量接收的是一个复制的副本，并不是原始调用的变量。如果要通过数组修改底层数组数据可以像其他语言一样传入数组指针。 1234567891011main()&#123; num := [3]int&#123;1, 2, 3&#125; changeNumPoint(&amp;num) fmt.Println(num[0])&#125;func changeNumPoint(num *[3]int) &#123; num[0] = 0&#125;//output0 2.slice切片，和python中切片相似度较高，slice相比数组，其长度是可扩展的,slice创建一个合适大小的数组，然后slice的指针指向底层的数组。 slice 内部结构体 123456struct Slice &#123; byte* array; // actual data uintgo len; // number of elements uintgo cap; // allocated number of elements &#125;; 第一个指针是指向底层数组，后面是slice的长度和容量。在底层，make创建了一个匿名的数组变量，然后返回一个slice；只有通过返回的slice才能引用底层匿名的数组变量。我们可使用内建make创建slice。 1make([]T, len, cap) //cap省缺于len值相同 slice长度： 切片包含的元素个数。 slice容量： 从它的第一个元素开始数，到其底层数组元素末尾的个数。使用append添加数据时，容量小于长度时会自动扩展为原来两倍。 slice值包含指向第一个slice元素的指针，因此向函数传递slice将允许在函数内部修改底层数组的元素，上文提到数组传入函数是以副本形式。 1234567891011main()&#123; sliceNum := []int&#123;4, 5, 6&#125; changeSlice(sliceNum) fmt.Println(sliceNum[0]&#125;func changeSlice(sliceNum []int) &#123; sliceNum[0] = 0&#125;//output0 使用append追加多个元素： 1x = append(x, 4, 5, 6) copy(dest,src),对位复制。 123slice1 := []int&#123;1, 2, 3, 4, 5&#125;slice2 := []int&#123;6, 7, 8&#125;copy(slice2, slice1) 3.Map哈希表，类似于python中dic字典，map是一个无序的key/value对的集合。 通slice一样，可以使用make创建一个map，也可以通过range遍历： 1ages := make(map[string]int)&#123;&quot;key&quot;:value&#125; 通过key作为索引下标来访问map将产生一个value。如果key在map中是存在的，那么将得到与key对应的value；如果key不存在，那么将得到value对应类型的零值。我们可以通过这个值来判断key索引的value是否存在。 1234age, ok := ages[&quot;key&quot;]if !ok &#123; ... &#125;//可以优化为if age, ok := ages[&quot;bob&quot;]; !ok &#123; ... &#125; map的比较 1234567891011func equal(x, y map[string]int) bool &#123; if len(x) != len(y) &#123; return false &#125; for k, xv := range x &#123; if yv, ok := y[k]; !ok || yv != xv &#123; return false &#125; &#125; return true&#125; 使用map做统计 123456var m = make(map[string]int)func k(list []string) string &#123; return fmt.Sprintf(&quot;%q&quot;, list) &#125;func Add(list []string) &#123; m[k(list)]++ &#125;func Count(list []string) int &#123; return m[k(list)] &#125; 4.结构体结构体定义 123456789101112type Point struct &#123; Salary int ManagerID int&#125;var p Point //结构体变量//另一种写法type Point struct&#123; X, Y int &#125;p := Point&#123;1, 2&#125; 和变量、函数一样，构体成员名字是以大写字母开头的，那么该成员就是导出的。一个结构体可能同时包含导出和未导出的成员。 结构体作为函数的参数和返回值： 12345func Scale(p Point, factor int) Point &#123; return Point&#123;p.X * factor, p.Y * factor&#125;&#125;fmt.Println(Scale(Point&#123;1, 2&#125;, 5)) // &quot;&#123;5 10&#125;&quot; 指针的方式（提高效率）,可以在函数中修改结构体成员。 123func AwardAnnualRaise(e *Employee) &#123; e.Salary = e.Salary * 105 / 100&#125; 结构体嵌套，使定义简化和清晰。 1234567891011121314//点type Point struct &#123; X, Y int&#125;//圆type Circle struct &#123; Center Point Radius int&#125;//轮type Wheel struct &#123; Circle Circle Spokes int&#125; 访问成员 12345var w Wheelw.Circle.Center.X = 8w.Circle.Center.Y = 8w.Circle.Radius = 5w.Spokes = 20 显然要访问多层结构体较为复杂，go还提供了匿名成员来处理结构嵌套。匿名成员的数据类型必须是命名的类型或指向一个命名的类型的指针。 我们修改圆和轮为如下： 123456789type Circle struct &#123; Point Radius int&#125;type Wheel struct &#123; Circle Spokes int&#125; 访问成员： 12345var w Wheelw.X = 8 w.Y = 8 w.Radius = 5 w.Spokes = 20 从上面来看匿名成员特性只是对访问嵌套成员的点运算符提供了简短的语法糖。，其实在go中匿名成员并不要求是结构体类型；其实任何命名的类型都可以作为结构体的匿名成员。这个机制可以用于将一些有简单行为的对象组合成有复杂行为的对象。组合会在go面向对象中会大展拳脚。 文章有不足的地方欢迎在评论区指出。 欢迎收藏、点赞、提问。关注顶级饮水机管理员，除了管烧热水，有时还做点别的。","categories":[{"name":"golang","slug":"golang","permalink":"https://iqsing.github.io/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"https://iqsing.github.io/tags/golang/"}]},{"title":"golang快速入门（七）数据类型特别之处(上)","slug":"golang快速入门（七）数据类型特别之处(上)","date":"2021-06-03T16:47:21.000Z","updated":"2022-02-04T05:16:11.834Z","comments":true,"path":"2021/06/04/golang快速入门（七）数据类型特别之处(上)/","link":"","permalink":"https://iqsing.github.io/2021/06/04/golang%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%83%EF%BC%89%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E7%89%B9%E5%88%AB%E4%B9%8B%E5%A4%84(%E4%B8%8A)/","excerpt":"","text":"提示：本系列文章适合对Go有持续冲动的读者 Go语言将数据类型分为四类：基础类型、复合类型、引用类型和接口类型。在在节更多关注特有数据类型。 一、golang基础数据类型一些特别之处1. rune 、byterune == int32 byte==uint8，其中byte以用于强调数值是一个原始的数据而不是一个小的整数。 2. 无符号的整数类型uintptr在基本数据类型中有一种无符号的整数类型uintptr，没有指定具体的bit大小但是足以容纳指针。uintptr类型只有在底层编程时才需要，特别是Go语言和C语言函数库或操作系统接口相交互的地方。 3. 运算符差异在Go语言中，%取模运算符的符号和被取模数的符号总是一致的，因此-5%3和-5%-3结果都是-2。 除法运算符/的行为则依赖于操作数是否全为整数，比如5.0/4.0的结果是1.25，但是5/4的结果是1，因为整数除法会向着0方向截断余数。 4. 提供bit操作golang中提供了bit位操作（和C一样）： 123456&amp; 位运算 AND| 位运算 OR^ 位运算 XOR&amp;^ 位清空（AND NOT）&lt;&lt; 左移&gt;&gt; 右移 5. 字符不可变golang 中字符串的值是不可变的，一个字符串包含的字节序列永远不会被改变，但可以给一个字符串变量分配一个新字符串值。一个字符串是包含只读字节的数组，一旦创建，是不可变的。相比之下，一个字节slice的元素则可以自由地修改。 字符串和字节slice之间可以相互转换： 123s := &quot;abc&quot;b := []byte(s)s2 := string(b) 6. 不做转义字符串：`反斜杠引用的字符串7. 字符串长度问题 len(s)返回的为unicode字节长度，可以通过unicode/utf8包RuneCountInString(s)获取utf-8字符长度 12345678910111213141516171819202122package mainimport ( &quot;fmt&quot; &quot;unicode/utf8&quot;)func main() &#123; s := &quot;ab你好&quot; fmt.Println(len(s)) fmt.Println(utf8.RuneCountInString(s)) //使用range进行隐式转换 //for i, value := range s &#123; // fmt.Printf(&quot;%d\\t%q\\n&quot;, i, value) //&#125;&#125;//output[root@VM-0-5-centos course7]# go run string.go 84 8. 整数类型转换位字符将一个整数转为字符串，一种方法是用fmt.Sprintf返回一个格式化的字符串；另一个方法是用strconv.Itoa(“整数到ASCII”)： 123x := 123y := fmt.Sprintf(&quot;%d&quot;, x)fmt.Println(y, strconv.Itoa(x)) // &quot;123 123&quot; 9. iota 常量生产器在一个const声明语句中，在第一个声明的常量所在的行，iota将会被置为0，余下行自动累加1，我们可以通过构造含有iota的表达式对常量声明序列化。 典型的对星期的初始化。 123456789const ( Sunday = iota Monday //1 Tuesday //2 Wednesday Thursday Friday Saturday //6) 每次对bit偏移10位： 1234567891011const ( _ = 1 &lt;&lt; (10 * iota) KiB // 1024 MiB // 1048576 GiB // 1073741824 TiB // 1099511627776 1 &lt;&lt; 32 PiB // 1125899906842624 EiB // 1152921504606846976 ZiB // 1180591620717411303424 1 &lt;&lt; 64 YiB // 1208925819614629174706176) 10. 无类型常量在golang 中常量的定义潜在类型是基础类型 如 int float外，类型由编译器推断。在无类型的整数中，编译器为这些没有明确基础类型的数字常量提供比基础类型更高精度的算术运算达到了至少有256bit的运算精度。 比如如上对字节的定义，可以做如下运算。 1fmt.Println(YiB/ZiB) // &quot;1024&quot; 将无类型常量赋值给变量时无类型的常量将会被隐式转换为对应的类型。当无论是隐式或显式转换，将一种类型转换为另一种类型都要求目标可以表示原始值，以防值溢出。 在无类型整数常量转换为int，它的内存大小是不确定的，但是无类型浮点数和复数常量则转换为内存大小明确的float64和complex128。 1234fmt.Printf(&quot;%T\\n&quot;, 0) // &quot;int&quot;fmt.Printf(&quot;%T\\n&quot;, 0.0) // &quot;float64&quot;fmt.Printf(&quot;%T\\n&quot;, 0i) // &quot;complex128&quot;fmt.Printf(&quot;%T\\n&quot;, &#x27;\\000&#x27;) // &quot;int32&quot; (rune) 下一篇将一起学习一下golang复合类型特别之处。 文章有不足的地方欢迎在评论区指出。 欢迎收藏、点赞、提问。关注顶级饮水机管理员，除了管烧热水，有时还做点别的。","categories":[{"name":"golang","slug":"golang","permalink":"https://iqsing.github.io/categories/golang/"}],"tags":[{"name":"DataType","slug":"DataType","permalink":"https://iqsing.github.io/tags/DataType/"}]},{"title":"golang快速入门（六）特有程序结构","slug":"golang快速入门（六）特有程序结构","date":"2021-06-01T16:47:21.000Z","updated":"2022-02-04T05:16:11.883Z","comments":true,"path":"2021/06/02/golang快速入门（六）特有程序结构/","link":"","permalink":"https://iqsing.github.io/2021/06/02/golang%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%EF%BC%88%E5%85%AD%EF%BC%89%E7%89%B9%E6%9C%89%E7%A8%8B%E5%BA%8F%E7%BB%93%E6%9E%84/","excerpt":"","text":"提示：本系列文章适合对Go有持续冲动的读者 阅前须知：在程序结构这章，更多会关注golang中特有结构，与其他语言如C、python中相似结构（命名、声明、赋值、作用域等）不再赘述。 一、golang类型别名在go1.9中引入类型别名的特性，其中两个内置类型byte、 rune是uint8和int32的别名。类型别名即字面意思换个名字，两者是相同类型。 类型声明语句一般出现在包一级，因此如果新创建的类型名字的首字符大写，则在包外部也可以使用。 解决问题：大规模重构期间启用渐进式代码修复，特别是将类型从一个包移动到另一个包，以便引用旧名称的代码与引用新名称的代码互操作。类型别名对于允许使用单个顶级导出 API 将大型包拆分为多个实现包，以及对现有包的扩展版本进行试验也可能很有用。 定义方法： 1type T1 = T2 提案地址：18130-type-alias 二、变量初始化 如果初始化表达式被省略，那么将用零值初始化该变量，即如下： int –&gt;0 bool –&gt;false string–&gt;”” 接口类型：slice、指针、map、chan、函数 –&gt;nil 数组或结构体: 每个元素或字段初始为对应类型零值 零值初始化机制可以确保每个声明的变量总是有一个良好定义的值，这个特性可以减少许多额外工作 函数返回值初始化变量,比如err的获取。在赋值语句左边的变量和右边最终的求到的值必须有相同的数据类型。 123if err:=http.Get(addr),err&#123; //&#125; 初始化表达式可以是字面量或任意的表达式。在包级别声明的变量会在main入口函数执行前完成初始化，局部变量将在声明语句被执行到的时候完成初始化 简短式（:=）局部变量声明只可用于局部变量声明。并且简短变量声明语句对这些已经声明过的变量具有赋值行为。如下第二个声明只声明了out一个变量。 123in, err := os.Open(infile)// ...out, err := os.Create(outfile) 函数内生命的变量必须使用，不过可以使用下划线_ 丢弃。 三、变量生命周期对于在包一级声明的变量来说，它们的生命周期和整个程序的运行周期是一致的。 而相比之下，局部变量的生命周期则是动态的：每次从创建一个新变量的声明语句开始，直到该变量不再被引用为止，然后变量的存储空间可能被回收。 函数的参数变量和返回值变量都是局部变量。它们在函数每次被调用的时候创建。 四、golang package（包）与golang Module（模块）1. package:对一组函数或功能的封装，即避免重复造轮子。在1.12之后版本（引入了module）安装后，会在用户目录下创建一个 go 文件夹作为默认的 GOPATH，go get 会将远程的软件包下载到GOPTH目录下 pkg/mod 里。GOROOT为go文件和安装目录。$GOROOT/src为内置标准库。 GOPATH目录已下载的package $GOROOT/src目录内置package 在沒有使用 golang module 前导入他人分享的 package 需要先使用 go get 下载到本地，然后才可以通过import引用此 package 2. module:对package进行管理，即管理应用的依赖和package版本。引入module后项目目录不再只限制在GOPATH/src，你可以在任意文件夹下创建你的项目，再使用go mod init初始化即可。如下初始化一个module为github.com/csgo 在当前文件夹下产生go.mod文件。 1234567891011[root@VM-0-5-centos water_man]# lsgo.mod water_man.go[root@VM-0-5-centos water_man]# cat go.mod module github.com/csgo //文件名称go 1.15 //go版本replace github.com/csgo/rush_b =&gt; ../rush_b //重定向package到../rush_b文件夹下require github.com/csgo/rush_b v0.0.0-00010101000000-000000000000 //非标准库依赖包和版本 上层目录下自定义的package： go module会根据 go.mod 的依赖引用关系导入三方包。如果发现本地cache没有，就会从远程拉取。即如果github.com/csgo/rush_b未重定向到本地就会使用 go get 。当 go module下载了远程包后，同时会自动更新 go.mod 。 需要注意自定义的package函数、变量如果需要被外部引用则首字母应大写，即包级别的名字都是以大写字母开头。 文章有不足的地方欢迎在评论区指出。 欢迎收藏、点赞、提问。关注顶级饮水机管理员，除了管烧热水，有时还做点别的。","categories":[{"name":"golang","slug":"golang","permalink":"https://iqsing.github.io/categories/golang/"}],"tags":[{"name":"struct","slug":"struct","permalink":"https://iqsing.github.io/tags/struct/"}]},{"title":"golang快速入门（五）初尝web服务","slug":"golang快速入门（五）初尝web服务","date":"2021-05-31T16:47:21.000Z","updated":"2022-02-04T05:16:11.868Z","comments":true,"path":"2021/06/01/golang快速入门（五）初尝web服务/","link":"","permalink":"https://iqsing.github.io/2021/06/01/golang%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%EF%BC%88%E4%BA%94%EF%BC%89%E5%88%9D%E5%B0%9Dweb%E6%9C%8D%E5%8A%A1/","excerpt":"","text":"提示：本系列文章适合对Go有持续冲动的读者 初探golang web服务golang web开发是其一项重要且有竞争力的应用，本小结来看看再golang中怎么创建一个简单的web服务。 在不适用web框架的情况下，可以使用net/http包搭建一个web服务。 这里我们使用net/http创建一个打印请求URL的web服务。 1234567891011121314151617package mainimport ( //&quot;log&quot; &quot;fmt&quot; &quot;net/http&quot;)func main() &#123; http.HandleFunc(&quot;/&quot;, handler) http.ListenAndServe(&quot;localhost:6677&quot;, nil)&#125;func handler(w http.ResponseWriter, r *http.Request) &#123; fmt.Fprintf(w, &quot;url.path=%q\\n&quot;, r.URL.Path) //输出到文件流&#125; http.HandleFunc函数可以理解为URL路由。 http.ListenAndServe是web服务的创建核心。 handler是http请求处理函数，接受一个http.ResponseWriter文件流 和http.Request类型的对象。 123[root@VM-0-5-centos ~]# curl localhost:6677/123url.path=&quot;/123&quot; 我们通过handler函数来对访问url做访问数计算。 引入golang sync中的互斥锁，这样同时存在多个请求时只有一个goroutine改变count计数。互斥锁后续深入了解。 1234567891011121314151617181920212223package mainimport ( //&quot;log&quot; &quot;fmt&quot; &quot;net/http&quot; &quot;sync&quot;)var count intvar mutex sync.Mutex //使用互斥锁func main() &#123; http.HandleFunc(&quot;/&quot;, handler) http.ListenAndServe(&quot;localhost:6677&quot;, nil)&#125;func handler(w http.ResponseWriter, r *http.Request) &#123; mutex.Lock() count++ mutex.Unlock() fmt.Fprintf(w, &quot;request url.path:%q has %d times\\n&quot;, r.URL.Path, count)&#125; 我们来看看请求结果如下： 123456[root@VM-0-5-centos ~]# curl localhost:6677/golangrequest url.path:&quot;/golang&quot; has 1 times[root@VM-0-5-centos ~]# curl localhost:6677/golangrequest url.path:&quot;/golang&quot; has 2 times[root@VM-0-5-centos ~]# curl localhost:6677/golangrequest url.path:&quot;/golang&quot; has 3 times http.Request类型对象除了URL.Path属性外还有Method、Proto等。我们通过handler函数分别打印出来。 123456789101112131415161718192021222324252627package mainimport ( //&quot;log&quot; &quot;fmt&quot; &quot;net/http&quot; &quot;sync&quot;)var count intvar mutex sync.Mutex //使用互斥锁func main() &#123; http.HandleFunc(&quot;/&quot;, handler) http.ListenAndServe(&quot;localhost:6677&quot;, nil)&#125;func handler(w http.ResponseWriter, r *http.Request) &#123; fmt.Fprintf(w, &quot;%s,%s,%s,\\n&quot;, r.Method, r.URL, r.Proto) fmt.Fprintf(w, &quot;host:%q\\nremoteaddr:%q\\n&quot;, r.Host, r.RemoteAddr) for k, v := range r.Header &#123; fmt.Fprintf(w, &quot;Header[%q]:%q\\n&quot;, k, v) &#125; for k, v := range r.Form &#123; fmt.Fprintf(w, &quot;Form[%q]:%q\\n&quot;, k, v) &#125;&#125; 创建表单接受后输出如下： 12345678//outputGET,/helloweb,HTTP/1.1,host:&quot;localhost:6677&quot;remoteaddr:&quot;127.0.0.1:58088&quot;Header[&quot;User-Agent&quot;]:[&quot;curl/7.29.0&quot;]Header[&quot;Accept&quot;]:[&quot;*/*&quot;]Form[parm1]:helloForm[parm2]:web 本次简单的了解了一下golang web服务，也是初尝章节结束。接下来会比较深入的学习golang的精彩细节与精华。 文章有不足的地方欢迎在评论区指出。 欢迎收藏、点赞、提问。关注顶级饮水机管理员，除了管烧热水，有时还做点别的。","categories":[{"name":"golang","slug":"golang","permalink":"https://iqsing.github.io/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"https://iqsing.github.io/tags/golang/"}]},{"title":"golang快速入门（四）初尝http请求","slug":"golang快速入门（四）初尝http请求","date":"2021-05-29T16:47:21.000Z","updated":"2022-02-04T05:16:11.891Z","comments":true,"path":"2021/05/30/golang快速入门（四）初尝http请求/","link":"","permalink":"https://iqsing.github.io/2021/05/30/golang%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%EF%BC%88%E5%9B%9B%EF%BC%89%E5%88%9D%E5%B0%9Dhttp%E8%AF%B7%E6%B1%82/","excerpt":"","text":"提示：本系列文章适合有其他语音基础并对Go有持续冲动的读者 一、golang获取HTTP请求1.在golang标准库中提供了net包来处理网络连接，通过http.Get创建http请求并返回服务器响应流。再通过ReadAll读取response全部内容。1234567891011121314151617181920212223242526package mainimport ( &quot;fmt&quot; &quot;io/ioutil&quot; &quot;net/http&quot; &quot;os&quot;)func main() &#123; for _, arg := range os.Args[1:] &#123; res, err := http.Get(arg) if err != nil &#123; fmt.Println(err) os.Exit(1) &#125; b, err := ioutil.ReadAll(res.Body) res.Body.Close() if err != nil &#123; fmt.Println(err) os.Exit(1) &#125; fmt.Printf(&quot;%s&quot;, b) &#125;&#125; 以访问360为例 超时会当成错误被捕获 二、练习1.函数调用io.Copy(dst, src)会从src中读取内容，并将读到的结果写入到dst中，使用这个函数替代掉例子中的ioutil.ReadAll来拷贝响应结构体到os.Stdout，避免申请一个缓冲区（例子中的b）来存储。记得处理io.Copy返回结果中的错误。 12345678910111213141516171819202122232425262728293031package mainimport ( &quot;bufio&quot; &quot;fmt&quot; &quot;io&quot; &quot;net/http&quot; &quot;os&quot;)func main() &#123; for _, arg := range os.Args[1:] &#123; res, err := http.Get(arg) if err != nil &#123; fmt.Println(err) os.Exit(1) &#125; out, err := os.Create(&quot;/tmp/buf_file2.txt&quot;) // 初始化一个 io.Writer wt := bufio.NewWriter(out) result, err := io.Copy(wt, res.Body) defer res.Body.Close() if err != nil &#123; fmt.Println(err) os.Exit(1) &#125; fmt.Println(result) wt.Flush() &#125;&#125; 2.如果输入的url参数没有 http:// 前缀的话，为这个url加上该前缀。你可能会用到strings.HasPrefix这个函数。 strings.Hasprefix(s, prefix)可以识别字符串的开头，如s字符串以prefix开头则返回true，否则为false，函数原型如下： 123func HasPrefix(s, prefix string) bool &#123; return len(s) &gt;= len(prefix) &amp;&amp; s[0:len(prefix)] == prefix //通过切片处理&#125; 我们只需对参数做判断即可。 12345678910111213141516171819202122232425262728293031323334353637383940414243package mainimport ( &quot;bufio&quot; &quot;fmt&quot; &quot;io&quot; &quot;net/http&quot; &quot;os&quot; &quot;strings&quot;)func main() &#123; for _, arg := range os.Args[1:] &#123; if strings.HasPrefix(arg, &quot;http://&quot;) &#123; fmt.Println(arg) get_txt(arg) &#125; else &#123; arg = &quot;http://&quot; + arg get_txt(arg) fmt.Println(arg) &#125; &#125;&#125;func get_txt(arg string) &#123; res, err := http.Get(arg) if err != nil &#123; fmt.Println(err) os.Exit(1) &#125; out, err := os.Create(&quot;/tmp/buf_file2.txt&quot;) // 初始化一个 io.Writer wt := bufio.NewWriter(out) result, err := io.Copy(wt, res.Body) defer res.Body.Close() if err != nil &#123; fmt.Println(err) os.Exit(1) &#125; fmt.Println(result) wt.Flush()&#125; 3.h打印出HTTP协议的状态码，可以从resp.Status变量得到该状态码。 我们增加一个get_code函数如下，在main将get_txt替换为get_code即可： 123456789func get_code(arg string) &#123; res, err := http.Get(arg) if err != nil &#123; fmt.Println(err) os.Exit(1) &#125; status := res.Status fmt.Println(&quot;Http Code :&quot;, status)&#125; 结果如下： 三、初尝golang并发（扩展）轻松创建高并发应用是go的特点之一，在我们请求多个url时也可以通过goroutime（我理解为协程）来创建并发，再通过信道来传递协程中的数据。我们来创建一个同时请求所有url的应用。 新创建一个new_routine(arg string, ch chan&lt;- string goroutine，传入命令行参数、行道。该函数中包含start_time和end_time用于计算此协程运行的时间。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package mainimport ( &quot;bufio&quot; &quot;fmt&quot; &quot;io&quot; &quot;io/ioutil&quot; &quot;net/http&quot; &quot;os&quot; &quot;strings&quot; &quot;time&quot;)func get_txt(arg string) &#123;// ...&#125;func get_code(arg string) &#123;// ...&#125;func main() &#123; start := time.Now() //记录开始时间 ch := make(chan string) //创建一个字符信道 for _, arg := range os.Args[1:] &#123; //根据参数开启协程 if strings.HasPrefix(arg, &quot;http://&quot;) &#123; go new_routine(arg, ch) &#125; else &#123; arg = &quot;http://&quot; + arg go new_routine(arg, ch) &#125; &#125; //读取信道数据 for range os.Args[1:] &#123; fmt.Println(&lt;-ch) &#125; end := time.Since(start).Seconds() //结束时间 fmt.Printf(&quot;time used :%.2fs\\n&quot;, end)&#125;func new_routine(arg string, ch chan&lt;- string) &#123; start_time := time.Now() res, err := http.Get(arg) if err != nil &#123; ch &lt;- fmt.Sprintf(&quot;err:&quot;, err) return &#125; size_bytes, err := io.Copy(ioutil.Discard, res.Body) res.Body.Close() if err != nil &#123; ch &lt;- fmt.Sprintf(&quot;reading usl:%v&quot;, err) return &#125; end_time := time.Since(start_time).Seconds() ch &lt;- fmt.Sprintf(&quot;%.2fs %10d %s&quot;, end_time, size_bytes, arg) //执行时间、请求大小、url&#125; Sprintf将数据输出到字符串或信道对象中。 ioutil.Discard是一个临时垃圾回收站，可以将不关注数据输出到此。 执行后结果如下： 书籍参考：Go语言圣经 文章有不足的地方欢迎在评论区指出。 欢迎收藏、点赞、提问。关注顶级饮水机管理员，除了管烧热水，有时还做点别的。","categories":[{"name":"golang","slug":"golang","permalink":"https://iqsing.github.io/categories/golang/"}],"tags":[{"name":"http","slug":"http","permalink":"https://iqsing.github.io/tags/http/"}]},{"title":"golang快速入门（三）初尝IO输入输出","slug":"golang快速入门（三）初尝IO输入输出","date":"2021-05-23T16:47:21.000Z","updated":"2022-02-04T05:16:11.842Z","comments":true,"path":"2021/05/24/golang快速入门（三）初尝IO输入输出/","link":"","permalink":"https://iqsing.github.io/2021/05/24/golang%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%E5%88%9D%E5%B0%9DIO%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA/","excerpt":"","text":"提示：本系列文章适合有其他语音基础并对Go有持续冲动的读者 一、关于map与输入方式的窥探在python 中可以通过字典dict做字符等统计，在go中有类似数据结构map。 map存储了键/值（key/value）的集合，对集合元素，提供常数时间的存、取或测试操作。键可以是任意类型，只要其值能用==运算符比较，最常见的例子是字符串；值则可以是任意类型。这个例子中的键是字符串，值是整数。 1.通过标准输入bufio包读取数据12345678910111213141516171819package mainimport ( &quot;bufio&quot; &quot;fmt&quot; &quot;os&quot;)func main() &#123; counts := make(map[string]int) //创建一个空map input_data := bufio.NewScanner(os.Stdin) for i := 0; i &lt; 5; i++ &#123; input_data.Scan() counts[input_dat a.Text()]++ &#125; for index, value := range counts &#123; //通过range遍历map fmt.Printf(&quot;%s\\t%d\\n&quot;, index, value) &#125;&#125; bufio包，它使处理输入和输出方便又高效。Scanner类型是该包最有用的特性之一，它读取输入并将其拆成行或单词；通常是处理行形式的输入最简单的方法。 通过range对map的迭代顺序是随机的，这种设计是有意为之的，因为能防止程序依赖特定遍历顺序，这里后续在深入学习。 2. 通过文件读取数据123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960package mainimport ( &quot;bufio&quot; &quot;fmt&quot; &quot;os&quot;)func main() &#123; file := os.Args[1:] counts := make(map[string]int) //创建一个空map if len(file) == 0 &#123; //无有文件参数时调用标准输入函数 countline(counts) //传入counts的copy &#125; else &#123; for _, arg := range file &#123; f, err := os.Open(arg) if err != nil &#123; fmt.Println(os.Stderr) continue &#125; read_file(f, counts) //传入文件和counts的copy &#125; &#125; for index, value := range counts &#123; //打印行和统计数字 fmt.Printf(&quot;%s\\t%d\\n&quot;, index, value) //类c语言中的prinf函数 &#125;&#125;//读取文件，传入一个已打开文件、map类型参数counts func read_file(f *os.File, counts map[string]int) &#123; input := bufio.NewScanner(f) for input.Scan() &#123; //Scan搜索文件下一行 counts[input.Text()]++ &#125;&#125;func countline(counts map[string]int) &#123; input_data := bufio.NewScanner(os.Stdin) for i := 0; i &lt; 5; i++ &#123; input_data.Scan() counts[input_data.Text()]++ &#125;&#125;// myfile[root@VM-0-5-centos course2]# cat myfile aaabbbaaacccddd//output[root@VM-0-5-centos course2]# go run counts.go myfileddd 1aaa 2bbb 1ccc 1 map作为参数传递给某函数时，该函数接收这个引用的一份拷贝（copy，或译为副本），被调用函数对map底层数据结构的任何修改，调用者函数都可以通过持有的map引用看到。即在上面程序中read_file和countline对counts这个map做修改后可以被main发现，在其他语言中可能需要通过return返回数据，在go就不需要啦。 go中fmt.Printf函数提供类c语言中的prinf的输出格式化能力。 12345678910%d 十进制整数%x, %o, %b 十六进制，八进制，二进制整数。%f, %g, %e 浮点数： 3.141593 3.141592653589793 3.141593e+00%t 布尔：true或false%c 字符（rune） (Unicode码点)%s 字符串%q 带双引号的字符串&quot;abc&quot;或带单引号的字符&#x27;c&#x27;%v 变量的自然形式（natural format）%T 变量的类型%% 字面上的百分号标志（无操作数） 3.通过ReadFile读取文件ReadFile一次性读取所有内容。ReadFile函数返回一个字节切片（byte slice），必须把它转换为string，再通过strings.Split分割。 123456789101112131415161718192021222324252627282930313233343536373839package mainimport ( &quot;fmt&quot; &quot;io/ioutil&quot; &quot;os&quot; &quot;strings&quot;)func main() &#123; counts := make(map[string]int) //创建一个空map for _, filename := range os.Args[1:] &#123; data, err := ioutil.ReadFile(filename) if err != nil &#123; fmt.Println(os.Stderr, &quot;%v&quot;, err) continue &#125; //fmt.Println(string(data)) for _, line := range strings.Split(string(data), &quot;\\n&quot;) &#123; if len(line) &gt; 0 &#123; counts[line]++ &#125; &#125; output(counts) &#125;&#125;func output(counts map[string]int) &#123; for index, value := range counts &#123; fmt.Printf(&quot;%s\\t%d\\n&quot;, index, value) &#125;&#125;//output[root@VM-0-5-centos course2]# go run ioutil.go myfileaaa 2bbb 1ccc 1ddd 1 实现上，bufio.Scanner、ioutil.ReadFile和ioutil.WriteFile都使用*os.File的Read和Write方法，但是，大多数程序员很少需要直接调用那些低级（lower-level）函数。高级（higher-level）函数，像bufio和io/ioutil包中所提供的那些，用起来要容易点。 二、 练习练习 1.4： 修改code，出现重复的行时打印文件名称。 调整output函数如下即可。 123456789101112131415161718func output(counts map[string]int) &#123; for index, value := range counts &#123; if value &gt;=2&#123; fmt.Printf(&quot;filename:%s %s\\t%d\\n&quot;, os.Args[0],index, value) &#125;else&#123; fmt.Printf(&quot;%s\\t%d\\n&quot;, index, value) &#125; &#125;&#125;//output[root@VM-0-5-centos course2]# go run ioutil.go myfilebbb 1ccc 1ddd 1filename:/tmp/go-build303515636/b001/exe/ioutil aaa 2 文章来源公众号【容器云实践】golang快速入门（二）初尝命令行参数 欢迎关注容器云实践，每天学点怪东西。","categories":[{"name":"golang","slug":"golang","permalink":"https://iqsing.github.io/categories/golang/"}],"tags":[{"name":"IO","slug":"IO","permalink":"https://iqsing.github.io/tags/IO/"}]},{"title":"golang快速入门（二）初尝命令行参数","slug":"golang快速入门（二）初尝命令行参数","date":"2021-05-22T16:47:21.000Z","updated":"2022-02-04T05:16:11.863Z","comments":true,"path":"2021/05/23/golang快速入门（二）初尝命令行参数/","link":"","permalink":"https://iqsing.github.io/2021/05/23/golang%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%EF%BC%88%E4%BA%8C%EF%BC%89%E5%88%9D%E5%B0%9D%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%8F%82%E6%95%B0/","excerpt":"","text":"一、package介绍​ Go语言的代码是通过package来组织的，package的概念和你知道的其它语言 里的libraries或者modules概念比较类似。 ​ 一个package会包含一个或多个.go结束的源代码文件。每一 个源文件都是以一个package xxx的声明语句开头的，比如我们的例子里就是package main。这行声明语 句表示该文件是属于哪一个package，紧跟着是一系列import的package名，表示这个文件中引入的 package。再之后是本文件本身的代码。 ​ package main是一个比较特殊的package。这个package里会定义一个独立的程序，这个程序是可以运行 的，而不是像其它package一样对应一个library。 ​ 这也正是因为go语言必须引入所有要用到的package的原则，假如你没有在代码里import需要用到的 package，程序将无法编译通过，同时当你import了没有用到的package，也会无法编译通过。 二、命令行参数​ os这个package提供了操作系统无关（跨平台）的，与系统交互的一些函数和相关的变量，运行时程序的 命令行参数可以通过os包中一个叫Args的变量来获取；当在os包外部使用该变量时，需要用os.Args来访问。 os.Args的第一个元素，即os.Args[0]是命令行执行时的命令本身；其它的元素则是执行该命令时传给这 个程序的参数。前面提到的切片表达式，s[m:n]会返回第m到第n-1个元素，所以下一个例子里需要用到 的os.Args[1:len(os.Args)]即是除了命令本身外的所有传入参数。如果我们省略s[m:n]里的m和n，那么 默认这个表达式会填入0:len(s)，所以这里我们还可以省略掉n，写成os.Args[1:]。 1.输出命令行参数12345678910111213141516171819// go 中命令行参数的使用package mainimport ( &quot;fmt&quot; &quot;os&quot;)func main() &#123; var s, seq string seq = &quot; &quot; for i := 1; i &lt; len(os.Args); i++ &#123; s += os.Args[i] + seq &#125; fmt.Println(s)&#125;[root@VM-0-5-centos course1]# go run lesson-1.go hello worldhello world 2.关于循环在Go语言里只有for循环一种循环。 1234567891011121314for initialization; condition; post &#123; // zero or more statements&#125;// a traditional &quot;while&quot; loopfor condition &#123; // ...&#125;// a traditional infinite loopfor &#123; // ...&#125; range 每次循环迭代，range产生一对值；索引以及在该索引处的元素值。这个例子不需要索引，但range的语法要求, 要处理元素, 必须处理索引。一种思路是把索引赋值给一个临时变量, 如temp, 然后忽略它的值，但Go语言不允许使用无用的局部变量（local variables），因为这会导致编译错误。 Go语言中这种情况的解决方法是用空标识符（blank identifier），即_（也就是下划线）。空标识符可用于任何语法需要变量名但程序逻辑不需要的时候, 例如, 在循环里，丢弃不需要的循环索引, 保留元素值。大多数的Go程序员都会像上面这样使用range和_写echo程序，因为隐式地而非显式地索引os.Args，容易写对。 12345678910111213141516package mainimport ( &quot;fmt&quot; &quot;os&quot;)func main() &#123; var s string seq := &quot; &quot; for _, arg := range os.Args[1:] &#123; s += arg + seq &#125; fmt.Println(s)&#125; strings.Join连接参数 12345678910111213141516171819package mainimport ( &quot;fmt&quot; &quot;os&quot; &quot;strings&quot;)func main() &#123; var s string s = strings.Join(os.Args[1:], &quot; &quot;) fmt.Println(s) fmt.Println(os.Args[1:]) //切片的方式输出&#125;[root@VM-0-5-centos course1]# go run join.go wo apple ibm ciscowo apple ibm cisco[wo apple ibm cisco] 三、 练习练习 1.1： 修改echo程序，使其打印每个参数的索引和值，每个一行。 123456789101112131415161718package mainimport ( &quot;fmt&quot; &quot;os&quot;)func main() &#123; var s string seq := &quot; &quot; for index, arg := range os.Args[1:] &#123; s += arg + seq //fmt.Println(&quot;yees&quot; + s) fmt.Println(index, arg) &#125; //fmt.Println(s)&#125; 练习 1.2： 修改echo程序，使其能够打印os.Args[0]，即被执行命令本身的名字。 1234567891011121314151617181920212223package mainimport ( &quot;fmt&quot; &quot;os&quot;)func main() &#123; var s string seq := &quot; &quot; for index, arg := range os.Args[1:] &#123; s += arg + seq //fmt.Println(&quot;yees&quot; + s) fmt.Println(index, arg) &#125; fmt.Println(os.Args[0])&#125;//output[root@VM-0-5-centos course1]# go run for_range.go apple ibm0 apple1 ibm/tmp/go-build844979177/b001/exe/for_range 练习 1.3： 做实验测量潜在低效的版本和使用了strings.Join的版本的运行时间差异。（time包） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950//for循环遍历package mainimport ( &quot;fmt&quot; &quot;os&quot; &quot;time&quot;)func main() &#123; var s, seq string seq = &quot; &quot; var start_time = time.Now() for i := 1; i &lt; len(os.Args); i++ &#123; s += os.Args[i] + seq &#125; fmt.Println(s) end_time := time.Now() fmt.Println(end_time.Sub(start_time))&#125;[root@VM-0-5-centos course1]# go run comandline.go apple ibmapple ibm 19.667µs//join连接package mainimport ( &quot;fmt&quot; &quot;os&quot; &quot;strings&quot; &quot;time&quot;)func main() &#123; var s string start_time := time.Now() s = strings.Join(os.Args[1:], &quot; &quot;) fmt.Println(s) //fmt.Println(os.Args[1:]) end_time := time.Now() fmt.Println(end_time.Sub(start_time))&#125;[root@VM-0-5-centos course1]# go run join.go apple ibmapple ibm16.782µs 书籍参考：Go语言圣经中文版 文章有不足的地方欢迎在评论区指出。 欢迎收藏、点赞、提问。关注顶级饮水机管理员，除了管烧热水，有时还做点别的。","categories":[{"name":"golang","slug":"golang","permalink":"https://iqsing.github.io/categories/golang/"}],"tags":[{"name":"commandline","slug":"commandline","permalink":"https://iqsing.github.io/tags/commandline/"}]},{"title":"golang快速入门（一）初尝Go","slug":"golang快速入门（一）初尝Go","date":"2021-05-18T16:47:21.000Z","updated":"2022-02-04T05:16:11.829Z","comments":true,"path":"2021/05/19/golang快速入门（一）初尝Go/","link":"","permalink":"https://iqsing.github.io/2021/05/19/golang%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%80%EF%BC%89%E5%88%9D%E5%B0%9DGo/","excerpt":"","text":"提示：本系列文章适合有其他语音基础并对Go有持续冲动的读者 一、见初国内官网https://golang.google.cn/ 1.linux二进制安装1234567891011121314#获取go1.16.4wget https://golang.google.cn/dl/go1.16.4.linux-amd64.tar.gz tar -xf go1.16.4.linux-amd64.tar.gz -C /usr/localecho &quot;export PATH=$PATH:/usr/local/go/bin&quot; &gt;&gt; /etc/profilesource /etc/profile#验证安装[root@VM-0-5-centos bin]# go versiongo version go1.16.4 linux/amd64 2.Hello, Water Man!vim water_man.go 1234567package mainimport &quot;fmt&quot;func main() &#123; fmt.Println(&quot;Hello, Water Man!&quot;)&#125; go run water_man.go 1Hello, Water Man! go应用可直接编译目标平台二进制可执行文件 12345[root@VM-0-5-centos hellogo]# go build water_man.go [root@VM-0-5-centos hellogo]# lswater_man water_man.go[root@VM-0-5-centos hellogo]# ./water_man Hello, Water Man! 二、了解go1. go modulegolang的包管理工具。 包设计的初衷是方便模块化设计和使用，在go中模块是包的集合，如果模块需要被其他人使用，则应该在go mod init初始化时以可以下载的web地址命名,产生的go.mod在go运行或编译时可跟踪依赖。 初始化rush_b模块 12345678mkdir rush_b &amp;&amp; cd rush_b[root@VM-0-5-centos rush_b]# go mod init github.com/csgo/rush_bgo: creating new go.mod: module github.com/csgo/rush_b#获得包含包列表名和go版本号的go.mod文件[root@VM-0-5-centos rush_b]# lsgo.mod 创建rush_b.go 编写模块实现函数，用于共享的功能模块函数首字母应大写。 123456789package rush_bimport &quot;fmt&quot;func Rush(name string) string&#123; message := fmt.Sprintf(&quot;%v, Follow me!&quot;,name) return message&#125; go中函数定义中需要把声明的类型放在后面。 := 相当于同时定义和赋值，是下面的简写 12var message stringmessage = fmt.Sprinf(&quot;%v,Follow me!&quot;,name) 创建好go.mod 、rush_b.go后我们的github.com/csgo/rush_b模块就创建好了，接下来该考虑怎么引用它。 同理我们创建一个water_man文件夹 12345[root@VM-0-5-centos hellogo]# ll water_man/total 8-rw-r--r-- 1 root root 32 May 11 20:49 go.mod-rw-r--r-- 1 root root 169 May 11 20:54 water_man.go 导入rush_b模块，使用其中的rush函数。 1234567891011package mainimport ( &quot;fmt&quot; &quot;github.com/csgo/rush_b&quot;)func main() &#123; fmt.Println(&quot;Hello&quot;) message :=rush_b.Rush(&quot;water man&quot;) fmt.Println(message)&#125; go run之后会根据地址去下载模块，由于模块是本地模块，所以我们go mod edit --replace=github.com/csgo/rush_b=../rush_b 告诉go 使用本地模块。 再此go run，正常调用了rush_b模块的Rush函数。 2.errors与log 用于规范错误提示和日志排查。当调用的Rush函数入参为空时返回错误。 1234567891011121314151617181920212223242526272829303132333435363738394041//rush_b.gopackage rush_bimport ( &quot;fmt&quot; &quot;errors&quot;)func Rush(name string) (string,error)&#123; if name == &quot;&quot;&#123; return &quot;&quot;,errors.New(&quot;empty name&quot;) &#125; message := fmt.Sprintf(&quot;%v, Fllow me!&quot;,name) return message,nil&#125;//water_man.gopackage mainimport ( &quot;fmt&quot; &quot;log&quot; &quot;github.com/csgo/rush_b&quot;)func main() &#123; //fmt.Println(&quot;Hello&quot;) log.SetPrefix(&quot;logging:&quot;) //设置前缀 log.SetFlags(0) message,err := rush_b.Rush(&quot;&quot;) fmt.Println(message) if err !=nil&#123; //fmt.Println(&quot;it is an err&quot;) log.Fatal(err) //打印error &#125;&#125; [root@VM-0-5-centos water_man]# go run water_man.go logging:empty nameexit status 1 3.让S1mple跟我rush Brush_b.go 当调用Rush时会随机返回addr切片中的一个标语。 12345678910111213141516171819202122232425262728package rush_bimport ( &quot;fmt&quot; &quot;errors&quot; &quot;math/rand&quot; &quot;time&quot;)func Rand_init()&#123; //随机数种子 rand.Seed(time.Now().UnixNano()) &#125;func RandomFormat() string &#123; addr :=[]string&#123; &quot;%v ,follow me rush A&quot;, &quot;%v ,follow me rush B&quot;, &quot;%v ,follow me rush dust2&quot;, &#125; return addr[rand.Intn(len(addr))] //len函数、Intn方法返回一个[0,n)的整数&#125;func Rush(name string) (string,error)&#123; if name == &quot;&quot;&#123; return &quot;&quot;,errors.New(&quot;empty name&quot;) &#125; message := fmt.Sprintf(RandomFormat(),name) return message,nil&#125; water_man.go 1234567891011121314151617181920package mainimport ( &quot;fmt&quot; &quot;log&quot; &quot;github.com/csgo/rush_b&quot;)func main() &#123; //fmt.Println(&quot;Hello&quot;) rush_b.Rand_init() //初始化种子 log.SetPrefix(&quot;logging:&quot;) log.SetFlags(0) message,err := rush_b.Rush(&quot;s1mple&quot;) //传入S1mple fmt.Println(message) if err !=nil&#123; //fmt.Println(&quot;it is an err&quot;) log.Fatal(err) &#125;&#125; go run water_man.go rush到哪不一定！ 1234567[root@VM-0-5-centos water_man]# go run water_man.go s1mple ,follow me rush dust2[root@VM-0-5-centos water_man]# go run water_man.go s1mple ,follow me rush B[root@VM-0-5-centos water_man]# go run water_man.go s1mple ,follow me rush dust2[root@VM-0-5-centos water_man]# 4. 让top3给我打佯攻使用到了map，类似于python中的字典 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465// rush_b.gopackage rush_bimport ( &quot;fmt&quot; &quot;errors&quot; &quot;math/rand&quot; &quot;time&quot;)func Rand_init()&#123; rand.Seed(time.Now().UnixNano()) &#125;func RandomFormat( name string) (string,error) &#123; if name == &quot;&quot;&#123; return &quot;&quot;,errors.New(&quot;empty name&quot;) &#125; addr := []string &#123; &quot;%v ,follow me rush A&quot;, &quot;%v ,follow me rush B&quot;, &quot;%v ,follow me rush dust2&quot;, &#125; return fmt.Sprintf(addr[rand.Intn(len(addr))],name),nil&#125;func Yg(names []string ) (map[string]string,error)&#123; //佯攻 messages := make(map[string]string) // map初始化 for _,name := range names&#123; message,err := RandomFormat(name) if err != nil &#123; return nil, err &#125; messages[name] = message &#125; return messages,nil&#125;//water_man.gopackage mainimport ( &quot;fmt&quot; &quot;log&quot; &quot;github.com/csgo/rush_b&quot;)func main() &#123; //fmt.Println(&quot;Hello&quot;) rush_b.Rand_init() log.SetPrefix(&quot;logging:&quot;) log.SetFlags(0) hero:=[]string&#123;&quot;s2mple&quot;,&quot;zywoo1&quot;,&quot;device&quot;&#125; message,err := rush_b.Yg(hero) for i := 0; i &lt; len(hero); i++ &#123; fmt.Println(message[hero[i]]) &#125; if err !=nil&#123; //fmt.Println(&quot;it is an err&quot;) log.Fatal(err) &#125;&#125; go run water_man.go 执行情况如下： 1234[root@VM-0-5-centos water_man]# go run water_man.go s2mple ,follow me rush Bzywoo1 ,follow me rush Adevice ,follow me rush A 文章有不足的地方欢迎在评论区指出。 欢迎收藏、点赞、提问。关注WeChat，一起涨知识。","categories":[{"name":"golang","slug":"golang","permalink":"https://iqsing.github.io/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"https://iqsing.github.io/tags/golang/"}]},{"title":"vim命令使用（必备系列）","slug":"vim命令使用（必备系列）","date":"2021-05-14T16:47:21.000Z","updated":"2022-02-04T05:16:12.053Z","comments":true,"path":"2021/05/15/vim命令使用（必备系列）/","link":"","permalink":"https://iqsing.github.io/2021/05/15/vim%E5%91%BD%E4%BB%A4%E4%BD%BF%E7%94%A8%EF%BC%88%E5%BF%85%E5%A4%87%E7%B3%BB%E5%88%97%EF%BC%89/","excerpt":"","text":"掌握关键vim命令化解查bug、改配置文件、查日志、讲解时%99的尴尬病！ 一、我要插入（普通–&gt;插入模式）： i：在光标所在行的行首插入 a：从目前光标所在位置的下一个位置开始插入 A：在光标所在行的行末插入 o：在光标下开辟一行插入 O：在光标上开辟一行插入 二、反复横跳（普通模式下移动光标）： Ctrl+f：屏幕向下移动一页 h：左移光标 l：右移光标 j：下移光标 k：上移光标 Ctrl+b：屏幕向上移动一页 Ctrl+e: 往下滚动 Ctrl+y: 往上滚动 G：移动到文件首行 gg：移动到文件行尾，相当于1G H 移动到屏幕最上面一行 M 移动光标到屏幕中间行 L 移动光标到屏幕最下面行 0：移动到行首 $：移动到行尾 w：光标跳到下个字的开头 e：光标跳到下个字的字尾 b：光标回到上个字的开头 三、查找和替换（命令模式下） /Ame：向下寻找一个名称为Ame的字符串。 n，查找下一个 N，查找上一个 ?Ame：向上寻找一个名称为Ame的字符串。n,N同上 :n1,n2s/Ame/FlyFly/g：在第n1行和n2行之间寻找这个字符串，并且将其替换为FlyFly. :1,$s/Ame/FlyFly/g：全文寻找Ame这个字符串，并且将其替换为FlyFly. %s/Ame/FlyFly/g 同上 :1,$s/Ame/FlyFly/gc：替换时提示用户 四、删除、修改、复制和粘贴： x,X：在一行文本中，x为向后删除一个字符（相当于[Del]键），X为向前删除一个字符（相当于[Backspace]） dd：删除光标所在的一整行 dw: 删除向前的一个单词 daw：删除一个单词 d$: 删除光标到行尾 c$: 修改光标到行尾内容 cw：修改1个单词， c2w修改两个.. ndd：删除光标所在的向下n行 yy：复制光标所在的一行 nyy：复制光标所在的向下n行 p,P：p为将已复制的内容在光标处粘贴，P则为粘贴在光标的上一行 u：撤销操作 Ctrl+r：重做上一个操作。（可用于反撤销） r：替换光标所在处的一个字符 R：替换光标所到处的字符，直到按下“ESC”键为止 J 当前行连接下一行 全文删除：按esc键后，先按gg（到达顶部），然后dG 全文复制：按esc键后，先按gg，然后yG 五、命令模式下： set nu：列出行号 set nonu：取消列出行号 set ic：搜索时忽略大小写 set noic：取消在搜索时忽略大小写 n：跳到文件中的某一行,“n”表示一个数字，如输入数字15,再回车就会跳到文本的第15行 !pwd：运行shell命令pwd 六、可视化模式 字符模式 v（小写） 复制删除粘贴。选择高亮文本后操作同普通模式 修改。 选择高亮文本后 按c后删除并进入插入模式 行模式 V（大写） 缩进 。按住 &gt; 和 &lt; 键向右或向左移动代码块。 复制删除粘贴。选择高亮文本后操作同普通模式 块模式 Ctrl + v 缩进。同上 复制删除粘贴。选择高亮文本后操作同普通模式 可视化模式下的应用： 复制删除粘贴。选择高亮文本后操作同普通模式 修改。 选择高亮文本后 按c后删除并进入插入模式 缩进 。按住 &gt; 和 &lt; 键向右或向左移动代码块。 大小写转换。 按U将选中内容变大写-按u将选中内容变小写-按~将大小翻转。 添加（删除注释），常用于配置文件管理。 （1）添加批量注释。ctrl+v 进入块模式，向下或向上移动光标，把需要注释的行的开头标记起来，然后按大写的I(shift+i)，再插入注释符，比如”#”,再按两次Esc，就会全部注释了。 （2）批量去掉注释。ctrl+v进入块模式，横向选中列的个数(如”#”注释符号）然后按d， 就会删除注释符号。 七、代码块中应用 % 跳转到相配对的括号 &gt;增加缩进 &lt; 减少缩进 { 跳到上一段的开头 } 跳到下一段的的开头 ( 移到这个句子的开头 ) 移到下一个句子的开头 Ctrl+] 跳转到函数、变量定义处 Ctrl+o Ctrl+i 返回跳转的位置 ‘’ 跳转到光标上次停靠的地方, 是两个’, 而不是一个” 八、怪东西 :!ls 运行shell命令 : r file.txt 将file文件内容不到光标处 :w file.txt 将内容保存为文件file.txt :e file.txt 重新打开一个文件 v w ：进入可视化模式选择并选择一个单词 掌握关键vim命令： 收藏本文 将你现在的编辑器装上vim插件 practice！ practice！ practice！ 文章有不足的地方欢迎在评论区指出。 欢迎收藏、点赞、提问。关注顶级饮水机管理员，除了管烧热水，有时还做点别的。","categories":[{"name":"linux","slug":"linux","permalink":"https://iqsing.github.io/categories/linux/"}],"tags":[{"name":"vim","slug":"vim","permalink":"https://iqsing.github.io/tags/vim/"}]},{"title":"docker-compose基础使用","slug":"docker-compose基础使用","date":"2021-05-10T16:47:21.000Z","updated":"2022-02-04T05:16:11.739Z","comments":true,"path":"2021/05/11/docker-compose基础使用/","link":"","permalink":"https://iqsing.github.io/2021/05/11/docker-compose%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8/","excerpt":"","text":"一、docker-compose简介docker-compose 是官方开源容器创建、多容器编排的工具。 我们知道一个完整的应用系统往往包含多个容器，相辅相成提供服务。如一个web应用需要包含web服务、数据库等，此时通过使用docker-compose来轻松实现对多容器的控制和管理。 开源地址：https://github.com/docker/compose 三、docker-compose安装docker-compose由python编写，可以通过pip包管理安装 12345678pip install docker-compose#或yum方式安装wget https://repo.huaweicloud.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repoyum install docker-compose#或二进制安装clone GitHubrelease 二、docker-compose的三个关键Docker Compose 将所管理的容器分为三层，分别是工程（project）、服务（service）、容器（container） docker-compose.yml文件所在目录为docker-compose工程，一个工程包含多个服务，每个服务中定义了容器运行的镜像、容器名、端口、网络等参数。即一个服务可包含多个容器实例。 如下的一个docker-compose.yml文件 12345678910111213141516171819202122232425262728293031version: &#x27;2.3&#x27; services: log: image: goharbor/harbor-log:v2.1.5 container_name: harbor-log restart: always dns_search: . cap_drop: - ALL cap_add: - CHOWN - DAC_OVERRIDE - SETGID - SETUID volumes: - /var/log/harbor/:/var/log/docker/:z - type: bind source: ./common/config/log/logrotate.conf target: /etc/logrotate.d/logrotate.conf - type: bind source: ./common/config/log/rsyslog_docker.conf target: /etc/rsyslog.d/rsyslog_docker.conf ports: - 127.0.0.1:1514:10514 networks: - harbor registry: image: goharbor/registry-photon:v2.1.5 container_name: registry restart: always ... 三、docker-compose 常用命令 ps：列出所有运行容器 1docker-compose ps logs：查看服务日志输出 1docker-compose logs build：构建或者重新构建服务 1docker-compose build start：启动指定服务已存在的容器 1docker-compose start nginx stop：停止已运行的服务的容器 1docker-compose stop nginx rm：删除指定服务的容器 1docker-compose rm nginx up：构建、启动容器 1docker-compose up kill：通过发送 SIGKILL 信号来停止指定服务的容器 1docker-compose kill nginx pull：下载服务镜像 1234567891011121314151617181920212223242526272829303132version: &#x27;2&#x27;services: db: image: postgres web: build: . command: bundle exec rails s -p 3000 -b &#x27;0.0.0.0&#x27; volumes: - .:/myapp ports: - &quot;3000:3000&quot; depends_on: - db $ docker-compose pull dbPulling db (postgres:latest)...latest: Pulling from library/postgrescd0a524342ef: Pull complete9c784d04dcb0: Pull completed99dddf7e662: Pull completee5bff71e3ce6: Pull completecb3e0a865488: Pull complete31295d654cd5: Pull completefc930a4e09f5: Pull complete8650cce8ef01: Pull complete61949acd8e52: Pull complete527a203588c0: Pull complete26dec14ac775: Pull complete0efc0ed5a9e5: Pull complete40cd26695b38: Pull completeDigest: sha256:fd6c0e2a9d053bebb294bb13765b3e01be7817bf77b01d58c2377ff27a4a46dcStatus: Downloaded newer image for postgres:latest scale：设置指定服务运气容器的个数. 1docker-compose scale nginx=3 redis=3 run：在一个服务上执行一个命令 1docker-compose run web bash 详细命令和介绍参考官方文档https://docs.docker.com/compose/reference/ 四、docker-compose实践使用docker-compose部署一个python web应用。 1.创建一个工程目录12mkdir mycomposecd mycompose 2. 创建flask应用 app.py1234567891011121314151617181920212223import timeimport redisfrom flask import Flaskapp = Flask(__name__)cache = redis.Redis(host=&#x27;redis&#x27;, port=6379)def get_hit_count(): retries = 5 while True: try: return cache.incr(&#x27;hits&#x27;) except redis.exceptions.ConnectionError as exc: if retries == 0: raise exc retries -= 1 time.sleep(0.5)@app.route(&#x27;/&#x27;)def hello(): count = get_hit_count() return &#x27;Hello World! I have been seen &#123;&#125; times.\\n&#x27;.format(count) 3.创建requirements.txt12flaskredis 4.创建Dockerfile文件1234567891011# syntax=docker/dockerfile:1FROM python:3.7-alpineWORKDIR /codeENV FLASK_APP=app.pyENV FLASK_RUN_HOST=0.0.0.0RUN apk add --no-cache gcc musl-dev linux-headersCOPY requirements.txt requirements.txtRUN pip install -r requirements.txtEXPOSE 5000COPY . .CMD [&quot;flask&quot;, &quot;run&quot;] 5.创建你的docker-compose.yml 包含两个容器 web和redis12345678version: &quot;3.0&quot;services: web: build: . ports: - &quot;5000:5000&quot; redis: image: &quot;redis:alpine&quot; 6.构建和运行docker-compose工程123456#此时你的工程目录如下：[root@VM-0-5-centos mycompose]# lsapp.py docker-compose.yml Dockerfile requirements.txtdocker-compose up 开始构建容器 docker ps可查看到两个容器已经起来了 验证安装 http://ip ，如下安装已完成。 更新docker-compose.yml 添加挂载 123456789101112version: &quot;3.0&quot;services: web: build: . ports: - &quot;5000:5000&quot; volumes: - .:/code environment: FLASK_ENV: development redis: image: &quot;redis:alpine&quot; 编辑app.py 123456789101112131415161718192021222324import timeimport redisfrom flask import Flaskapp = Flask(__name__)cache = redis.Redis(host=&#x27;redis&#x27;, port=6379)def get_hit_count(): retries = 5 while True: try: return cache.incr(&#x27;hits&#x27;) except redis.exceptions.ConnectionError as exc: if retries == 0: raise exc retries -= 1 time.sleep(0.5)@app.route(&#x27;/&#x27;)def hello(): count = get_hit_count() return &#x27; I have been seen &#123;&#125; times.\\n&#x27;.format(count) #去除hello world 重新构建web容器 123[root@VM-0-5-centos mycompose]# docker-compose up -dStarting mycompose_web_1 ... Starting mycompose_web_1 ... done 此时访问web页面如下： docker-compose基础使用完毕啦，深入学习可参考官方文档https://docs.docker.com/compose/reference/ 文章有不足的地方欢迎在评论区指出。 欢迎收藏、点赞、提问。关注顶级饮水机管理员，除了管烧热水，有时还做点别的。","categories":[{"name":"docker","slug":"docker","permalink":"https://iqsing.github.io/categories/docker/"}],"tags":[{"name":"docker-compose","slug":"docker-compose","permalink":"https://iqsing.github.io/tags/docker-compose/"}]},{"title":"Typora+PicGo+cos图床打造开发者文档神器","slug":"Typora+PicGo+cos图床打造开发者文档神器","date":"2021-05-05T16:47:21.000Z","updated":"2022-02-04T05:16:11.729Z","comments":true,"path":"2021/05/06/Typora+PicGo+cos图床打造开发者文档神器/","link":"","permalink":"https://iqsing.github.io/2021/05/06/Typora+PicGo+cos%E5%9B%BE%E5%BA%8A%E6%89%93%E9%80%A0%E5%BC%80%E5%8F%91%E8%80%85%E6%96%87%E6%A1%A3%E7%A5%9E%E5%99%A8/","excerpt":"","text":"一、Typora简介markdown简单、高效的语法，被每一个开发者所喜爱。Typora又是一款简约、强悍的实时渲染markdown编辑器。本文将介绍Typora搭配PicGo与腾讯cos对象存储（图床）打造开发者文档（写博客）神器。 本文就是使用Typora编写的。搭配图床，上传到多个博客只需要粘贴即可。 官方下载地址:https://typora.io/ 二、PicGo简介PicGo是Typora官方支持的图床工具。 PicGo官方地址:https://molunerfinn.com/PicGo/ 三、创建腾讯云cos对象存储（图床）1.在对象存储控制台创建存储桶 ，创建一个公有读私有写的存储桶。 本例中创建markdown-1257692304桶，桶下面创建一个文件夹用于存放图片。 2.在访问管理-访问密钥中创建一个API密钥 四、PicGo 配置在腾讯云cos中填上创建的API即可。 五、Typora配置 文件-&gt; 偏好设置-&gt;图像 2.在插入图片是选择上传图片 ，配置PicGo路径 。 3.验证图片上传选项看到如下则配置完成。 这时候你粘贴到Typora的图片已经自动上传到腾讯云对象存储啦，当你发布博客到多个平台是再也不需要手动上传图片了。 六、为啥选则收费图床？不管是阿里云、还是腾讯云对于写博客来说对象存储费用低。 可以看到即使阅读100w/月（这个阅读量已经业界是大哥大了！），费用也是相当低的。 当然也可以选择github来做图床，只是国内加载你的图片有多慢，懂得都懂。 有不理解的地方可在评论区指出。欢迎收藏、点赞、提问。关注顶级饮水机管理员，除了管烧热水，有时还做点别的。","categories":[{"name":"tools","slug":"tools","permalink":"https://iqsing.github.io/categories/tools/"}],"tags":[{"name":"Typora","slug":"Typora","permalink":"https://iqsing.github.io/tags/Typora/"},{"name":"picGo","slug":"picGo","permalink":"https://iqsing.github.io/tags/picGo/"}]},{"title":"zabbix API创建监控项、模板","slug":"grafana接入zabbix数据源","date":"2021-04-29T16:47:21.000Z","updated":"2022-02-04T05:16:11.903Z","comments":true,"path":"2021/04/30/grafana接入zabbix数据源/","link":"","permalink":"https://iqsing.github.io/2021/04/30/grafana%E6%8E%A5%E5%85%A5zabbix%E6%95%B0%E6%8D%AE%E6%BA%90/","excerpt":"","text":"一、grafana介绍grafana是开源免费的应用数据可视化仪表盘，由于zabbix本身对监控数据可视化并不侧重，所以大多使用第三方数据可视化工具来做大屏。下面向小伙伴们介绍grafana接入zabbix监控数据。 二、使用步骤1.grafana安装docker方式： 1docker run -d --name=grafana -p 3000:3000 grafana/grafana 参考安装方式：官方地址 安装好后访问_ip:3000_如下： 2.安装zabbix插件进入docker容器使用grafana-cli： 1234docker exec -it grafana grafana-cli plugins install alexanderzobnin-zabbix-app docker restart grafana 3.接入数据源在grafana界面中点击setting-plugins 可看到zabbix插件已经安装好了。 点击启用插件 配置zabbix api接入 添加zabbix数据源，以添加Zabbix server 这台机器 5分钟负载为例。 保存后仪表盘如下： 接下来可以开始定制自己的仪表盘啦。欢迎收藏、点赞、提问。关注顶级饮水机管理员，除了管烧热水，有时还做点别的。","categories":[{"name":"zabbix","slug":"zabbix","permalink":"https://iqsing.github.io/categories/zabbix/"}],"tags":[{"name":"api","slug":"api","permalink":"https://iqsing.github.io/tags/api/"}]},{"title":"zabbix API创建监控项、模板","slug":"zabbix API创建监控项、模板","date":"2021-04-29T16:47:21.000Z","updated":"2022-02-04T05:16:12.065Z","comments":true,"path":"2021/04/30/zabbix API创建监控项、模板/","link":"","permalink":"https://iqsing.github.io/2021/04/30/zabbix%20API%E5%88%9B%E5%BB%BA%E7%9B%91%E6%8E%A7%E9%A1%B9%E3%80%81%E6%A8%A1%E6%9D%BF/","excerpt":"","text":"一、zabbix API简介zabbix开放了一套完善的API，可用于二次开发、API自动化操作等，zabbix API是基于json-rpc2.0协议的web API，即客户端对API的请求和响应使用json格式编码。Zabbix API由许多名义上分组的独立API方法组成。大多数API至少包含四种方法： get， create， update 和 delete。 zabbix5.0手册现已支持中文，官方手册地址:zabbix 官方中文手册 二、pyzabbix 简介pyzabbix是用python封装的zabbix api，支持1.8至5.0 API使用。github地址：pyzabbix 安装pyzabbix 1pip install pyzabbix 测试API 12345678from pyzabbix import ZabbixAPIzapi = ZabbixAPI(&quot;http://192.168.43.130/zabbix&quot;) #zabbix地址zapi.login(&quot;Admin&quot;, &quot;zabbix&quot;) #zabbix用户名、密码print(&quot;Connected to Zabbix API Version %s&quot; % zapi.api_version())for h in zapi.host.get(output=&quot;extend&quot;): print(h[&#x27;hostid&#x27;]) 输出如下,正常连接至zabbix 12345678910Connected to Zabbix API Version 4.0.16103401036610267103331033410335103371035510358 三、通过API创建监控项、触发器和模板1.创建模板在zabbix中模板必须有所属模板组，所以创新之前需要获取模板组id 123456789101112131415161718def get_template_group_id(api, name): #入参 api对象、组名称 group_re = api.hostgroup.get(output=[&#x27;groupid&#x27;], filter=&#123;&quot;name&quot;: [name]&#125;) if group_re: group_id = group_re[0][&#x27;groupid&#x27;] else: group_id = &#x27;&#x27; return group_iddef create_template(api, gid, name, alias): #入参 api对象、模板所属组id、名称（只能英文）、别名 ret = api.template.create( host=name, name=alias, groups=&#123; &#x27;groupid&#x27;: gid &#125; ) print(&quot;template_name:&#123;&#125;&quot;.format(name)) return ret[&#x27;templateids&#x27;][0] 2.为模板创建监控项、触发器1234567891011121314151617181920def create_item_trigger(api, template_id, template_name, ip_addr,port, status=1, delay=&#x27;1m&#x27;, expression=&#x27;=0&#x27;, level=3, atime=3): item_name = &#x27;Telnet&#123;0&#125; 端口(10050)状态&#x27;.format(ip_addr) #监控项名称 key = &#x27;net.tcp.port[&#123;0&#125;,10050]&#x27;.format(ip_addr) #键值 item_id = api.item.create( name=item_name, key_=key, hostid=str(template_id), type=7, value_type=3, delay=delay )[&#x27;itemids&#x27;][0] print(&quot;&#123;&#125; created success!&quot;.format(ip_addr)) trigger_id = api.trigger.create( description=&quot;端口(&#123;0&#125;)状态异常&quot;.format(port), #触发器名称 expression=&#x27;&#123;%s:%s.min(#%d)&#125;%s&#x27; % (template_name, key, atime, expression), #触发器表达式 priority=level, status=status )[&#x27;triggerids&#x27;][0] print(&quot;item_id:&#123;&#125;\\n trigger_id:&#123;&#125;\\n&quot;.format(item_id,trigger_id)) 四、创建一个完整模板以模板名称为API_create_template ，监控项为Telnet192.168.43.140 端口(10050)状态 ，触发器为 端口(10050)状态异常为例创建监控模板。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475from pyzabbix import ZabbixAPI, ZabbixAPIExceptiondef get_api(url, user, pw): api = ZabbixAPI(url) # api地址 api.login(user, pw) # Zabbix用户名、密码 return apidef get_template_group_id(api, name): group_re = api.hostgroup.get(output=[&#x27;groupid&#x27;], filter=&#123;&quot;name&quot;: [name]&#125;) if group_re: group_id = group_re[0][&#x27;groupid&#x27;] else: group_id = &#x27;&#x27; return group_iddef read_file(fn): data = get_data(fn) return datadef create_template(api, gid, name, alias): ret = api.template.create( host=name, name=alias, groups=&#123; &#x27;groupid&#x27;: gid &#125; ) print(&quot;template_name:&#123;&#125;&quot;.format(name)) return ret[&#x27;templateids&#x27;][0]def get_template_id(api,tpl_name): tpl_id = &#x27;&#x27; try: res = api.template.get(filter=&#123;&#x27;host&#x27;:tpl_name&#125;, output=[&#x27;templateid&#x27;]) if res: tpl_id = res[0][&#x27;templateid&#x27;] except ZabbixAPIException as e: print(e) return tpl_iddef create_item_trigger(api, template_id, template_name, ip_addr,port, status=1, delay=&#x27;1m&#x27;, expression=&#x27;=0&#x27;, level=3, atime=3): item_name = &#x27;Telnet&#123;0&#125; 端口(10050)状态&#x27;.format(ip_addr) #监控项名称 key = &#x27;net.tcp.port[&#123;0&#125;,10050]&#x27;.format(ip_addr) #键值 item_id = api.item.create( name=item_name, key_=key, hostid=str(template_id), type=7, value_type=3, delay=delay )[&#x27;itemids&#x27;][0] print(&quot;&#123;&#125; created success!&quot;.format(ip_addr)) trigger_id = api.trigger.create( description=&quot;端口(&#123;0&#125;)状态异常&quot;.format(port), #触发器名称 expression=&#x27;&#123;%s:%s.min(#%d)&#125;%s&#x27; % (template_name, key, atime, expression), #触发器表达式 priority=level, status=status )[&#x27;triggerids&#x27;][0] print(&quot;item_id:&#123;&#125;\\n trigger_id:&#123;&#125;\\n&quot;.format(item_id,trigger_id))if __name__ == &#x27;__main__&#x27;: template_name=template_alias=&quot;API_create_template&quot; api=get_api(&quot;http://192.168.43.130/zabbix&quot;,&quot;Admin&quot;,&quot;zabbix&quot;) #创建api对象 gid=get_template_group_id(api, &#x27;Templates/Applications&#x27;) #获取模板分组id tpl_id=create_template(api, gid, template_name, template_alias) #创建模板 tpl_id=get_template_id(api,template_name) try: create_item_trigger(api,tpl_id,template_name,&quot;192.168.43.40&quot;,10050) #创建监控项、触发器 except Exception as e: print(e) 结果如下： 包含一个监控项和触发器 有不理解的地方可在评论区指出。欢迎收藏、点赞、提问。关注顶级饮水机管理员，除了管烧热水，有时还做点别的。","categories":[{"name":"zabbix","slug":"zabbix","permalink":"https://iqsing.github.io/categories/zabbix/"}],"tags":[{"name":"api","slug":"api","permalink":"https://iqsing.github.io/tags/api/"}]},{"title":"zabbix容器化安装及监控docker应用","slug":"zabbix容器化安装及监控docker应用","date":"2021-04-29T16:47:21.000Z","updated":"2022-02-04T05:16:12.073Z","comments":true,"path":"2021/04/30/zabbix容器化安装及监控docker应用/","link":"","permalink":"https://iqsing.github.io/2021/04/30/zabbix%E5%AE%B9%E5%99%A8%E5%8C%96%E5%AE%89%E8%A3%85%E5%8F%8A%E7%9B%91%E6%8E%A7docker%E5%BA%94%E7%94%A8/","excerpt":"","text":"一、zabbix agent2 介绍从Zabbix 4.4之后，官方推出了Zabbix Agent 2，意味着zabbix 不在只是物理机监控的代名词，现在你可以使用Go为Zabbix编写插件，来监控各类应用及微服务。以下为官方对zabbix agent2的介绍： Zabbix agent2是新一代Zabbix代理，可以代替Zabbix代理使用。Zabbix agent2已开发为： 减少TCP连接数 具有更大的支票并发 易于通过插件扩展。插件应该能够： 提供仅由几行简单代码组成的琐碎检查 提供由长期运行的脚本和独立的数据收集组成的复杂检查，并定期发送回数据 替代Zabbix代理（因为它支持所有以前的功能） agent2用Go编写（重用了Zabbix代理的一些C代码）。构建Zabbix agent2需要配置的Go版本1.13+环境。 本文主要介绍如何通过zabbix agent2 来监控你的docker应用。 二、zabbix 5.x 安装1.安装docker 版本不限制，过程参考网络不再赘述。2.容器化方式安装 zabbix server 安装华为zabbix 5.2 yum源1rpm -ivh https://repo.huaweicloud.com/zabbix/zabbix/5.2/rhel/7/x86_64/zabbix-release-5.2-1.el7.noarch.rpm 创建容器网络1docker network create --subnet 172.20.0.0/16 --ip-range 172.20.240.0/20 zabbix-net 创建数据库容器12345678910docker run --name mysql-server -t \\ -e MYSQL_DATABASE=&quot;zabbix&quot; \\ -e MYSQL_USER=&quot;zabbix&quot; \\ -e MYSQL_PASSWORD=&quot;zabbix_pwd&quot; \\ -e MYSQL_ROOT_PASSWORD=&quot;root_pwd&quot; \\ --network=zabbix-net \\ -d mysql:8.0 \\ --restart unless-stopped \\ --character-set-server=utf8 --collation-server=utf8_bin \\ --default-authentication-plugin=mysql_native_password 创建 zabbix-java-getway 容器1234docker run --name zabbix-java-gateway -t \\ --network=zabbix-net \\ --restart unless-stopped \\ -d zabbix/zabbix-java-gateway:alpine-5.2-latest 创建 zabbix-server-mysql 容器12345678910111213docker run --name zabbix-server-mysql -t \\ -e DB_SERVER_HOST=&quot;mysql-server&quot; \\ -e MYSQL_DATABASE=&quot;zabbix&quot; \\ -e MYSQL_USER=&quot;zabbix&quot; \\ -e MYSQL_PASSWORD=&quot;zabbix_pwd&quot; \\ -e MYSQL_ROOT_PASSWORD=&quot;root_pwd&quot; \\ -e ZBX_JAVAGATEWAY=&quot;zabbix-java-gateway&quot; \\ --network=zabbix-net \\ -p 10051:10051 \\ --restart unless-stopped \\ -d zabbix/zabbix-server-mysql:alpine-5.2-latest # 暴露 10051/TCP端口 创建web前端容器123456789101112docker run --name zabbix-web-nginx-mysql -t \\ -e ZBX_SERVER_HOST=&quot;zabbix-server-mysql&quot; \\ -e DB_SERVER_HOST=&quot;mysql-server&quot; \\ -e MYSQL_DATABASE=&quot;zabbix&quot; \\ -e MYSQL_USER=&quot;zabbix&quot; \\ -e MYSQL_PASSWORD=&quot;zabbix_pwd&quot; \\ -e MYSQL_ROOT_PASSWORD=&quot;root_pwd&quot; \\ --network=zabbix-net \\ -p 80:8080 \\ --restart unless-stopped \\ -d zabbix/zabbix-web-nginx-mysql:alpine-5.2-latest#暴露 80端口 3. 登录server ip地址验证安装1234567# docker ps 四个容器已正常启动[root@VM-0-5-centos ~]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES5dc24c3d05e5 mysql:8.0 &quot;docker-entrypoint...&quot; 3 weeks ago Up 3 weeks 3306/tcp, 33060/tcp mysql-server12aa15d78de2 zabbix/zabbix-web-nginx-mysql:alpine-5.0-latest &quot;docker-entrypoint.sh&quot; 3 weeks ago Up 3 weeks 8443/tcp, 0.0.0.0:80-&gt;8080/tcp zabbix-web-nginx-mysql0deae5fc6fc0 zabbix/zabbix-server-mysql:alpine-5.0-latest &quot;/sbin/tini -- /us...&quot; 3 weeks ago Up 3 weeks 0.0.0.0:10051-&gt;10051/tcp zabbix-server-mysqla417d9958ad2 zabbix/zabbix-java-gateway:alpine-5.0-latest &quot;docker-entrypoint...&quot; 3 weeks ago Up 3 weeks 10052/tcp zabbix-java-gateway 安装方式可参考官方文档：容器化方式安装zabbix server 三、zabbix监控docker应用1.安装zabbix-agent2123456yum install zabbix-aget2#启动agent2[root@VM-0-5-centos ~]# ps -ef|grep agent2zabbix 23184 1 0 Mar30 ? 00:12:40 /usr/sbin/zabbix_agent2 -c /etc/zabbix/zabbix_agent2.confroot 30458 30414 0 14:33 pts/6 00:00:00 grep --color=auto agent2 2.将docker模板链接到zabbix-server主机，并更新。docker模板监控项如下 3.给/var/run/docker.sock 所有用户可读权限（zabbix用户）1chmod 666 /var/run/docker.sock 4.docker中的应用列表如下 可以看到docker模板已经自动发现了docker中的应用，如下所示 以监控mysql应用为例，其部分监控项如下 5. 添加docker 仪表盘在仪表盘中添加图形，选择需要展示的监控项 多个图形构成的mysql docker应用的仪表盘 后续可添加告警相关内容，本文不做展开，小伙伴们可自行探索哈。 有不理解的地方可在评论区指出。欢迎收藏、点赞、提问。关注顶级饮水机管理员，除了管烧热水，有时还做点别的。","categories":[{"name":"zabbix","slug":"zabbix","permalink":"https://iqsing.github.io/categories/zabbix/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://iqsing.github.io/tags/docker/"}]}],"categories":[{"name":"prometheus","slug":"prometheus","permalink":"https://iqsing.github.io/categories/prometheus/"},{"name":"k8s","slug":"k8s","permalink":"https://iqsing.github.io/categories/k8s/"},{"name":"network","slug":"network","permalink":"https://iqsing.github.io/categories/network/"},{"name":"docker","slug":"docker","permalink":"https://iqsing.github.io/categories/docker/"},{"name":"linux","slug":"docker/linux","permalink":"https://iqsing.github.io/categories/docker/linux/"},{"name":"python","slug":"python","permalink":"https://iqsing.github.io/categories/python/"},{"name":"docker","slug":"k8s/docker","permalink":"https://iqsing.github.io/categories/k8s/docker/"},{"name":"golang","slug":"golang","permalink":"https://iqsing.github.io/categories/golang/"},{"name":"linux","slug":"linux","permalink":"https://iqsing.github.io/categories/linux/"},{"name":"tools","slug":"tools","permalink":"https://iqsing.github.io/categories/tools/"},{"name":"zabbix","slug":"zabbix","permalink":"https://iqsing.github.io/categories/zabbix/"}],"tags":[{"name":"tsdb","slug":"tsdb","permalink":"https://iqsing.github.io/tags/tsdb/"},{"name":"PromQL","slug":"PromQL","permalink":"https://iqsing.github.io/tags/PromQL/"},{"name":"service-discovery","slug":"service-discovery","permalink":"https://iqsing.github.io/tags/service-discovery/"},{"name":"helm","slug":"helm","permalink":"https://iqsing.github.io/tags/helm/"},{"name":"https","slug":"https","permalink":"https://iqsing.github.io/tags/https/"},{"name":"RBAC","slug":"RBAC","permalink":"https://iqsing.github.io/tags/RBAC/"},{"name":"loadBalancer","slug":"loadBalancer","permalink":"https://iqsing.github.io/tags/loadBalancer/"},{"name":"ingress","slug":"ingress","permalink":"https://iqsing.github.io/tags/ingress/"},{"name":"service","slug":"service","permalink":"https://iqsing.github.io/tags/service/"},{"name":"configmap","slug":"configmap","permalink":"https://iqsing.github.io/tags/configmap/"},{"name":"secret","slug":"secret","permalink":"https://iqsing.github.io/tags/secret/"},{"name":"network","slug":"network","permalink":"https://iqsing.github.io/tags/network/"},{"name":"job","slug":"job","permalink":"https://iqsing.github.io/tags/job/"},{"name":"DaemonSet","slug":"DaemonSet","permalink":"https://iqsing.github.io/tags/DaemonSet/"},{"name":"statefulset","slug":"statefulset","permalink":"https://iqsing.github.io/tags/statefulset/"},{"name":"deployment","slug":"deployment","permalink":"https://iqsing.github.io/tags/deployment/"},{"name":"workflow","slug":"workflow","permalink":"https://iqsing.github.io/tags/workflow/"},{"name":"k8s","slug":"k8s","permalink":"https://iqsing.github.io/tags/k8s/"},{"name":"dockerfile","slug":"dockerfile","permalink":"https://iqsing.github.io/tags/dockerfile/"},{"name":"storage","slug":"storage","permalink":"https://iqsing.github.io/tags/storage/"},{"name":"bridge","slug":"bridge","permalink":"https://iqsing.github.io/tags/bridge/"},{"name":"OverlayFS","slug":"OverlayFS","permalink":"https://iqsing.github.io/tags/OverlayFS/"},{"name":"Cgroup","slug":"Cgroup","permalink":"https://iqsing.github.io/tags/Cgroup/"},{"name":"Namespace","slug":"Namespace","permalink":"https://iqsing.github.io/tags/Namespace/"},{"name":"Copilot","slug":"Copilot","permalink":"https://iqsing.github.io/tags/Copilot/"},{"name":"harbor","slug":"harbor","permalink":"https://iqsing.github.io/tags/harbor/"},{"name":"celery","slug":"celery","permalink":"https://iqsing.github.io/tags/celery/"},{"name":"django","slug":"django","permalink":"https://iqsing.github.io/tags/django/"},{"name":"commandline","slug":"commandline","permalink":"https://iqsing.github.io/tags/commandline/"},{"name":"git","slug":"git","permalink":"https://iqsing.github.io/tags/git/"},{"name":"restful","slug":"restful","permalink":"https://iqsing.github.io/tags/restful/"},{"name":"vue.js","slug":"vue-js","permalink":"https://iqsing.github.io/tags/vue-js/"},{"name":"DateType","slug":"DateType","permalink":"https://iqsing.github.io/tags/DateType/"},{"name":"golang","slug":"golang","permalink":"https://iqsing.github.io/tags/golang/"},{"name":"DataType","slug":"DataType","permalink":"https://iqsing.github.io/tags/DataType/"},{"name":"struct","slug":"struct","permalink":"https://iqsing.github.io/tags/struct/"},{"name":"http","slug":"http","permalink":"https://iqsing.github.io/tags/http/"},{"name":"IO","slug":"IO","permalink":"https://iqsing.github.io/tags/IO/"},{"name":"vim","slug":"vim","permalink":"https://iqsing.github.io/tags/vim/"},{"name":"docker-compose","slug":"docker-compose","permalink":"https://iqsing.github.io/tags/docker-compose/"},{"name":"Typora","slug":"Typora","permalink":"https://iqsing.github.io/tags/Typora/"},{"name":"picGo","slug":"picGo","permalink":"https://iqsing.github.io/tags/picGo/"},{"name":"api","slug":"api","permalink":"https://iqsing.github.io/tags/api/"},{"name":"docker","slug":"docker","permalink":"https://iqsing.github.io/tags/docker/"}]}